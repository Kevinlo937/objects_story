{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kevinlo937/objects_story/blob/main/cv_MultipleObjectDection_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rok3It0N_HRE",
        "outputId": "03453b8b-5325-4a72-ef4c-c6a17e7d7a10",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.63-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Downloading roboflow-1.1.63-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.63\n"
          ]
        }
      ],
      "source": [
        "# 安裝 Roboflow 套件（用於資料集下載）\n",
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaUfQsdE-7W8",
        "outputId": "3bbd7952-501b-4ddb-de33-4e05f3d40c31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in package-at-front-door-2 to yolov8:: 100%|██████████| 31292/31292 [00:01<00:00, 19043.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to package-at-front-door-2 in yolov8:: 100%|██████████| 2598/2598 [00:00<00:00, 8749.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in find-luggage-1 to yolov8:: 100%|██████████| 2852/2852 [00:00<00:00, 3586.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to find-luggage-1 in yolov8:: 100%|██████████| 206/206 [00:00<00:00, 8540.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow workspace...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in person-2 to yolov8:: 100%|██████████| 8413/8413 [00:01<00:00, 8394.75it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to person-2 in yolov8:: 100%|██████████| 482/482 [00:00<00:00, 8086.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow workspace...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in deliverman-1 to yolov8:: 100%|██████████| 10326/10326 [00:01<00:00, 10051.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to deliverman-1 in yolov8:: 100%|██████████| 328/328 [00:00<00:00, 6026.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow workspace...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in food_deliverman-1 to yolov8:: 100%|██████████| 4262/4262 [00:00<00:00, 4900.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to food_deliverman-1 in yolov8:: 100%|██████████| 132/132 [00:00<00:00, 4241.18it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import roboflow\n",
        "\n",
        "# Set your Roboflow API key as an environment variable\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = \"NKaOxmYYLFRPaf6w0uF6\" # Replace with your actual API key\n",
        "\n",
        "rf = roboflow.Roboflow(api_key=os.environ.get(\"ROBOFLOW_API_KEY\"))\n",
        "\n",
        "# The rest of your code remains the same...\n",
        "# Download the package dataset, 276 views , 18 downloads, mAP@50=99.5% , Precision=100%, Recall=99.2%\n",
        "# package_detection_dataset = rf.workspace(\"packagedetection\").project(\"package-detection-kt9ut\").version(2).download(\"yolov8\")\n",
        "\n",
        "# Download the Packages at Front Door dataset, 2022-11-30, 1.4k views , 85 downloads , mAP@50=96.4%, Precision=95.7%, Recall=88.2%, class:package\n",
        "package_dataset = rf.workspace(\"package-detection\").project(\"package-at-front-door\").version(2).download(\"yolov8\")\n",
        "\n",
        "# Download the Bags dataset, class:\"paper bag\"\n",
        "paperbag_dataset = rf.workspace(\"findluggage\").project(\"find-luggage\").version(1).download(\"yolov8\")\n",
        "\n",
        "# Download the Persons dataset, class:person\n",
        "person_dataset = rf.workspace(\"project-ii\").project(\"person-7y27w\").version(2).download(\"yolov8\")\n",
        "\n",
        "# Download the deliverman dataset, class:mailman\n",
        "mailman_dataset = rf.workspace(\"kevinlo937\").project(\"deliverman\").version(1).download(\"yolov8\")\n",
        "\n",
        "# Download the food_deliverman dataset, class:Foodpenda\n",
        "foodpenda_dataset = rf.workspace(\"kevinlo937\").project(\"food_deliverman\").version(1).download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttR-MpAGerlq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# 創建合併資料集的目錄\n",
        "merged_dataset_dir = \"merged_dataset\"\n",
        "os.makedirs(merged_dataset_dir, exist_ok=True)\n",
        "os.makedirs(f\"{merged_dataset_dir}/train/images\", exist_ok=True)\n",
        "os.makedirs(f\"{merged_dataset_dir}/train/labels\", exist_ok=True)\n",
        "os.makedirs(f\"{merged_dataset_dir}/valid/images\", exist_ok=True)\n",
        "os.makedirs(f\"{merged_dataset_dir}/valid/labels\", exist_ok=True)\n",
        "os.makedirs(f\"{merged_dataset_dir}/test/images\", exist_ok=True)\n",
        "os.makedirs(f\"{merged_dataset_dir}/test/labels\", exist_ok=True)\n",
        "\n",
        "# 定義資料集路徑列表\n",
        "dataset_paths = [\n",
        "    package_dataset.location,\n",
        "    # paperbag_dataset.location,\n",
        "    person_dataset.location,\n",
        "    mailman_dataset.location,\n",
        "    foodpenda_dataset.location\n",
        "]\n",
        "\n",
        "# 合併資料集\n",
        "for dataset_path in dataset_paths:\n",
        "    # 複製訓練集\n",
        "    for img in Path(f\"{dataset_path}/train/images\").glob(\"*.*\"):\n",
        "        shutil.copy(img, f\"{merged_dataset_dir}/train/images/\")\n",
        "    for label in Path(f\"{dataset_path}/train/labels\").glob(\"*.txt\"):\n",
        "        shutil.copy(label, f\"{merged_dataset_dir}/train/labels/\")\n",
        "\n",
        "    # 複製驗證集\n",
        "    for img in Path(f\"{dataset_path}/valid/images\").glob(\"*.*\"):\n",
        "        shutil.copy(img, f\"{merged_dataset_dir}/valid/images/\")\n",
        "    for label in Path(f\"{dataset_path}/valid/labels\").glob(\"*.txt\"):\n",
        "        shutil.copy(label, f\"{merged_dataset_dir}/valid/labels/\")\n",
        "\n",
        "    # 複製測試集\n",
        "    for img in Path(f\"{dataset_path}/test/images\").glob(\"*.*\"):\n",
        "        shutil.copy(img, f\"{merged_dataset_dir}/test/images/\")\n",
        "    for label in Path(f\"{dataset_path}/test/labels\").glob(\"*.txt\"):\n",
        "        shutil.copy(label, f\"{merged_dataset_dir}/test/labels/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_dataset_dir='/content/merged_dataset' # 明確指定路徑 merged_dataset_dir='/content/merged_dataset'\n",
        "print(merged_dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMjFlo4Gx_HO",
        "outputId": "5833698f-bec1-45f6-ecf8-ef5a8e3b3f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/merged_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -h -o '^[0-9]' merged_dataset/train/labels/*.txt | sort | uniq -c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh-eCMFZF5f_",
        "outputId": "5b16c3bf-4b79-4290-ba52-6830ca2bb0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   2264 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立映射表\n",
        "id_map = {\n",
        "    \"package\": 0,\n",
        "    \"person\": 1,\n",
        "    \"mailman\": 2,\n",
        "    \"foodpanda\": 3,\n",
        "}\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# 針對不同來源資料集給一個「正確的新 ID」\n",
        "source_to_new_id = {\n",
        "    \"package-at-front-door\": 0,\n",
        "    \"person-7y27w\": 1,\n",
        "    \"deliverman\": 2,\n",
        "    \"food_deliverman\": 3,\n",
        "}\n",
        "\n",
        "for label_file in Path(\"merged_dataset\").rglob(\"labels/*.txt\"):\n",
        "    # 依檔案路徑判定它來自哪個資料集\n",
        "    for src, new_id in source_to_new_id.items():\n",
        "        if src in str(label_file):\n",
        "            lines = label_file.read_text().strip().splitlines()\n",
        "            new_lines = []\n",
        "            for ln in lines:\n",
        "                parts = ln.split()\n",
        "                parts[0] = str(new_id)     # 換 class id\n",
        "                new_lines.append(\" \".join(parts))\n",
        "            label_file.write_text(\"\\n\".join(new_lines))\n",
        "            break\n"
      ],
      "metadata": {
        "id": "onyIfl_-GFNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -h -o '^[0-9]' merged_dataset/train/labels/*.txt | sort | uniq -c"
      ],
      "metadata": {
        "id": "fHD3N2DFHDCC",
        "outputId": "bb299412-0a43-443f-f222-02fc5228b374",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   2264 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_CSRztzfXE1"
      },
      "outputs": [],
      "source": [
        "# 創建標籤映射文件\n",
        "with open(f\"{merged_dataset_dir}/data.yaml\", \"w\") as f:\n",
        "    f.write(f\"train: {merged_dataset_dir}/train/images\\n\")  # Specify full path for train images\n",
        "    f.write(f\"val: {merged_dataset_dir}/valid/images\\n\")  # Specify full path for validation images\n",
        "    f.write(f\"test: {merged_dataset_dir}/test/images\\n\\n\")  # Specify full path for test images\n",
        "\n",
        "    # 定義類別映射\n",
        "    f.write(\"names:\\n\")\n",
        "    f.write(\"  0: package\\n\")\n",
        "    # f.write(\"  1: paper bag\\n\")    # a class name with blank may cause class losed ?\n",
        "    f.write(\"  1: person\\n\")\n",
        "    f.write(\"  2: mailman\\n\") # 郵差快遞員\n",
        "    f.write(\"  3: foodpenda\\n\")   # 送餐員\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GspdTlDqfgSp"
      },
      "outputs": [],
      "source": [
        "### 5. 資料增強（針對資料量較少的類別）\n",
        "\n",
        "'''python\n",
        "from ultralytics.yolo.data.augment import Albumentations\n",
        "\n",
        "# 定義資料增強策略\n",
        "transform = Albumentations([\n",
        "    # 幾何變換\n",
        "    dict(type='ShiftScaleRotate', p=0.5),\n",
        "    dict(type='RandomRotate90', p=0.5),\n",
        "    dict(type='RandomResizedCrop', height=640, width=640, scale=(0.8, 1.0), p=0.5),\n",
        "\n",
        "    # 顏色變換\n",
        "    dict(type='RandomBrightnessContrast', p=0.5),\n",
        "    dict(type='HueSaturationValue', p=0.5),\n",
        "    dict(type='GaussianBlur', p=0.3),\n",
        "\n",
        "    # 其他變換\n",
        "    dict(type='HorizontalFlip', p=0.5),\n",
        "    dict(type='VerticalFlip', p=0.1),\n",
        "])\n",
        "\n",
        "# 資料增強可以在訓練時通過參數傳遞給 YOLOv8\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GANZNmgiC5B",
        "outputId": "aee046a9-4bfa-489b-ac33-a09ef56aa10f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.124-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.124-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m846.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.124 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "# 安裝 Ultralytics YOLOv8\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5wTSRfkfyYh",
        "outputId": "c9f77dc1-5261-4605-de8b-6424c2cdc3a3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 375MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.124 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/content/merged_dataset/data.yaml, epochs=3, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=package_person_detection, name=yolov8m_merged, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, cutmix=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=package_person_detection/yolov8m_merged\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 44.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3778012  ultralytics.nn.modules.head.Detect           [4, [192, 384, 576]]          \n",
            "Model summary: 169 layers, 25,858,636 parameters, 25,858,620 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 469/475 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 138MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 81.0±96.4 MB/s, size: 24.5 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/merged_dataset/train/labels... 1579 images, 3 backgrounds, 0 corrupt: 100%|██████████| 1579/1579 [00:02<00:00, 650.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/merged_dataset/train/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 55.7±75.9 MB/s, size: 56.1 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/merged_dataset/valid/labels... 113 images, 1 backgrounds, 0 corrupt: 100%|██████████| 113/113 [00:00<00:00, 461.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/merged_dataset/valid/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to package_person_detection/yolov8m_merged/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mpackage_person_detection/yolov8m_merged\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/3      6.12G      1.021      1.639       1.39         28        640: 100%|██████████| 99/99 [00:52<00:00,  1.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        113        116      0.213      0.285      0.148     0.0744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/3      7.33G      1.085      1.173      1.416         26        640: 100%|██████████| 99/99 [00:50<00:00,  1.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        113        116      0.649        0.5      0.544      0.308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/3      7.39G     0.9941     0.9884      1.367         29        640: 100%|██████████| 99/99 [00:51<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        113        116      0.726      0.754      0.779      0.554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3 epochs completed in 0.049 hours.\n",
            "Optimizer stripped from package_person_detection/yolov8m_merged/weights/last.pt, 52.0MB\n",
            "Optimizer stripped from package_person_detection/yolov8m_merged/weights/best.pt, 52.0MB\n",
            "\n",
            "Validating package_person_detection/yolov8m_merged/weights/best.pt...\n",
            "Ultralytics 8.3.124 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 92 layers, 25,842,076 parameters, 0 gradients, 78.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        113        116      0.726      0.755      0.779      0.554\n",
            "               package        112        116      0.726      0.755      0.779      0.554\n",
            "Speed: 0.3ms preprocess, 10.1ms inference, 0.0ms loss, 6.9ms postprocess per image\n",
            "Results saved to \u001b[1mpackage_person_detection/yolov8m_merged\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "## 三、模型訓練\n",
        "\n",
        "### 1. 基本訓練\n",
        "\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# 載入預訓練模型\n",
        "model = YOLO('yolov8m.pt')  # 使用中型 YOLOv8 模型\n",
        "\n",
        "# 開始訓練\n",
        "results = model.train(\n",
        "    data=f\"{merged_dataset_dir}/data.yaml\",\n",
        "    epochs=3, # try 3 first\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    patience=10,\n",
        "    save=True,\n",
        "    device=0,  # 使用 GPU 0\n",
        "    project=\"package_person_detection\",\n",
        "    name=\"yolov8m_merged\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofXdDm7CgHt2"
      },
      "outputs": [],
      "source": [
        "### 2. 進階訓練設置\n",
        "\n",
        "'''\n",
        "# 使用更多進階參數的訓練\n",
        "results = model.train(\n",
        "    data=f\"{merged_dataset_dir}/data.yaml\",\n",
        "    epochs=200,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    patience=30,\n",
        "    save=True,\n",
        "    device=0,\n",
        "    project=\"package_person_detection\",\n",
        "    name=\"yolov8m_advanced\",\n",
        "    lr0=0.01,\n",
        "    lrf=0.001,\n",
        "    momentum=0.937,\n",
        "    weight_decay=0.0005,\n",
        "    warmup_epochs=3,\n",
        "    warmup_momentum=0.8,\n",
        "    warmup_bias_lr=0.1,\n",
        "    box=7.5,\n",
        "    cls=0.5,\n",
        "    dfl=1.5,\n",
        "    fl_gamma=0.0,\n",
        "    label_smoothing=0.0,\n",
        "    nbs=64,\n",
        "    hsv_h=0.015,\n",
        "    hsv_s=0.7,\n",
        "    hsv_v=0.4,\n",
        "    degrees=0.0,\n",
        "    translate=0.1,\n",
        "    scale=0.5,\n",
        "    shear=0.0,\n",
        "    perspective=0.0,\n",
        "    flipud=0.0,\n",
        "    fliplr=0.5,\n",
        "    mosaic=1.0,\n",
        "    mixup=0.0,\n",
        "    copy_paste=0.0\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiKMgB3lgQB_"
      },
      "source": [
        "### 3. 使用 Ultralytics 進行人工標註（針對缺乏資料的類別）\n",
        "\n",
        "```python\n",
        "# 使用 Ultralytics 標註工具\n",
        "# 1. 安裝 labelImg 工具\n",
        "# pip install labelImg\n",
        "\n",
        "# 2. 啟動標註工具\n",
        "# labelImg path/to/images path/to/labels/output yolo\n",
        "\n",
        "# 3. 標註完成後，將新標註的資料加入訓練集\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHoACAsUgaPV"
      },
      "source": [
        "## 四、模型評估與優化\n",
        "\n",
        "### 1. 模型評估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGBlafyp5Hif",
        "outputId": "bcbec0ec-c5ef-44dd-a4c9-c11287ceb73f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.124 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 92 layers, 25,842,076 parameters, 0 gradients, 78.7 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1096.3±354.5 MB/s, size: 43.1 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/merged_dataset/valid/labels.cache... 113 images, 1 backgrounds, 0 corrupt: 100%|██████████| 113/113 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:05<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        113        116      0.726      0.753      0.779      0.559\n",
            "               package        112        116      0.726      0.753      0.779      0.559\n",
            "Speed: 6.4ms preprocess, 21.2ms inference, 0.0ms loss, 3.3ms postprocess per image\n",
            "Results saved to \u001b[1mpackage_person_detection/yolov8m_merged2\u001b[0m\n",
            "mAP50: 0.779193414320808\n",
            "mAP50-95: 0.5592150662651035\n"
          ]
        }
      ],
      "source": [
        "# 在測試集上評估模型\n",
        "results = model.val(data=f\"{merged_dataset_dir}/data.yaml\")\n",
        "\n",
        "# 查看評估指標\n",
        "# metrics = results.box.metrics.dict  # This line is causing the error\n",
        "# metrics = results.box.metrics  # Access the DetMetrics object # This line is redundant.\n",
        "# Access metrics directly from results.box\n",
        "print(f\"mAP50: {results.box.map50}\")\n",
        "print(f\"mAP50-95: {results.box.map}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get precision and recall for each class\n",
        "precision = results.box.p\n",
        "recall = results.box.r\n",
        "\n",
        "# Print precision and recall for each class\n",
        "for i, (p, r) in enumerate(zip(precision, recall)):\n",
        "    print(f\"Class {i}: Precision = {p:.4f}, Recall = {r:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA061XLX7Kw2",
        "outputId": "ee1d1217-69d7-4e42-9015-005892a1ac63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: Precision = 0.9213, Recall = 0.9343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVXeL9cJJ-8R",
        "outputId": "db9618da-521b-4dc4-ee81-6bcf214e2caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class package             : \tPrecision = 0.9213, \tRecall = 0.9343\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "# 讀取 data.yaml 檔案以獲取類別名稱\n",
        "with open(f\"{merged_dataset_dir}/data.yaml\", 'r') as f:\n",
        "    data = yaml.safe_load(f)\n",
        "class_names = data['names']  # 獲取類別名稱字典\n",
        "\n",
        "# Get precision and recall for each class\n",
        "precision = results.box.p\n",
        "recall = results.box.r\n",
        "\n",
        "# Print precision and recall for each class with names\n",
        "for i, (p, r) in enumerate(zip(precision, recall)):\n",
        "    class_name = class_names[i]  # 獲取類別名稱\n",
        "    print(f\"Class {class_name:<20}: \\tPrecision = {p:.4f}, \\tRecall = {r:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -h -o '^[0-9]' merged_dataset/train/labels/*.txt | sort | uniq -c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7XhA3NuFPnj",
        "outputId": "a386bebc-5e52-4b53-8cf8-95ee8956185b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   2264 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4TEaw1Qglq0"
      },
      "source": [
        "### 2. 模型優化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9I3uNcEgj36"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# 模型剪枝（減少模型大小）\n",
        "pruned_model = model.prune(0.3)  # 剪枝 30% 的權重\n",
        "\n",
        "# 模型量化（減少模型大小和推理時間）\n",
        "quantized_model = model.export(format='onnx', dynamic=True, simplify=True)\n",
        "\n",
        "# 模型蒸餾（從大模型學習到小模型）\n",
        "teacher = YOLO('yolov8l.pt')  # 大模型作為教師\n",
        "student = YOLO('yolov8s.pt')  # 小模型作為學生\n",
        "\n",
        "# 使用教師模型的知識蒸餾到學生模型\n",
        "student.train(\n",
        "    data=f\"{merged_dataset_dir}/data.yaml\",\n",
        "    epochs=100,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    project=\"package_person_detection\",\n",
        "    name=\"distilled_model\",\n",
        "    teacher=teacher\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovKdyEe-g5mJ"
      },
      "source": [
        "## 五、模型部署\n",
        "\n",
        "### 1. 模型導出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dThIYydAg8Bv",
        "outputId": "e0197c65-c33c-48d1-f7e5-bc633552bba0",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.118 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'package_person_detection/yolov8m_merged3/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 10, 8400) (49.6 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxslim\n",
            "  Downloading onnxslim-0.1.50-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.12.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.12.0) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxslim) (1.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxslim) (24.2)\n",
            "Collecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxslim) (1.3.0)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/16.0 MB 303.7 MB/s eta 0:00:00\n",
            "Downloading onnxslim-0.1.50-py3-none-any.whl (144 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 144.5/144.5 kB 225.8 MB/s eta 0:00:00\n",
            "Downloading onnxruntime_gpu-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (280.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.8/280.8 MB 168.9 MB/s eta 0:00:00\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 189.8 MB/s eta 0:00:00\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 228.3 MB/s eta 0:00:00\n",
            "Installing collected packages: onnx, humanfriendly, onnxslim, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-gpu-1.21.1 onnxslim-0.1.50\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 18.2s, installed 3 packages: ['onnx>=1.12.0', 'onnxslim', 'onnxruntime-gpu']\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.50...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 40.6s, saved as 'package_person_detection/yolov8m_merged3/weights/best.onnx' (98.7 MB)\n",
            "\n",
            "Export complete (43.8s)\n",
            "Results saved to \u001b[1m/content/package_person_detection/yolov8m_merged3/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=package_person_detection/yolov8m_merged3/weights/best.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=package_person_detection/yolov8m_merged3/weights/best.onnx imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "Ultralytics 8.3.118 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'package_person_detection/yolov8m_merged3/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 10, 8400) (49.6 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.50...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 26.6s, saved as 'package_person_detection/yolov8m_merged3/weights/best.onnx' (98.7 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['tensorrt>7.0.0,!=10.1.0'] not found, attempting AutoUpdate...\n",
            "Collecting tensorrt!=10.1.0,>7.0.0\n",
            "  Downloading tensorrt-10.9.0.34.tar.gz (40 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.7/40.7 kB 5.3 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tensorrt_cu12==10.9.0.34 (from tensorrt!=10.1.0,>7.0.0)\n",
            "  Downloading tensorrt_cu12-10.9.0.34.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tensorrt_cu12_libs==10.9.0.34 (from tensorrt_cu12==10.9.0.34->tensorrt!=10.1.0,>7.0.0)\n",
            "  Downloading tensorrt_cu12_libs-10.9.0.34.tar.gz (704 bytes)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting tensorrt_cu12_bindings==10.9.0.34 (from tensorrt_cu12==10.9.0.34->tensorrt!=10.1.0,>7.0.0)\n",
            "  Downloading tensorrt_cu12_bindings-10.9.0.34-cp311-none-manylinux_2_28_x86_64.whl.metadata (606 bytes)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from tensorrt_cu12_libs==10.9.0.34->tensorrt_cu12==10.9.0.34->tensorrt!=10.1.0,>7.0.0) (12.4.127)\n",
            "Downloading tensorrt_cu12_bindings-10.9.0.34-cp311-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 38.3 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: tensorrt, tensorrt_cu12, tensorrt_cu12_libs\n",
            "  Building wheel for tensorrt (setup.py): started\n",
            "  Building wheel for tensorrt (setup.py): finished with status 'done'\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.9.0.34-py2.py3-none-any.whl size=46629 sha256=20bf8616a435f80f04891804ea1db18a5331d68eb0123db50c640beaca1bba95\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ojwer6s1/wheels/3a/4d/72/f28cb367f1435d026243047d4f60fde8f1c9cbb06a204f842f\n",
            "  Building wheel for tensorrt_cu12 (setup.py): started\n",
            "  Building wheel for tensorrt_cu12 (setup.py): finished with status 'done'\n",
            "  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.9.0.34-py2.py3-none-any.whl size=17466 sha256=f135f7832670cc4ba4e5e0a0b384b942f207c8716e01276c688bc559b8287b87\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ojwer6s1/wheels/75/09/76/6b405075fe4c04097f5713ec0a688df7892aaee823bc141952\n",
            "  Building wheel for tensorrt_cu12_libs (pyproject.toml): started\n",
            "  Building wheel for tensorrt_cu12_libs (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for tensorrt_cu12_libs: filename=tensorrt_cu12_libs-10.9.0.34-py2.py3-none-manylinux_2_28_x86_64.whl size=3103291777 sha256=4a82f0bda2874596f202f6edc8dae99b86a3c4ec2fa142a9c847c4d3a57864a0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ojwer6s1/wheels/33/d0/06/35d7b3006eead25828debb658da848328ebfd38962a2bcd096\n",
            "Successfully built tensorrt tensorrt_cu12 tensorrt_cu12_libs\n",
            "Installing collected packages: tensorrt_cu12_bindings, tensorrt_cu12_libs, tensorrt_cu12, tensorrt\n",
            "Successfully installed tensorrt-10.9.0.34 tensorrt_cu12-10.9.0.34 tensorrt_cu12_bindings-10.9.0.34 tensorrt_cu12_libs-10.9.0.34\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 208.1s, installed 1 package: ['tensorrt>7.0.0,!=10.1.0']\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.9.0.34...\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(-1, 10, -1) DataType.FLOAT\n",
            "WARNING ⚠️ \u001b[34m\u001b[1mTensorRT:\u001b[0m 'dynamic=True' model requires max batch size, i.e. 'batch=16'\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as package_person_detection/yolov8m_merged3/weights/best.engine\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 356.0s, saved as 'package_person_detection/yolov8m_merged3/weights/best.engine' (124.7 MB)\n",
            "\n",
            "Export complete (356.5s)\n",
            "Results saved to \u001b[1m/content/package_person_detection/yolov8m_merged3/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=package_person_detection/yolov8m_merged3/weights/best.engine imgsz=640  \n",
            "Validate:        yolo val task=detect model=package_person_detection/yolov8m_merged3/weights/best.engine imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "Ultralytics 8.3.118 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "ERROR ❌️ argument 'dynamic' is not supported for format='coreml'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-f3dc53ad4dc2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 導出為 CoreML 格式（適用於 iOS 設備）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coreml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 導出為 TensorFlow SavedModel 格式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         }  # method defaults\n\u001b[1;32m    726\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcustom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"export\"\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# highest priority args on the right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mExporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     def train(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/exporter.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# Argument compatibility checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mfmt_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmts_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Arguments\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mvalidate_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/exporter.py\u001b[0m in \u001b[0;36mvalidate_args\u001b[0;34m(format, passed_args, valid_args)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mnot_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassed_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnot_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"ERROR ❌️ argument '{arg}' is not supported for format='{format}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: ERROR ❌️ argument 'dynamic' is not supported for format='coreml'"
          ]
        }
      ],
      "source": [
        "# 導出為 ONNX 格式（適用於多種推理框架）\n",
        "model.export(format='onnx', dynamic=True, simplify=True)\n",
        "\n",
        "# 導出為 TensorRT 格式（NVIDIA GPU 上的高性能推理）\n",
        "model.export(format='engine', dynamic=True, simplify=True, device=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q_EfxZe9OICC",
        "outputId": "d389a4cd-36ef-4a87-cb02-4a85d57b9e4e",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.118 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'package_person_detection/yolov8m_merged3/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 10, 8400) (49.6 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['coremltools>=8.0'] not found, attempting AutoUpdate...\n",
            "Collecting coremltools>=8.0\n",
            "  Downloading coremltools-8.2-cp311-none-manylinux1_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (24.2)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (25.3.0)\n",
            "Collecting cattrs (from coremltools>=8.0)\n",
            "  Downloading cattrs-24.1.3-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pyaml (from coremltools>=8.0)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml->coremltools>=8.0) (6.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->coremltools>=8.0) (1.3.0)\n",
            "Downloading coremltools-8.2-cp311-none-manylinux1_x86_64.whl (2.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 49.4 MB/s eta 0:00:00\n",
            "Downloading cattrs-24.1.3-py3-none-any.whl (66 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 kB 299.0 MB/s eta 0:00:00\n",
            "Downloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, cattrs, coremltools\n",
            "Successfully installed cattrs-24.1.3 coremltools-8.2 pyaml-25.1.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 5.5s, installed 1 package: ['coremltools>=8.0']\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:coremltools:scikit-learn version 1.6.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\n",
            "WARNING:coremltools:XGBoost version 2.1.4 has not been tested with coremltools. You may run into unexpected errors. XGBoost 1.4.2 is the most recent version that has been tested.\n",
            "WARNING:coremltools:TensorFlow version 2.18.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
            "WARNING:coremltools:Torch version 2.6.0+cu124 has not been tested with coremltools. You may run into unexpected errors. Torch 2.5.0 is the most recent version that has been tested.\n",
            "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLCPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLGPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLNeuralEngineComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLComputePlanProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLModelAssetProxy: No module named 'coremltools.libcoremlpython'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mCoreML:\u001b[0m starting export with coremltools 8.2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 720/721 [00:00<00:00, 3343.52 ops/s]\n",
            "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 93.39 passes/s]\n",
            "Running MIL default pipeline: 100%|██████████| 89/89 [00:02<00:00, 33.41 passes/s]\n",
            "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 89.17 passes/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mCoreML:\u001b[0m export success ✅ 27.1s, saved as 'package_person_detection/yolov8m_merged3/weights/best.mlpackage' (49.5 MB)\n",
            "\n",
            "Export complete (28.9s)\n",
            "Results saved to \u001b[1m/content/package_person_detection/yolov8m_merged3/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=package_person_detection/yolov8m_merged3/weights/best.mlpackage imgsz=640  \n",
            "Validate:        yolo val task=detect model=package_person_detection/yolov8m_merged3/weights/best.mlpackage imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "Ultralytics 8.3.118 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "ERROR ❌️ argument 'dynamic' is not supported for format='saved_model'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8d42c6bea6a2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 導出為 TensorFlow SavedModel 格式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'saved_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         }  # method defaults\n\u001b[1;32m    726\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcustom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"export\"\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# highest priority args on the right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mExporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     def train(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/exporter.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# Argument compatibility checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mfmt_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmts_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Arguments\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mvalidate_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/exporter.py\u001b[0m in \u001b[0;36mvalidate_args\u001b[0;34m(format, passed_args, valid_args)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mnot_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassed_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnot_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"ERROR ❌️ argument '{arg}' is not supported for format='{format}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: ERROR ❌️ argument 'dynamic' is not supported for format='saved_model'"
          ]
        }
      ],
      "source": [
        "# 導出為 CoreML 格式（適用於 iOS 設備）\n",
        "model.export(format='coreml', simplify=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "-IZxatrTOXWc",
        "outputId": "3c031f97-8da0-4a4c-fb06-6a40749bb456",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.118 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'package_person_detection/yolov8m_merged3/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 10, 8400) (49.6 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'ai-edge-litert>=1.2.0', 'onnx2tf>=1.26.3'] not found, attempting AutoUpdate...\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting sng4onnx>=1.0.1\n",
            "  Downloading sng4onnx-1.0.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting onnx_graphsurgeon>=0.3.26\n",
            "  Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting ai-edge-litert>=1.2.0\n",
            "  Downloading ai_edge_litert-1.2.0-cp311-cp311-manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting onnx2tf>=1.26.3\n",
            "  Downloading onnx2tf-1.27.2-py3-none-any.whl.metadata (147 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.7/147.7 kB 8.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from onnx_graphsurgeon>=0.3.26) (2.0.2)\n",
            "Requirement already satisfied: onnx>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from onnx_graphsurgeon>=0.3.26) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from ai-edge-litert>=1.2.0) (25.2.10)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.14.0->onnx_graphsurgeon>=0.3.26) (5.29.4)\n",
            "Downloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\n",
            "Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl (57 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.9/57.9 kB 249.0 MB/s eta 0:00:00\n",
            "Downloading ai_edge_litert-1.2.0-cp311-cp311-manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 102.7 MB/s eta 0:00:00\n",
            "Downloading onnx2tf-1.27.2-py3-none-any.whl (446 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 446.6/446.6 kB 289.3 MB/s eta 0:00:00\n",
            "Installing collected packages: sng4onnx, onnx2tf, ai-edge-litert, onnx_graphsurgeon\n",
            "Successfully installed ai-edge-litert-1.2.0 onnx2tf-1.27.2 onnx_graphsurgeon-0.5.8 sng4onnx-1.0.4\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 6.4s, installed 4 packages: ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'ai-edge-litert>=1.2.0', 'onnx2tf>=1.26.3']\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.18.0...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/calibration_image_sample_data_20x128x128x3_float32.npy.zip to 'calibration_image_sample_data_20x128x128x3_float32.npy.zip'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.11M/1.11M [00:00<00:00, 30.7MB/s]\n",
            "Unzipping calibration_image_sample_data_20x128x128x3_float32.npy.zip to /content/calibration_image_sample_data_20x128x128x3_float32.npy...: 100%|██████████| 1/1 [00:00<00:00, 50.74file/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.50...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 3.5s, saved as 'package_person_detection/yolov8m_merged3/weights/best.onnx' (98.8 MB)\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.27.2...\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success ✅ 190.1s, saved as 'package_person_detection/yolov8m_merged3/weights/best_saved_model' (247.6 MB)\n",
            "\n",
            "Export complete (192.0s)\n",
            "Results saved to \u001b[1m/content/package_person_detection/yolov8m_merged3/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=package_person_detection/yolov8m_merged3/weights/best_saved_model imgsz=640  \n",
            "Validate:        yolo val task=detect model=package_person_detection/yolov8m_merged3/weights/best_saved_model imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'package_person_detection/yolov8m_merged3/weights/best_saved_model'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 導出為 TensorFlow SavedModel 格式\n",
        "model.export(format='saved_model', simplify=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmSxAhPnhBhU"
      },
      "source": [
        "### 2. 部署到出入口即時包裹與人物意圖偵測系統"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qdiAxzcUOQN"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output  # << 重要：清除上個frame的顯示"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 載入模型\n",
        "model = YOLO('/content/package_person_detection/yolov8m_merged3/weights/best.pt')\n",
        "\n",
        "# 設定參數\n",
        "conf_threshold = 0.5\n",
        "video_source = 1\n",
        "detect_interval = 15  # 每15張推論一次\n",
        "snapshot_interval_sec = 5  # 每5秒存一張快照\n",
        "snapshot_dir = \"/content/snapshots\"\n",
        "os.makedirs(snapshot_dir, exist_ok=True)\n",
        "\n",
        "if video_source == 0:\n",
        "    def get_camera_stream(ip, username, password, channel=1):\n",
        "        rtsp_url = f\"rtsp://{username}:{password}@{ip}/Streaming/Channels/{channel}01\"\n",
        "        cap = cv2.VideoCapture(rtsp_url)\n",
        "        return cap\n",
        "    camera = get_camera_stream(\"106.107.183.134\", \"admin\", \"Qazwsx1122\")\n",
        "elif video_source == 1:\n",
        "    video_path = \"videoplayback.mp4\"  # 替換成你的影片路徑\n",
        "    camera = cv2.VideoCapture(video_path)\n",
        "else:\n",
        "    print(\"Invalid video source selected.\")\n",
        "    exit()\n",
        "\n",
        "# 取得影片資訊\n",
        "fps = camera.get(cv2.CAP_PROP_FPS) or 30  # 預設30fps\n",
        "frame_width = int(camera.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "snapshot_interval_frames = int(fps * snapshot_interval_sec)\n",
        "\n",
        "# 設定執行時間\n",
        "end_time = time.time() + 300  # 最長5分鐘\n",
        "\n",
        "frame_count = 0\n",
        "inference_count = 0\n",
        "latest_detected_classes = defaultdict(int)   # 最新一輪推論結果\n",
        "cumulative_detected_classes = defaultdict(int)  # 累積所有推論結果\n",
        "latest_names = []\n",
        "frames = []  # 保存所有處理後的frame\n",
        "\n",
        "# 主循環\n",
        "while time.time() < end_time:\n",
        "    ret, frame = camera.read()\n",
        "    if not ret:\n",
        "        print(\"無法讀取影像，可能是影片結束了。\")\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    # 每detect_interval張推論一次\n",
        "    if frame_count % detect_interval == 0:\n",
        "        results = model(frame, conf=conf_threshold)\n",
        "        boxes = results[0].boxes.data.cpu().numpy() if results[0].boxes.data is not None else []\n",
        "        latest_names = results[0].names\n",
        "        inference_count += 1\n",
        "\n",
        "        # 更新最新推論結果 + 累積推論統計\n",
        "        latest_detected_classes = defaultdict(int)\n",
        "        for det in boxes:\n",
        "            _, _, _, _, score, class_id = det\n",
        "            if score >= conf_threshold:\n",
        "                class_name = latest_names[int(class_id)]\n",
        "                latest_detected_classes[class_name] += 1\n",
        "                cumulative_detected_classes[class_name] += 1\n",
        "\n",
        "    # 畫推論資訊文字在frame上\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    # 左上角推論次數\n",
        "    cv2.putText(annotated_frame, f\"Inference Count: {inference_count}\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "    # 右上角目前frame編號\n",
        "    cv2.putText(annotated_frame, f\"Frame: {frame_count}\", (frame_width - 250, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2)\n",
        "\n",
        "    # 左下角顯示累積偵測統計\n",
        "    y_offset = 70\n",
        "    for cls_name, count in cumulative_detected_classes.items():\n",
        "        text = f\"{cls_name}: {count}\"\n",
        "        cv2.putText(annotated_frame, text, (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "        y_offset += 40\n",
        "\n",
        "    # 存進frames清單\n",
        "    frames.append(annotated_frame)\n",
        "\n",
        "    # 每隔snapshot_interval_frames儲存一張快照\n",
        "    if frame_count % snapshot_interval_frames == 0:\n",
        "        snapshot_path = os.path.join(snapshot_dir, f\"snapshot_frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(snapshot_path, annotated_frame)\n",
        "        print(f\"儲存快照: {snapshot_path}\")\n",
        "\n",
        "camera.release()\n",
        "print(f\"總共處理了 {len(frames)} 張frame。\")\n",
        "\n",
        "# --- 合成影片 ---\n",
        "output_video_path = '/content/inference_output.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "for frame in frames:\n",
        "    out.write(frame)\n",
        "out.release()\n",
        "\n",
        "print(f\"影片已儲存到 {output_video_path}\")\n",
        "\n",
        "# 🔥 🔥 🔥 在最後列印累積推論結果 🔥 🔥 🔥\n",
        "print(\"\\n===== 最後累積推論結果 =====\")\n",
        "for cls_name, count in cumulative_detected_classes.items():\n",
        "    print(f\"{cls_name}: {count} 次\")\n",
        "print(\"==============================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FBdG0vk7ip3C",
        "outputId": "6e24edeb-e3dd-44eb-a461-23bec5724ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 (no detections), 26.3ms\n",
            "Speed: 3.4ms preprocess, 26.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.7ms\n",
            "Speed: 2.8ms preprocess, 22.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.7ms\n",
            "Speed: 2.8ms preprocess, 22.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.7ms\n",
            "Speed: 3.0ms preprocess, 22.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.7ms\n",
            "Speed: 2.6ms preprocess, 22.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.7ms\n",
            "Speed: 2.5ms preprocess, 22.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.8ms\n",
            "Speed: 2.5ms preprocess, 22.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.7ms\n",
            "Speed: 2.7ms preprocess, 22.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_149.jpg\n",
            "\n",
            "0: 384x640 (no detections), 22.8ms\n",
            "Speed: 2.7ms preprocess, 22.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.7ms\n",
            "Speed: 2.6ms preprocess, 22.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.8ms\n",
            "Speed: 2.6ms preprocess, 22.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.8ms\n",
            "Speed: 2.6ms preprocess, 22.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.8ms\n",
            "Speed: 2.7ms preprocess, 22.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.7ms\n",
            "Speed: 2.7ms preprocess, 22.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.8ms\n",
            "Speed: 2.2ms preprocess, 22.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.8ms\n",
            "Speed: 5.2ms preprocess, 22.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.8ms\n",
            "Speed: 3.5ms preprocess, 22.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 bag, 25.5ms\n",
            "Speed: 6.1ms preprocess, 25.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_298.jpg\n",
            "\n",
            "0: 384x640 (no detections), 23.6ms\n",
            "Speed: 2.5ms preprocess, 23.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 22.8ms\n",
            "Speed: 2.6ms preprocess, 22.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.7ms preprocess, 25.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.1ms\n",
            "Speed: 2.6ms preprocess, 26.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.0ms preprocess, 25.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.8ms preprocess, 25.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.0ms\n",
            "Speed: 3.3ms preprocess, 27.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.7ms preprocess, 25.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.7ms preprocess, 25.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_447.jpg\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.7ms\n",
            "Speed: 2.7ms preprocess, 27.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.0ms\n",
            "Speed: 2.5ms preprocess, 26.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 4.8ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.7ms preprocess, 25.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 2.7ms preprocess, 25.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.3ms preprocess, 25.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 25.3ms\n",
            "Speed: 2.7ms preprocess, 25.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_596.jpg\n",
            "\n",
            "0: 384x640 1 package, 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 25.3ms\n",
            "Speed: 2.9ms preprocess, 25.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 25.2ms\n",
            "Speed: 2.3ms preprocess, 25.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.0ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.7ms preprocess, 25.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.3ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.2ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.4ms\n",
            "Speed: 2.7ms preprocess, 25.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.4ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_745.jpg\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.1ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.6ms\n",
            "Speed: 2.5ms preprocess, 25.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.0ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.3ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.2ms preprocess, 25.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.7ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.2ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.8ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_894.jpg\n",
            "\n",
            "0: 384x640 (no detections), 25.5ms\n",
            "Speed: 2.5ms preprocess, 25.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.5ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.0ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.2ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.9ms preprocess, 25.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.5ms\n",
            "Speed: 2.4ms preprocess, 25.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.7ms preprocess, 25.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.1ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.6ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.1ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_1043.jpg\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.7ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.7ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.5ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.5ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.5ms preprocess, 25.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.5ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.8ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.6ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.9ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_1192.jpg\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.4ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.5ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.5ms\n",
            "Speed: 2.6ms preprocess, 25.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.3ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.8ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.8ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.2ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_1341.jpg\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.3ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.1ms preprocess, 25.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.1ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.8ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.5ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.7ms preprocess, 25.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "無法讀取影像，可能是影片結束了。\n",
            "總共處理了 1469 張frame。\n",
            "影片已儲存到 /content/inference_output.mp4\n",
            "\n",
            "===== 最後累積推論結果 =====\n",
            "bag: 1 次\n",
            "package: 4 次\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/package_person_detection.zip /content/package_person_detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SJDJBOqVkTDh",
        "outputId": "1d0580ae-a30c-492d-9960-20084a167116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/package_person_detection/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/confusion_matrix_normalized.png (deflated 22%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/F1_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/val_batch1_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/val_batch2_pred.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/val_batch1_pred.jpg (deflated 6%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/PR_curve.png (deflated 13%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/R_curve.png (deflated 9%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/confusion_matrix.png (deflated 26%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/val_batch2_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/P_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/val_batch0_labels.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged32/val_batch0_pred.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/confusion_matrix_normalized.png (deflated 22%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/F1_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/val_batch1_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/val_batch2_pred.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/val_batch1_pred.jpg (deflated 6%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/PR_curve.png (deflated 13%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/R_curve.png (deflated 9%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/confusion_matrix.png (deflated 26%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/val_batch2_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/P_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/val_batch0_labels.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged35/val_batch0_pred.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/confusion_matrix_normalized.png (deflated 22%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/F1_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/val_batch1_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/val_batch2_pred.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/val_batch1_pred.jpg (deflated 6%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/PR_curve.png (deflated 13%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/R_curve.png (deflated 9%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/confusion_matrix.png (deflated 26%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/val_batch2_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/P_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/val_batch0_labels.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged34/val_batch0_pred.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/confusion_matrix_normalized.png (deflated 22%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/F1_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/val_batch1_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/val_batch2_pred.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/val_batch1_pred.jpg (deflated 6%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/PR_curve.png (deflated 13%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/R_curve.png (deflated 9%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/confusion_matrix.png (deflated 26%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/val_batch2_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/P_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/val_batch0_labels.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged36/val_batch0_pred.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged/args.yaml (deflated 53%)\n",
            "  adding: content/package_person_detection/yolov8m_merged/weights/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/confusion_matrix_normalized.png (deflated 22%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/results.csv (deflated 61%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/F1_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/args.yaml (deflated 53%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best.onnx (deflated 17%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best.pt (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best.mlpackage/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best.mlpackage/Manifest.json (deflated 59%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best.mlpackage/Data/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best.mlpackage/Data/com.apple.CoreML/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best.mlpackage/Data/com.apple.CoreML/weights/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best.mlpackage/Data/com.apple.CoreML/weights/weight.bin (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best.mlpackage/Data/com.apple.CoreML/model.mlmodel (deflated 89%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best.engine (deflated 10%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best_saved_model/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best_saved_model/variables/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best_saved_model/variables/variables.data-00000-of-00001 (deflated 86%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best_saved_model/variables/variables.index (deflated 33%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best_saved_model/metadata.yaml (deflated 37%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best_saved_model/saved_model.pb (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best_saved_model/best_float16.tflite (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best_saved_model/assets/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best_saved_model/fingerprint.pb (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/best_saved_model/best_float32.tflite (deflated 17%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/weights/last.pt (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/results.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/train_batch0.jpg (deflated 6%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/train_batch1.jpg (deflated 3%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/val_batch1_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/val_batch2_pred.jpg (deflated 6%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/labels.jpg (deflated 20%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/val_batch1_pred.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/PR_curve.png (deflated 13%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/R_curve.png (deflated 9%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/train_batch2.jpg (deflated 4%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/confusion_matrix.png (deflated 26%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/val_batch2_labels.jpg (deflated 6%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/labels_correlogram.jpg (deflated 35%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/P_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/val_batch0_labels.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged3/val_batch0_pred.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/confusion_matrix_normalized.png (deflated 22%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/F1_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/val_batch1_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/val_batch2_pred.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/val_batch1_pred.jpg (deflated 6%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/PR_curve.png (deflated 13%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/R_curve.png (deflated 9%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/confusion_matrix.png (deflated 26%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/val_batch2_labels.jpg (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/P_curve.png (deflated 7%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/val_batch0_labels.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged33/val_batch0_pred.jpg (deflated 8%)\n",
            "  adding: content/package_person_detection/yolov8m_merged2/ (stored 0%)\n",
            "  adding: content/package_person_detection/yolov8m_merged2/args.yaml (deflated 53%)\n",
            "  adding: content/package_person_detection/yolov8m_merged2/weights/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/package_person_detection.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Uhda2yUDkhP1",
        "outputId": "1051d912-1a5d-4d6d-f354-a91b81913b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d60ec78-4fc4-4213-8c61-d6d7ae079251\", \"package_person_detection.zip\", 606348627)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k3Hw0mIhGuO"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNgqV9aN/y2C354ilzEwjb9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}