{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Object Tracking & Intent Analysis (v7 - Interaction Distance, Full Annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BoxMOT and create_tracker imported successfully.\n"
          ]
        }
      ],
      "source": [
        "# Install Ultralytics (YOLOv8) and BoxMOT\n",
        "!pip install ultralytics --quiet\n",
        "!pip install boxmot --quiet\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import yaml\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, deque\n",
        "from ultralytics import YOLO\n",
        "import datetime\n",
        "# from google.colab.patches import cv2_imshow # Usually not needed if running locally or if cv2.imshow works\n",
        "\n",
        "# Define DummyTracker globally for fallback\n",
        "\n",
        "class DummyTracker:\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        print(\"Initialized DummyTracker for BoxMOT fallback.\")\n",
        "        self.frame_id = 0\n",
        "    def update(self, dets, img): # img argument is often expected by trackers\n",
        "        self.frame_id += 1\n",
        "        if dets is None or len(dets) == 0:\n",
        "            return np.empty((0, 7)) # BoxMOT ByteTrack returns 7 columns: x1,y1,x2,y2,id,cls,conf\n",
        "        \n",
        "        if hasattr(dets, 'cpu') and not isinstance(dets, np.ndarray):\n",
        "            dets_np = dets.cpu().numpy()\n",
        "        else:\n",
        "            dets_np = dets\n",
        "            \n",
        "        fake_tracks = []\n",
        "        for i, det_row in enumerate(dets_np):\n",
        "            x1, y1, x2, y2 = det_row[:4]\n",
        "            conf = det_row[4] if len(det_row) > 4 else 0.5 \n",
        "            cls = det_row[5] if len(det_row) > 5 else (i % 5) # Placeholder class\n",
        "            fake_tracks.append([x1, y1, x2, y2, self.frame_id * 1000 + i, cls, conf])\n",
        "        return np.array(fake_tracks)\n",
        "\n",
        "\n",
        "# Attempt to import BoxMOT and its utilities\n",
        "try:\n",
        "    from boxmot import create_tracker\n",
        "    from boxmot.utils import TRACKER_CONFIGS\n",
        "    print(\"BoxMOT and create_tracker imported successfully.\")\n",
        "    if TRACKER_CONFIGS is None:\n",
        "        print(\"WARNING: boxmot.utils.TRACKER_CONFIGS is None. main() will attempt to find config path manually.\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR: Failed to import BoxMOT: {e}\")\n",
        "    print(\"Please ensure BoxMOT is installed: pip install boxmot\")\n",
        "    print(\"WARNING: BoxMOT not available. Real tracking will not work. Falling back to DummyTracker.\")\n",
        "    create_tracker = lambda *args, **kwargs: DummyTracker(*args, **kwargs)\n",
        "    TRACKER_CONFIGS = Path(\"dummy_boxmot_configs\") # Relative path for dummy config\n",
        "    if not TRACKER_CONFIGS.exists():\n",
        "        TRACKER_CONFIGS.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration Parameters & Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration & Global Variables ---\n",
        "# ROI_MODE: 0 = Manual ROI, 1 = Dynamic ROI from frame margin\n",
        "ROI_MODE = 1 \n",
        "ROI_MARGIN_PIXELS = 10 # Margin in pixels for dynamic ROI (if ROI_MODE = 1)\n",
        "MANUAL_ROI = (100, 100, 500, 400) # Manual ROI: (x1, y1, x2, y2) or None (if ROI_MODE = 0)\n",
        "\n",
        "# Behavior analysis parameters\n",
        "LOITERING_THRESHOLD_SEC = 5\n",
        "INTERACTION_PROXIMITY_THRESHOLD = 70\n",
        "TRACK_HISTORY_LENGTH = 60 # Frames of history for behavior analysis\n",
        "EVENT_LOG_FILE = \"event_log_boxmot_v7.json\" # Relative path\n",
        "\n",
        "# Event Media Saving Parameters\n",
        "ENABLE_EVENT_CLIPS = True\n",
        "ENABLE_EVENT_SNAPSHOTS = True\n",
        "# EVENT_CLIP_OUTPUT_DIR = \"event_clips\" # Relative path, will be created if not exists\n",
        "# EVENT_SNAPSHOT_OUTPUT_DIR = \"event_snapshots\" # Relative path, will be created if not exists\n",
        "LOITERING_EVENT_CLIP_PRE_BUFFER_SEC = 2\n",
        "LOITERING_EVENT_CLIP_POST_BUFFER_SEC = 1 # Per user request, loitering clip ends at event time\n",
        "INSTANT_EVENT_CLIP_TOTAL_DURATION_SEC = 10 # Centered around event time (5s before, 5s after)\n",
        "FRAME_BUFFER_DURATION_SEC = 15 # Max duration of ANNOTATED frames to keep in memory\n",
        "\n",
        "# Global data structures\n",
        "ROI = None\n",
        "track_history = defaultdict(lambda: deque(maxlen=TRACK_HISTORY_LENGTH))\n",
        "object_loitering_start_time = defaultdict(lambda: None)\n",
        "event_log = [] # Will be populated by log_event, and saved at the end\n",
        "\n",
        "# Frame buffer for video clip saving (stores (ANNOTATED_frame_copy, timestamp))\n",
        "frame_buffer = deque()\n",
        "active_clip_capture_tasks = []\n",
        "\n",
        "# Statistics counters\n",
        "total_frames_read_count = 0\n",
        "total_frames_processed_count = 0\n",
        "cumulative_detected_class_counts = defaultdict(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b6c3cf21",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▍Path initialised\n",
            " RUN_DIR                  : output\\dali_cam2_0519am_otherman\\20250522_091346\n",
            " EVENT_CLIP_OUTPUT_DIR    : output\\dali_cam2_0519am_otherman\\20250522_091346\\clips\n",
            " EVENT_SNAPSHOT_OUTPUT_DIR: output\\dali_cam2_0519am_otherman\\20250522_091346\\snapshots\n",
            " EVENT_LOG_FILE           : output\\dali_cam2_0519am_otherman\\20250522_091346\\dali_cam2_0519am_otherman_events.json\n"
          ]
        }
      ],
      "source": [
        "# ===== Path Helper (放在 Notebook 最前面即可) =====================\n",
        "# from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# ▶ 修改這兩行就能切換資料來源與輸出根目錄\n",
        "INPUT_SOURCE = \"dali_cam2_0519am_otherman.mp4\"      # 或 RTSP/HTTP URL\n",
        "OUTPUT_ROOT  = Path(\"output\")               # 建議集中管理\n",
        "\n",
        "def init_paths(input_path: str | Path, add_timestamp: bool = True):\n",
        "    \"\"\"依輸入檔名自動建立版本化輸出目錄與全域變數。\"\"\"\n",
        "    global RUN_NAME, RUN_DIR, EVENT_LOG_FILE\n",
        "    global EVENT_CLIP_OUTPUT_DIR, EVENT_SNAPSHOT_OUTPUT_DIR\n",
        "\n",
        "    input_path = Path(str(input_path))\n",
        "    RUN_NAME   = input_path.stem\n",
        "    ts_layer   = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if add_timestamp else \"\"\n",
        "    RUN_DIR    = OUTPUT_ROOT / RUN_NAME / ts_layer\n",
        "\n",
        "    EVENT_CLIP_OUTPUT_DIR     = RUN_DIR / \"clips\"\n",
        "    EVENT_SNAPSHOT_OUTPUT_DIR = RUN_DIR / \"snapshots\"\n",
        "    EVENT_LOG_FILE            = RUN_DIR / f\"{RUN_NAME}_events.json\"\n",
        "\n",
        "    # 建立必要目錄\n",
        "    EVENT_CLIP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    EVENT_SNAPSHOT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"▍Path initialised\")\n",
        "    print(\" RUN_DIR                  :\", RUN_DIR)\n",
        "    print(\" EVENT_CLIP_OUTPUT_DIR    :\", EVENT_CLIP_OUTPUT_DIR)\n",
        "    print(\" EVENT_SNAPSHOT_OUTPUT_DIR:\", EVENT_SNAPSHOT_OUTPUT_DIR)\n",
        "    print(\" EVENT_LOG_FILE           :\", EVENT_LOG_FILE)\n",
        "\n",
        "# ★ 呼叫一次，之後整支 Notebook 都能用全域變數\n",
        "init_paths(INPUT_SOURCE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions (Analysis, Drawing, Logging, Media Saving)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_centroid(bbox):\n",
        "    x1, y1, x2, y2 = bbox[:4]\n",
        "    return int((x1 + x2) / 2), int((y1 + y2) / 2)\n",
        "\n",
        "def is_within_roi(centroid, current_roi):\n",
        "    if current_roi is None: return False\n",
        "    cx, cy = centroid; rx1, ry1, rx2, ry2 = current_roi\n",
        "    return rx1 <= cx <= rx2 and ry1 <= cy <= ry2\n",
        "\n",
        "def calculate_distance(p1, p2): return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
        "\n",
        "def save_event_snapshot(annotated_frame, event_type, track_id, event_timestamp):\n",
        "    if annotated_frame is None:\n",
        "        print(\"No frame provided for snapshot.\")\n",
        "        return\n",
        "    try:\n",
        "        snap_dir = Path(EVENT_SNAPSHOT_OUTPUT_DIR)\n",
        "        snap_dir.mkdir(parents=True, exist_ok=True)\n",
        "        ts_str = event_timestamp.strftime(\"%Y%m%d_%H%M%S_%f\")[:-3]\n",
        "        filename_parts = [event_type]\n",
        "        if track_id is not None: filename_parts.append(f\"id{track_id}\")\n",
        "        filename_parts.append(ts_str)\n",
        "        filename = \"_\".join(map(str, filename_parts)) + \".jpg\"\n",
        "        filepath = snap_dir / filename\n",
        "        cv2.imwrite(str(filepath), annotated_frame)\n",
        "        print(f\"Event snapshot saved: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR saving event snapshot: {e}\")\n",
        "\n",
        "def save_video_clip(frames_to_save, output_path_str, fps, frame_width, frame_height):\n",
        "    if not frames_to_save:\n",
        "        print(f\"No frames to save for {output_path_str}.\")\n",
        "        return\n",
        "    output_path = Path(output_path_str)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))\n",
        "    for frame in frames_to_save:\n",
        "        writer.write(frame)\n",
        "    writer.release()\n",
        "    print(f\"Event clip saved: {output_path}\")\n",
        "\n",
        "def log_event(event_data, annotated_frame_for_media):\n",
        "    global event_log, active_clip_capture_tasks, frame_buffer\n",
        "    \n",
        "    # Basic log entry structure from event_data\n",
        "    log_entry = {\n",
        "        \"timestamp\": event_data['event_timestamp'].strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3],\n",
        "        \"event_type\": event_data['event_type'],\n",
        "        \"track_id\": int(event_data['track_id']) if event_data.get('track_id') is not None else None,\n",
        "        \"class_name\": event_data.get('class_name'),\n",
        "        \"details\": event_data.get('details') or {}\n",
        "    }\n",
        "    event_log.append(log_entry)\n",
        "\n",
        "    # Save snapshot if enabled, using the provided fully annotated frame\n",
        "    if ENABLE_EVENT_SNAPSHOTS and annotated_frame_for_media is not None:\n",
        "        save_event_snapshot(annotated_frame_for_media, event_data['event_type'], event_data.get('track_id'), event_data['event_timestamp'])\n",
        "\n",
        "    # Create video clip task if enabled\n",
        "    if ENABLE_EVENT_CLIPS:\n",
        "        desired_clip_start_ts, desired_clip_end_ts = None, None\n",
        "        event_timestamp = event_data['event_timestamp']\n",
        "        event_type = event_data['event_type']\n",
        "        track_id = event_data.get('track_id')\n",
        "        details = event_data.get('details', {})\n",
        "        current_fps = event_data['current_fps']\n",
        "        frame_w = event_data['frame_w']\n",
        "        frame_h = event_data['frame_h']\n",
        "\n",
        "        ts_str = event_timestamp.strftime(\"%Y%m%d_%H%M%S_%f\")[:-3]\n",
        "        clip_name_parts = [event_type]\n",
        "\n",
        "        if event_type == \"loitering\":\n",
        "            loiter_start_time = object_loitering_start_time.get(track_id) # Assumes object_loitering_start_time is globally updated\n",
        "            if loiter_start_time:\n",
        "                desired_clip_start_ts = loiter_start_time - datetime.timedelta(seconds=LOITERING_EVENT_CLIP_PRE_BUFFER_SEC)\n",
        "                desired_clip_end_ts = event_timestamp + datetime.timedelta(seconds=LOITERING_EVENT_CLIP_POST_BUFFER_SEC) # POST_BUFFER_SEC is 0\n",
        "                if track_id is not None: clip_name_parts.append(f\"id{track_id}\")\n",
        "        elif event_type in [\"roi_enter\", \"roi_exit\", \"interaction\"]:\n",
        "            half_duration = datetime.timedelta(seconds=INSTANT_EVENT_CLIP_TOTAL_DURATION_SEC / 2)\n",
        "            desired_clip_start_ts = event_timestamp - half_duration\n",
        "            desired_clip_end_ts = event_timestamp + half_duration\n",
        "            if track_id is not None: clip_name_parts.append(f\"id{track_id}\")\n",
        "            if event_type == \"interaction\":\n",
        "                p_id = details.get(\"person_id\"); pkg_id = details.get(\"package_id\")\n",
        "                if p_id is not None: clip_name_parts.append(f\"p{p_id}\")\n",
        "                if pkg_id is not None: clip_name_parts.append(f\"pkg{pkg_id}\")\n",
        "        \n",
        "        if desired_clip_start_ts and desired_clip_end_ts:\n",
        "            clip_name_parts.append(ts_str)\n",
        "            filename = \"_\".join(map(str, clip_name_parts)) + \".mp4\"\n",
        "            output_filepath = Path(EVENT_CLIP_OUTPUT_DIR) / filename\n",
        "\n",
        "            task = {\n",
        "                'log_entry_ts': log_entry['timestamp'], # Use the string timestamp from log_entry\n",
        "                'desired_clip_start_ts': desired_clip_start_ts,\n",
        "                'desired_clip_end_ts': desired_clip_end_ts,\n",
        "                'collected_frames': [],\n",
        "                'output_filename': str(output_filepath),\n",
        "                'fps': current_fps, 'width': frame_w, 'height': frame_h,\n",
        "                'header_printed': False\n",
        "            }\n",
        "            # Pre-fill with ANNOTATED frames already in buffer\n",
        "            for f_in_buf, ts_in_buf in list(frame_buffer):\n",
        "                if ts_in_buf >= desired_clip_start_ts and ts_in_buf <= event_timestamp: # Collect up to current event time\n",
        "                    task['collected_frames'].append((f_in_buf, ts_in_buf))\n",
        "            active_clip_capture_tasks.append(task)\n",
        "\n",
        "def analyze_behavior(track_id, history, current_bbox, class_id, class_name, current_ts, fps_val, current_roi, f_w, f_h):\n",
        "    global object_loitering_start_time\n",
        "    events_to_log = []\n",
        "    if not history or current_roi is None: return events_to_log\n",
        "    centroid = get_centroid(current_bbox)\n",
        "    event_data_template = {'track_id': track_id, 'class_name': class_name, 'current_fps': fps_val, 'frame_w': f_w, 'frame_h': f_h}\n",
        "\n",
        "    if is_within_roi(centroid, current_roi):\n",
        "        if len(history) > 1:\n",
        "            _, prev_cx, prev_cy, _, _ = history[-2] # Timestamp of prev point is history[-2][0]\n",
        "            if not is_within_roi((prev_cx, prev_cy), current_roi):\n",
        "                events_to_log.append({**event_data_template, 'event_timestamp': current_ts, 'event_type': \"roi_enter\", 'details': {\"roi\": current_roi}})\n",
        "                object_loitering_start_time[track_id] = current_ts\n",
        "        elif object_loitering_start_time.get(track_id) is None: # First time in ROI for this track\n",
        "             events_to_log.append({**event_data_template, 'event_timestamp': current_ts, 'event_type': \"roi_enter\", 'details': {\"roi\": current_roi}})\n",
        "             object_loitering_start_time[track_id] = current_ts\n",
        "        \n",
        "        start_time = object_loitering_start_time.get(track_id)\n",
        "        if start_time:\n",
        "            duration = (current_ts - start_time).total_seconds()\n",
        "            if duration > LOITERING_THRESHOLD_SEC:\n",
        "                # Check if a loitering event for this track_id was logged recently to avoid spamming\n",
        "                if not any(e['event_type'] == \"loitering\" and e['track_id'] == track_id and \n",
        "                           abs((current_ts - datetime.datetime.strptime(e[\"timestamp\"], \"%Y-%m-%d %H:%M:%S.%f\")).total_seconds()) < LOITERING_THRESHOLD_SEC * 0.8 \n",
        "                           for e in reversed(event_log[-20:])):\n",
        "                    events_to_log.append({**event_data_template, 'event_timestamp': current_ts, 'event_type': \"loitering\", \n",
        "                                          'details': {\"duration_sec\": round(duration,1), \"roi\": current_roi}})\n",
        "    else: # Object is outside ROI\n",
        "        if object_loitering_start_time.get(track_id) is not None: # Was previously in ROI\n",
        "            events_to_log.append({**event_data_template, 'event_timestamp': current_ts, 'event_type': \"roi_exit\", 'details': {\"roi\": current_roi}})\n",
        "            object_loitering_start_time[track_id] = None # Reset loitering timer\n",
        "    return events_to_log\n",
        "\n",
        "def analyze_interactions_for_frame(trk_objs, current_ts, annotated_frame, cls_names, fps_val, f_w, f_h):\n",
        "    events_to_log = []\n",
        "    persons = [o for o in trk_objs if cls_names.get(int(o[5]),\".\").lower() in [\"person\",\"other_person\",\"delivery_worker\",\"food_delivery\"]]\n",
        "    packages = [o for o in trk_objs if cls_names.get(int(o[5]),\".\").lower() in [\"package\",\"bag\"]]\n",
        "    pairs = set() # To avoid duplicate interaction events for the same pair in the same frame analysis\n",
        "    event_data_template = {'current_fps': fps_val, 'frame_w': f_w, 'frame_h': f_h} # No track_id/class_name here, specific to interaction\n",
        "\n",
        "    for p_idx, p_data in enumerate(persons):\n",
        "        p_id, p_cent, p_cls_id = int(p_data[4]), get_centroid(p_data[:4]), int(p_data[5])\n",
        "        p_cls_name = cls_names.get(p_cls_id, \"Person\")\n",
        "        for pkg_idx, pkg_data in enumerate(packages):\n",
        "            pkg_id, pkg_cent, pkg_cls_id = int(pkg_data[4]), get_centroid(pkg_data[:4]), int(pkg_data[5])\n",
        "            pkg_cls_name = cls_names.get(pkg_cls_id, \"Package\")\n",
        "            \n",
        "            dist = calculate_distance(p_cent, pkg_cent)\n",
        "            if dist < INTERACTION_PROXIMITY_THRESHOLD:\n",
        "                # Ensure unique pair (person_id, package_id) to avoid duplicate logging if order changes\n",
        "                current_pair = tuple(sorted((p_id, pkg_id)))\n",
        "                if current_pair not in pairs:\n",
        "                    # Check if this specific interaction was logged very recently\n",
        "                    if not any(e['event_type'] == \"interaction\" and \n",
        "                               e['details'].get(\"person_id\") == p_id and \n",
        "                               e['details'].get(\"package_id\") == pkg_id and \n",
        "                               abs((current_ts - datetime.datetime.strptime(e[\"timestamp\"], \"%Y-%m-%d %H:%M:%S.%f\")).total_seconds()) < 5 # Cooldown for same pair\n",
        "                               for e in reversed(event_log[-30:])):\n",
        "                        \n",
        "                        interaction_details = {\n",
        "                            \"person_id\": p_id, \"person_class\": p_cls_name,\n",
        "                            \"package_id\": pkg_id, \"package_class\": pkg_cls_name,\n",
        "                            \"distance\": round(dist,1)\n",
        "                        }\n",
        "                        events_to_log.append({**event_data_template, 'event_timestamp': current_ts, 'event_type': \"interaction\", \n",
        "                                              'track_id': None, 'class_name': None, # Interaction is between two objects\n",
        "                                              'details': interaction_details})\n",
        "                        pairs.add(current_pair)\n",
        "                        \n",
        "                        # Draw interaction annotation (distance and line) onto the frame\n",
        "                        mid_x = int((p_cent[0] + pkg_cent[0]) / 2)\n",
        "                        mid_y = int((p_cent[1] + pkg_cent[1]) / 2)\n",
        "                        interaction_text = f\"Interaction Dist: {dist:.1f}px\"\n",
        "                        cv2.line(annotated_frame, p_cent, pkg_cent, (0, 255, 255), 2) # Yellow line\n",
        "                        cv2.putText(annotated_frame, interaction_text, (mid_x - 50, mid_y - 10), \n",
        "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2) # Yellow text\n",
        "    return events_to_log\n",
        "\n",
        "def draw_tracked_objects_and_stats(frame_to_draw_on, trk_objs, cls_names, current_roi):\n",
        "    # This function now MODIFIES frame_to_draw_on IN PLACE\n",
        "    global total_frames_read_count, total_frames_processed_count, cumulative_detected_class_counts\n",
        "    if current_roi: cv2.rectangle(frame_to_draw_on, (current_roi[0], current_roi[1]), (current_roi[2], current_roi[3]), (255,255,0),2); cv2.putText(frame_to_draw_on,\"ROI\",(current_roi[0],current_roi[1]-10),cv2.FONT_HERSHEY_SIMPLEX,0.7,(255,255,0),2)\n",
        "    for o in trk_objs:\n",
        "        if len(o)==7:\n",
        "            x1, y1, x2, y2, tid, cid, scr = map(float, o)\n",
        "            x1, y1, x2, y2, tid, cid = int(x1), int(y1), int(x2), int(y2), int(tid), int(cid)\n",
        "            cname = cls_names.get(cid, \"Unk\")\n",
        "            clr = get_color_by_id(tid)\n",
        "            cv2.rectangle(frame_to_draw_on, (x1, y1), (x2, y2), clr, 2)\n",
        "            lbl = f\"ID:{tid} {cname} {scr:.2f}\"\n",
        "            (lw, lh), bl = cv2.getTextSize(lbl, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "            ly = max(lh + 5, y1 - 5)\n",
        "            lx = x1\n",
        "            cv2.rectangle(frame_to_draw_on, (lx, ly - lh - bl), (lx + lw, ly + bl), clr, cv2.FILLED)\n",
        "            cv2.putText(frame_to_draw_on, lbl, (lx, ly), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
        "            if object_loitering_start_time.get(tid) and isinstance(object_loitering_start_time[tid], datetime.datetime):\n",
        "                dur = (datetime.datetime.now() - object_loitering_start_time[tid]).total_seconds()\n",
        "                cv2.putText(frame_to_draw_on, f\"Loiter:{dur:.1f}s\", (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "    '''\n",
        "    y_off=20; cv2.putText(frame_to_draw_on,f\"Read:{total_frames_read_count}\",(10,y_off),cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,255,0),2); y_off+=25; cv2.putText(frame_to_draw_on,f\"Proc:{total_frames_processed_count}\",(10,y_off),cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,255,0),2); y_off+=25; cv2.putText(frame_to_draw_on,\"Detects:\",(10,y_off),cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,255,0),2);y_off+=20\n",
        "    for cid, cnt in sorted(cumulative_detected_class_counts.items()):\n",
        "        cn = cls_names.get(cid, f\"Cls{cid}\")\n",
        "        cv2.putText(frame_to_draw_on, f\"- {cn}:{cnt}\", (15, y_off), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        y_off += 20\n",
        "        if y_off > frame_to_draw_on.shape[0] - 20:\n",
        "            break\n",
        "    '''\n",
        "    return frame_to_draw_on # Return the modified frame\n",
        "\n",
        "def get_color_by_id(track_id): np.random.seed(track_id); return tuple(np.random.randint(0,255,size=3).tolist())\n",
        "\n",
        "def save_event_log_final(log_data, filepath):\n",
        "    try:\n",
        "        existing_log = []\n",
        "        if os.path.exists(filepath):\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f_in:\n",
        "                try: existing_log = json.load(f_in)\n",
        "                except json.JSONDecodeError: existing_log = []\n",
        "                if not isinstance(existing_log, list): existing_log = []\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f_out:\n",
        "            json.dump(existing_log + log_data, f_out, indent=4, ensure_ascii=False)\n",
        "        print(f\"Event log ({len(log_data)} new entries) appended to: {filepath}\")\n",
        "    except Exception as e: print(f\"ERROR saving event log: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "def main():\n",
        "    global event_log, track_history, object_loitering_start_time, ROI, frame_buffer, active_clip_capture_tasks\n",
        "    global total_frames_read_count, total_frames_processed_count, cumulative_detected_class_counts\n",
        "    \n",
        "    event_log.clear(); track_history.clear(); object_loitering_start_time.clear(); ROI = None\n",
        "    frame_buffer.clear(); active_clip_capture_tasks.clear()\n",
        "    total_frames_read_count = 0; total_frames_processed_count = 0; cumulative_detected_class_counts.clear()\n",
        "\n",
        "    model_path = 'best.pt'\n",
        "    local_video_path = INPUT_SOURCE # Make sure this video exists or provide a new one\n",
        "    output_video_path = RUN_DIR / 'output_tracked_intent_boxmot_v7.mp4'\n",
        "    conf_threshold = 0.3\n",
        "    max_duration_sec = None # Set to None or a large number for full video processing\n",
        "\n",
        "    # Create output directories if they don't exist\n",
        "    if ENABLE_EVENT_CLIPS: Path(EVENT_CLIP_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    if ENABLE_EVENT_SNAPSHOTS: Path(EVENT_SNAPSHOT_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"Loading model: {model_path}\")\n",
        "    if not Path(model_path).exists():\n",
        "        print(f\"Model {model_path} not found. Downloading yolov8n.pt.\")\n",
        "        try: torch.hub.download_url_to_file('https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt', model_path); print(\"yolov8n.pt downloaded.\")\n",
        "        except Exception as e: print(f\"Error downloading default model: {e}. Upload manually.\"); return\n",
        "            \n",
        "    try: model = YOLO(model_path); class_names_dict = model.names; print(f\"Model loaded. Classes: {class_names_dict}\")\n",
        "    except Exception as e: print(f\"ERROR loading YOLO model: {e}\"); return\n",
        "\n",
        "    if not Path(local_video_path).exists():\n",
        "        print(f\"ERROR: Video {local_video_path} not found. Please upload a video named 'sample_video.mp4' or change the path.\"); return\n",
        "\n",
        "    cap = cv2.VideoCapture(local_video_path)\n",
        "    if not cap.isOpened(): print(f\"ERROR: Cannot open video: {local_video_path}\"); return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    print(f\"Video: {frame_width}x{frame_height} @ {fps:.2f} FPS\")\n",
        "    \n",
        "    frame_buffer = deque(maxlen=int(fps * FRAME_BUFFER_DURATION_SEC))\n",
        "    print(f\"Annotated frame buffer size: {frame_buffer.maxlen} frames ({FRAME_BUFFER_DURATION_SEC}s at {fps:.2f} FPS)\")\n",
        "\n",
        "    if ROI_MODE == 1:\n",
        "        if frame_width > 2*ROI_MARGIN_PIXELS and frame_height > 2*ROI_MARGIN_PIXELS:\n",
        "            ROI = (ROI_MARGIN_PIXELS, ROI_MARGIN_PIXELS, frame_width-ROI_MARGIN_PIXELS, frame_height-ROI_MARGIN_PIXELS)\n",
        "        else: ROI = (0,0,frame_width,frame_height); print(\"WARN: Frame too small for margin, using full frame ROI.\")\n",
        "    elif ROI_MODE == 0: ROI = MANUAL_ROI\n",
        "    else: ROI = (0,0,frame_width,frame_height); print(\"WARN: Invalid ROI_MODE, using full frame ROI.\")\n",
        "    if ROI: print(f\"Using ROI: {ROI}\")\n",
        "    else: print(\"No ROI defined (MANUAL_ROI is None and ROI_MODE is not 1 or frame too small). Processing full frame for ROI checks.\")\n",
        "\n",
        "    tracker = None; current_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    try:\n",
        "        tracker_type = 'bytetrack'\n",
        "        tracker_config_path_cand = TRACKER_CONFIGS / (tracker_type + '.yaml') if isinstance(TRACKER_CONFIGS, Path) and (TRACKER_CONFIGS / (tracker_type + '.yaml')).exists() else None\n",
        "        if not tracker_config_path_cand:\n",
        "            import boxmot; pkg_cfg_path = Path(boxmot.__file__).parent/'configs'/(tracker_type+'.yaml')\n",
        "            if pkg_cfg_path.exists(): tracker_config_path_cand = pkg_cfg_path\n",
        "            else: raise FileNotFoundError(f\"BoxMOT config for {tracker_type} not found.\")\n",
        "        print(f\"Using BoxMOT tracker config: {tracker_config_path_cand}\")\n",
        "        tracker = create_tracker(tracker_type, tracker_config_path_cand, None, current_device, False, False)\n",
        "        print(f\"BoxMOT {tracker_type} tracker initialized on {current_device}.\")\n",
        "    except Exception as e: print(f\"ERROR initializing BoxMOT: {e}. Using DummyTracker.\"); tracker = DummyTracker()\n",
        "    if tracker is None: print(\"CRITICAL: Tracker is None. Aborting.\"); return\n",
        "\n",
        "    out_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width,frame_height))\n",
        "    processing_start_time = time.time()\n",
        "    print(\"Starting video processing...\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, original_frame = cap.read()\n",
        "        if not ret: print(\"End of video or read error.\"); break\n",
        "        total_frames_read_count += 1\n",
        "        current_frame_timestamp = datetime.datetime.now()\n",
        "        \n",
        "        annotated_frame = original_frame.copy()\n",
        "\n",
        "        yolo_results = model.predict(original_frame, conf=conf_threshold, verbose=False)\n",
        "        detections_tensor = yolo_results[0].boxes.data\n",
        "        if detections_tensor.numel() > 0:\n",
        "            for cls_id in detections_tensor[:, 5].int().tolist(): cumulative_detected_class_counts[cls_id] += 1\n",
        "\n",
        "        detections_np = np.empty((0,6))\n",
        "        if isinstance(detections_tensor, torch.Tensor) and detections_tensor.numel() > 0:\n",
        "            detections_np = detections_tensor.detach().cpu().numpy().astype(\"float32\")\n",
        "\n",
        "        tracked_dets_np = np.empty((0,7))\n",
        "        if detections_np.shape[0] > 0 or not isinstance(tracker, DummyTracker):\n",
        "            tracked_dets_np = tracker.update(detections_np, original_frame) \n",
        "            total_frames_processed_count +=1\n",
        "        elif isinstance(tracker, DummyTracker):\n",
        "            tracked_dets_np = tracker.update(None, original_frame); total_frames_processed_count +=1\n",
        "        else: total_frames_processed_count +=1\n",
        "\n",
        "        current_tracked_objects_list = []\n",
        "        active_ids_this_frame = set()\n",
        "        all_events_for_this_frame = []\n",
        "\n",
        "        if tracked_dets_np.shape[0] > 0:\n",
        "            for trk_data in tracked_dets_np:\n",
        "                x1,y1,x2,y2,trk_id,conf,cls_id = trk_data[:7] # [x1, y1, x2, y2, track_id, score, cls_id]   # ← score 在第 6 欄\n",
        "                current_tracked_objects_list.append([x1,y1,x2,y2,trk_id,cls_id,conf])\n",
        "                active_ids_this_frame.add(int(trk_id))\n",
        "                trk_centroid = get_centroid(trk_data); trk_class_name = class_names_dict.get(int(cls_id), \"Unknown\")\n",
        "                track_history[int(trk_id)].append((current_frame_timestamp, trk_centroid[0], trk_centroid[1], int(cls_id), conf))\n",
        "                \n",
        "                # Analyze behavior for this object (e.g., loitering, ROI entry/exit)\n",
        "                # Pass annotated_frame here in case analyze_behavior needs to draw (though it currently doesn't)\n",
        "                behavior_events = analyze_behavior(int(trk_id), track_history[int(trk_id)], [x1,y1,x2,y2], int(cls_id), trk_class_name, current_frame_timestamp, fps, ROI, frame_width, frame_height)\n",
        "                all_events_for_this_frame.extend(behavior_events)\n",
        "        \n",
        "        # Analyze interactions between all currently tracked objects for this frame\n",
        "        # This function WILL draw on annotated_frame if interactions occur\n",
        "        interaction_events = analyze_interactions_for_frame(current_tracked_objects_list, current_frame_timestamp, annotated_frame, class_names_dict, fps, frame_width, frame_height)\n",
        "        all_events_for_this_frame.extend(interaction_events)\n",
        "        \n",
        "        # Clean up history for tracks that are no longer active\n",
        "        for inactive_id in list(track_history.keys() - active_ids_this_frame):\n",
        "            if inactive_id in track_history: del track_history[inactive_id]\n",
        "            if inactive_id in object_loitering_start_time: del object_loitering_start_time[inactive_id]\n",
        "\n",
        "        # Draw all general annotations (object boxes, stats, ROI) onto the annotated_frame\n",
        "        # This happens AFTER interaction-specific annotations might have been drawn by analyze_interactions_for_frame\n",
        "        draw_tracked_objects_and_stats(annotated_frame, current_tracked_objects_list, class_names_dict, ROI)\n",
        "\n",
        "        # Now, log all collected events for this frame, using the fully annotated_frame for media\n",
        "        for event_data_item in all_events_for_this_frame:\n",
        "            log_event(event_data_item, annotated_frame_for_media=annotated_frame)\n",
        "\n",
        "        # Add the fully ANNOTATED frame to the buffer for clip saving\n",
        "        frame_buffer.append((annotated_frame.copy(), current_frame_timestamp))\n",
        "\n",
        "        # Write the ANNOTATED frame to the output video\n",
        "        out_writer.write(annotated_frame)\n",
        "\n",
        "        # --- Handle active clip capture tasks (uses ANNOTATED frames from buffer) ---\n",
        "        if ENABLE_EVENT_CLIPS:\n",
        "            remaining_tasks = []\n",
        "            for task in active_clip_capture_tasks:\n",
        "                is_complete = False\n",
        "                # Check if enough frames collected or if it's the end of the video\n",
        "                if task['collected_frames']:\n",
        "                    # Ensure frames are sorted by timestamp before checking end condition\n",
        "                    task['collected_frames'].sort(key=lambda x: x[1])\n",
        "                    last_collected_ts = task['collected_frames'][-1][1]\n",
        "                    if last_collected_ts >= task['desired_clip_end_ts']:\n",
        "                        is_complete = True\n",
        "                \n",
        "                # If processing has ended (not ret) and task has frames, consider it complete for saving\n",
        "                if (not ret and task['collected_frames']) or is_complete:\n",
        "                    frames_data_to_save = [f_data for f_data, ts_data in task['collected_frames'] \n",
        "                                           if ts_data >= task['desired_clip_start_ts'] and ts_data <= task['desired_clip_end_ts']]\n",
        "                    if frames_data_to_save:\n",
        "                         save_video_clip(frames_data_to_save, task['output_filename'], task['fps'], task['width'], task['height'])\n",
        "                    # Mark as processed by not adding to remaining_tasks\n",
        "                else:\n",
        "                    # If not complete, keep collecting frames if current frame is within desired range\n",
        "                    if current_frame_timestamp <= task['desired_clip_end_ts']:\n",
        "                         # Only add if current frame is relevant to this task's time window\n",
        "                         if current_frame_timestamp >= task['desired_clip_start_ts']:\n",
        "                            # Check if frame already added (e.g. from pre-fill)\n",
        "                            if not any(f_ts == current_frame_timestamp for _, f_ts in task['collected_frames']):\n",
        "                                task['collected_frames'].append((annotated_frame.copy(), current_frame_timestamp))\n",
        "                    remaining_tasks.append(task)\n",
        "            active_clip_capture_tasks = remaining_tasks\n",
        "\n",
        "        if max_duration_sec and (time.time() - processing_start_time > max_duration_sec): print(f\"Max duration {max_duration_sec}s reached.\"); break\n",
        "        if total_frames_read_count % 100 == 0: print(f\"Processed {total_frames_read_count} frames... {len(active_clip_capture_tasks)} active clip tasks.\")\n",
        "\n",
        "    # After loop, process any remaining clip tasks\n",
        "    if ENABLE_EVENT_CLIPS:\n",
        "        for task in active_clip_capture_tasks:\n",
        "            if task['collected_frames']:\n",
        "                task['collected_frames'].sort(key=lambda x: x[1]) # Sort before final save\n",
        "                frames_data_to_save = [f_data for f_data, ts_data in task['collected_frames'] \n",
        "                                       if ts_data >= task['desired_clip_start_ts'] and ts_data <= task['desired_clip_end_ts']]\n",
        "                if frames_data_to_save:\n",
        "                    save_video_clip(frames_data_to_save, task['output_filename'], task['fps'], task['width'], task['height'])\n",
        "        active_clip_capture_tasks.clear()\n",
        "\n",
        "    cap.release(); out_writer.release()\n",
        "    print(f\"Processing finished. Output video: {output_video_path}\")\n",
        "    save_event_log_final(event_log, EVENT_LOG_FILE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Main Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: best.pt\n",
            "Model loaded. Classes: {0: 'package', 1: 'bag', 2: 'other_person', 3: 'delivery_worker', 4: 'food_delivery'}\n",
            "Video: 720x480 @ 23.98 FPS\n",
            "Annotated frame buffer size: 359 frames (15s at 23.98 FPS)\n",
            "Using ROI: (10, 10, 710, 470)\n",
            "Using BoxMOT tracker config: C:\\Users\\user\\venv_tracker\\Lib\\site-packages\\boxmot\\configs\\bytetrack.yaml\n",
            "BoxMOT bytetrack tracker initialized on cpu.\n",
            "Starting video processing...\n",
            "Event snapshot saved: output\\dali_cam2_0519am_otherman\\20250522_091346\\snapshots\\roi_enter_id1_20250522_091441_278.jpg\n",
            "Event snapshot saved: output\\dali_cam2_0519am_otherman\\20250522_091346\\snapshots\\roi_enter_id2_20250522_091442_878.jpg\n",
            "Processed 100 frames... 2 active clip tasks.\n",
            "Processed 200 frames... 2 active clip tasks.\n",
            "Processed 300 frames... 2 active clip tasks.\n",
            "Processed 400 frames... 2 active clip tasks.\n",
            "Processed 500 frames... 2 active clip tasks.\n",
            "Processed 600 frames... 2 active clip tasks.\n",
            "Processed 700 frames... 2 active clip tasks.\n",
            "Processed 800 frames... 2 active clip tasks.\n",
            "Processed 900 frames... 2 active clip tasks.\n",
            "Processed 1000 frames... 2 active clip tasks.\n",
            "Processed 1100 frames... 2 active clip tasks.\n",
            "Processed 1200 frames... 2 active clip tasks.\n",
            "Processed 1300 frames... 2 active clip tasks.\n",
            "Event snapshot saved: output\\dali_cam2_0519am_otherman\\20250522_091346\\snapshots\\roi_enter_id3_20250522_092630_490.jpg\n",
            "Processed 1400 frames... 3 active clip tasks.\n",
            "Processed 1500 frames... 3 active clip tasks.\n",
            "Processed 1600 frames... 3 active clip tasks.\n",
            "Processed 1700 frames... 3 active clip tasks.\n",
            "Processed 1800 frames... 3 active clip tasks.\n",
            "End of video or read error.\n",
            "Event clip saved: output\\dali_cam2_0519am_otherman\\20250522_091346\\clips\\roi_enter_id1_20250522_091441_278.mp4\n",
            "Event clip saved: output\\dali_cam2_0519am_otherman\\20250522_091346\\clips\\roi_enter_id2_20250522_091442_878.mp4\n",
            "Event clip saved: output\\dali_cam2_0519am_otherman\\20250522_091346\\clips\\roi_enter_id3_20250522_092630_490.mp4\n",
            "Processing finished. Output video: output\\dali_cam2_0519am_otherman\\20250522_091346\\output_tracked_intent_boxmot_v7.mp4\n",
            "Event log (3 new entries) appended to: output\\dali_cam2_0519am_otherman\\20250522_091346\\dali_cam2_0519am_otherman_events.json\n"
          ]
        }
      ],
      "source": [
        "# Run the main processing function\n",
        "# Make sure you have uploaded 'sample_video.mp4' and optionally 'yolov8n.pt' (if not auto-downloaded).\n",
        "# Event clips will be saved in 'event_clips/', snapshots in 'event_snapshots/' (relative to notebook execution).\n",
        "# You might need to create 'sample_video.mp4' or change 'local_video_path' in Cell 4.\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review Event Logs, Clips, and Snapshots (Illustrative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Event Log (output\\dali_cam2_0519am_otherman\\20250522_091346\\dali_cam2_0519am_otherman_events.json) Status ---\n",
            "Found 3 events in output\\dali_cam2_0519am_otherman\\20250522_091346\\dali_cam2_0519am_otherman_events.json.\n",
            "\n",
            "=== Object Class Distribution ===\n",
            " Object Class  Count\n",
            " other_person      2\n",
            "food_delivery      1\n",
            "\n",
            "=== Event Type Distribution ===\n",
            "Event Type  Count\n",
            " roi_enter      3\n",
            "\n",
            "=== Object × Event Crosstab ===\n",
            "               roi_enter\n",
            "other_person           2\n",
            "food_delivery          1\n",
            "\n",
            "--- Event Clips (output\\dali_cam2_0519am_otherman\\20250522_091346\\clips/) Status ---\n",
            "Found 3 video clips in output\\dali_cam2_0519am_otherman\\20250522_091346\\clips.\n",
            "\n",
            "--- Event Snapshots (output\\dali_cam2_0519am_otherman\\20250522_091346\\snapshots/) Status ---\n",
            "Found 3 snapshots in output\\dali_cam2_0519am_otherman\\20250522_091346\\snapshots.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd      # ✅ 用來排版統計表\n",
        "\n",
        "# --- Review Event Logs, Clips, and Snapshots (Enhanced) --------------------\n",
        "print(f\"\\n--- Event Log ({EVENT_LOG_FILE}) Status ---\")\n",
        "object_counter   = Counter()                 # 物件類別 → 出現次數\n",
        "event_counter    = Counter()                 # 事件類別 → 出現次數\n",
        "obj_event_matrix = defaultdict(Counter)      # 物件類別 → (事件類別 → 次數)\n",
        "\n",
        "if Path(EVENT_LOG_FILE).exists():\n",
        "    try:\n",
        "        with open(EVENT_LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            logged_events_content = json.load(f)\n",
        "\n",
        "        total_events = len(logged_events_content)\n",
        "        print(f\"Found {total_events} events in {EVENT_LOG_FILE}.\")\n",
        "\n",
        "        # ▍統計迴圈\n",
        "        for evt in logged_events_content:\n",
        "            # 1. 事件類別 (可依實際欄位名稱增減備援鍵)\n",
        "            evt_type = evt.get(\"event_type\") or evt.get(\"event\") or evt.get(\"type\") or \"<unknown>\"\n",
        "            event_counter[evt_type] += 1\n",
        "\n",
        "            # 2. 物件類別 (單一字串或 list 皆可)\n",
        "            raw_obj = evt.get(\"class_name\") or evt.get(\"class\") or evt.get(\"object_classes\")\n",
        "            obj_classes = raw_obj if isinstance(raw_obj, list) else [raw_obj or \"<unknown>\"]\n",
        "\n",
        "            for cls in obj_classes:\n",
        "                object_counter[cls] += 1\n",
        "                obj_event_matrix[cls][evt_type] += 1\n",
        "\n",
        "        # ▍輸出統計表 -------------------------------------------------------\n",
        "        obj_df  = (pd.DataFrame(object_counter.items(), columns=[\"Object Class\", \"Count\"])\n",
        "                     .sort_values(\"Count\", ascending=False))\n",
        "        evt_df  = (pd.DataFrame(event_counter.items(),  columns=[\"Event Type\",  \"Count\"])\n",
        "                     .sort_values(\"Count\", ascending=False))\n",
        "        cross_df = (pd.DataFrame(obj_event_matrix).fillna(0).astype(int).T\n",
        "                      .loc[obj_df[\"Object Class\"]])   # 依物件出現頻次排序\n",
        "\n",
        "        print(\"\\n=== Object Class Distribution ===\")\n",
        "        print(obj_df.to_string(index=False))\n",
        "\n",
        "        print(\"\\n=== Event Type Distribution ===\")\n",
        "        print(evt_df.to_string(index=False))\n",
        "\n",
        "        print(\"\\n=== Object × Event Crosstab ===\")\n",
        "        print(cross_df.to_string())\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading event log: {e}\")\n",
        "else:\n",
        "    print(f\"Event log file {EVENT_LOG_FILE} not found.\")\n",
        "\n",
        "# --- Event Clips Status (原樣保留) -----------------------------------------\n",
        "print(f\"\\n--- Event Clips ({EVENT_CLIP_OUTPUT_DIR}/) Status ---\")\n",
        "if Path(EVENT_CLIP_OUTPUT_DIR).is_dir():\n",
        "    clips = list(Path(EVENT_CLIP_OUTPUT_DIR).glob(\"*.mp4\"))\n",
        "    print(f\"Found {len(clips)} video clips in {EVENT_CLIP_OUTPUT_DIR}.\")\n",
        "else:\n",
        "    print(f\"Event clips directory {EVENT_CLIP_OUTPUT_DIR} not found.\")\n",
        "\n",
        "# --- Event Snapshots Status (原樣保留) --------------------------------------\n",
        "print(f\"\\n--- Event Snapshots ({EVENT_SNAPSHOT_OUTPUT_DIR}/) Status ---\")\n",
        "if Path(EVENT_SNAPSHOT_OUTPUT_DIR).is_dir():\n",
        "    snaps = list(Path(EVENT_SNAPSHOT_OUTPUT_DIR).glob(\"*.jpg\"))\n",
        "    print(f\"Found {len(snaps)} snapshots in {EVENT_SNAPSHOT_OUTPUT_DIR}.\")\n",
        "else:\n",
        "    print(f\"Event snapshots directory {EVENT_SNAPSHOT_OUTPUT_DIR} not found.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_tracker",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
