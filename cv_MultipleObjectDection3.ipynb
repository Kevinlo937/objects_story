{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kevinlo937/objects_story/blob/main/cv_MultipleObjectDection3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k3FFkJ-w1_B",
        "outputId": "e6619572-541c-446d-aea0-d5f08d2afb63",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.124-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.124-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.124 ultralytics-thop-2.0.14\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.63-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Downloading roboflow-1.1.63-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.63\n"
          ]
        }
      ],
      "source": [
        "# å®‰è£ Ultralytics YOLOv8\n",
        "!pip install ultralytics\n",
        "\n",
        "# å®‰è£ Roboflow å¥—ä»¶ï¼ˆç”¨æ–¼è³‡æ–™é›†ä¸‹è¼‰ï¼‰\n",
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXjz3PwNXQOT",
        "outputId": "66f197d3-749f-4e9d-a10a-efdc1566e907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨\n",
        "!python -c \"import torch; print(torch.cuda.is_available())\"\n",
        "\n",
        "# æª¢æŸ¥å¯ç”¨çš„ GPU æ•¸é‡\n",
        "!python -c \"import torch; print(torch.cuda.device_count())\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaUfQsdE-7W8",
        "outputId": "d8083ea9-972c-4f8b-ae29-1e2cbb53b27b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "é–‹å§‹ä¸‹è¼‰è³‡æ–™é›†...\n",
            "æ­£åœ¨ä¸‹è¼‰: package-detection/package-at-front-door v2\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in package-at-front-door-2 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31292/31292 [00:00<00:00, 43875.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to package-at-front-door-2 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2598/2598 [00:00<00:00, 10772.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸‹è¼‰å®Œæˆ: /content/package-at-front-door-2\n",
            "æ­£åœ¨ä¸‹è¼‰: findluggage/find-luggage v1\n",
            "\rloading Roboflow workspace...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in find-luggage-1 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2852/2852 [00:00<00:00, 19268.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to find-luggage-1 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [00:00<00:00, 10676.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸‹è¼‰å®Œæˆ: /content/find-luggage-1\n",
            "æ­£åœ¨ä¸‹è¼‰: project-ii/person-7y27w v2\n",
            "\rloading Roboflow workspace...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in person-2 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8413/8413 [00:00<00:00, 19105.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to person-2 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 482/482 [00:00<00:00, 8820.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸‹è¼‰å®Œæˆ: /content/person-2\n",
            "æ­£åœ¨ä¸‹è¼‰: kevinlo937/deliverman v1\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in deliverman-1 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10326/10326 [00:00<00:00, 24191.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to deliverman-1 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 328/328 [00:00<00:00, 6198.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸‹è¼‰å®Œæˆ: /content/deliverman-1\n",
            "æ­£åœ¨ä¸‹è¼‰: kevinlo937/food_deliverman v1\n",
            "\rloading Roboflow workspace...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in food_deliverman-1 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4262/4262 [00:00<00:00, 11596.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to food_deliverman-1 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [00:00<00:00, 6921.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸‹è¼‰å®Œæˆ: /content/food_deliverman-1\n",
            "æ‰€æœ‰è³‡æ–™é›†ä¸‹è¼‰å®Œæˆã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "ä¿®æ­£å¾Œçš„è³‡æ–™é›†åˆä½µèˆ‡è¨“ç·´è…³æœ¬\n",
        "\n",
        "ä¿®æ­£äº†åŸå§‹è…³æœ¬ä¸­æœªè™•ç†é¡åˆ¥ç´¢å¼•æ˜ å°„çš„å•é¡Œï¼Œç¢ºä¿åˆä½µå¾Œçš„å¤šå€‹è³‡æ–™é›†\n",
        "èƒ½å¤ æ­£ç¢ºè¨“ç·´å‡ºå¤šé¡åˆ¥ç‰©ä»¶åµæ¸¬æ¨¡å‹ã€‚\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import roboflow\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# --- 1. ç’°å¢ƒè¨­å®šèˆ‡è³‡æ–™é›†ä¸‹è¼‰ ---\n",
        "\n",
        "# è¨­å®š Roboflow API Key (è«‹æ›¿æ›æˆæ‚¨çš„é‡‘é‘°)\n",
        "API_KEY = \"NKaOxmYYLFRPaf6w0uF6\" # è«‹å‹™å¿…æ›¿æ›æˆæ‚¨çš„æœ‰æ•ˆé‡‘é‘°\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = API_KEY\n",
        "\n",
        "rf = roboflow.Roboflow(api_key=API_KEY)\n",
        "\n",
        "# å®šç¾©è¦ä¸‹è¼‰çš„è³‡æ–™é›†åŠå…¶ç›®æ¨™é¡åˆ¥åç¨±\n",
        "# æ ¼å¼: (workspace, project, version, {original_class_name: target_class_name})\n",
        "datasets_to_download = [\n",
        "    (\"package-detection\", \"package-at-front-door\", 2, {\"package\": \"package\"}),\n",
        "    (\"findluggage\", \"find-luggage\", 1, {\"paper bag\": \"bag\"}), # æ ¹æ“šéœ€è¦å–æ¶ˆè¨»è§£\n",
        "    (\"project-ii\", \"person-7y27w\", 2, {\"person\": \"other_person\"}),\n",
        "    (\"kevinlo937\", \"deliverman\", 1, {\"mailman\": \"delivery_worker\"}),\n",
        "    (\"kevinlo937\", \"food_deliverman\", 1, {\"Foodpenda\": \"food_delivery\"})\n",
        "]\n",
        "\n",
        "# çµ±ä¸€å®šç¾©æœ€çµ‚åˆä½µè³‡æ–™é›†çš„é¡åˆ¥åç¨±å’Œç´¢å¼•\n",
        "# **é€™æ˜¯é—œéµæ­¥é©Ÿï¼Œç¢ºä¿æ‰€æœ‰ä¾†æºçš„é¡åˆ¥è¢«æ˜ å°„åˆ°ä¸€è‡´çš„ç´¢å¼•**\n",
        "UNIFIED_CLASSES = {\n",
        "    \"package\": 0,          # åŒ…è£¹ (ä¾†è‡ª package-at-front-door)\n",
        "    \"bag\": 1,              # æè¢‹ (å¦‚æœä½¿ç”¨ find-luggage)\n",
        "    \"other_person\": 2,     # å…¶ä»–äººå“¡ (ä¾†è‡ª person-7y27w)\n",
        "    \"delivery_worker\": 3,  # éƒµå·®å¿«éå“¡ (ä¾†è‡ª deliverman)\n",
        "    \"food_delivery\": 4     # é€é¤å“¡ (ä¾†è‡ª food_deliverman)\n",
        "}\n",
        "# åå‘æ˜ å°„ï¼Œæ–¹ä¾¿æŸ¥æ‰¾åç¨±\n",
        "UNIFIED_CLASS_NAMES = {v: k for k, v in UNIFIED_CLASSES.items()}\n",
        "\n",
        "print(\"é–‹å§‹ä¸‹è¼‰è³‡æ–™é›†...\")\n",
        "downloaded_datasets_info = []\n",
        "for workspace, project, version, class_mapping in datasets_to_download:\n",
        "    try:\n",
        "        print(f\"æ­£åœ¨ä¸‹è¼‰: {workspace}/{project} v{version}\")\n",
        "        dataset = rf.workspace(workspace).project(project).version(version).download(\"yolov8\")\n",
        "        downloaded_datasets_info.append({\n",
        "            \"location\": dataset.location,\n",
        "            \"original_yaml\": Path(dataset.location) / \"data.yaml\",\n",
        "            \"class_mapping\": class_mapping\n",
        "        })\n",
        "        print(f\"ä¸‹è¼‰å®Œæˆ: {dataset.location}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ä¸‹è¼‰å¤±æ•—: {workspace}/{project} v{version}. éŒ¯èª¤: {e}\")\n",
        "\n",
        "if not downloaded_datasets_info:\n",
        "    print(\"æ²’æœ‰æˆåŠŸä¸‹è¼‰ä»»ä½•è³‡æ–™é›†ï¼Œè…³æœ¬çµ‚æ­¢ã€‚\")\n",
        "    exit()\n",
        "\n",
        "print(\"æ‰€æœ‰è³‡æ–™é›†ä¸‹è¼‰å®Œæˆã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. è³‡æ–™é›†åˆä½µèˆ‡æ¨™ç±¤é‡æ–°æ˜ å°„ ---\n",
        "\n",
        "print(\"é–‹å§‹åˆä½µè³‡æ–™é›†ä¸¦é‡æ–°æ˜ å°„æ¨™ç±¤...\")\n",
        "merged_dataset_dir = Path(\"merged_dataset\")\n",
        "\n",
        "# å‰µå»ºåˆä½µè³‡æ–™é›†çš„ç›®éŒ„çµæ§‹\n",
        "for split in [\"train\", \"valid\", \"test\"]:\n",
        "    os.makedirs(merged_dataset_dir / split / \"images\", exist_ok=True)\n",
        "    os.makedirs(merged_dataset_dir / split / \"labels\", exist_ok=True)\n",
        "\n",
        "# éæ­·æ¯å€‹ä¸‹è¼‰çš„è³‡æ–™é›†é€²è¡Œåˆä½µå’Œæ˜ å°„\n",
        "for dataset_info in downloaded_datasets_info:\n",
        "    dataset_path = Path(dataset_info[\"location\"])\n",
        "    original_yaml_path = dataset_info[\"original_yaml\"]\n",
        "    class_mapping = dataset_info[\"class_mapping\"]\n",
        "\n",
        "    print(f\"è™•ç†è³‡æ–™é›†: {dataset_path.name}\")\n",
        "\n",
        "    # è®€å–åŸå§‹è³‡æ–™é›†çš„ data.yaml ä»¥ç²å–åŸå§‹é¡åˆ¥åç¨±\n",
        "    try:\n",
        "        with open(original_yaml_path, 'r') as f:\n",
        "            original_data_config = yaml.safe_load(f)\n",
        "        original_class_names = original_data_config.get('names', [])\n",
        "        if not original_class_names:\n",
        "            print(f\"è­¦å‘Š: ç„¡æ³•å¾ {original_yaml_path} è®€å–åŸå§‹é¡åˆ¥åç¨±ï¼Œè·³éæ­¤è³‡æ–™é›†ã€‚\")\n",
        "            continue\n",
        "    except Exception as e:\n",
        "        print(f\"éŒ¯èª¤: ç„¡æ³•è®€å–æˆ–è§£æ {original_yaml_path}: {e}ï¼Œè·³éæ­¤è³‡æ–™é›†ã€‚\")\n",
        "        continue\n",
        "\n",
        "    # éæ­· train/valid/test åˆ†å‰²\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        source_images_dir = dataset_path / split / \"images\"\n",
        "        source_labels_dir = dataset_path / split / \"labels\"\n",
        "        target_images_dir = merged_dataset_dir / split / \"images\"\n",
        "        target_labels_dir = merged_dataset_dir / split / \"labels\"\n",
        "\n",
        "        if not source_images_dir.exists() or not source_labels_dir.exists():\n",
        "            print(f\"è­¦å‘Š: è³‡æ–™é›† {dataset_path.name} çš„ {split} åˆ†å‰²ä¸å®Œæ•´ï¼Œè·³éã€‚\")\n",
        "            continue\n",
        "\n",
        "        # è¤‡è£½åœ–ç‰‡æª”æ¡ˆ\n",
        "        for img_file in source_images_dir.glob(\"*.*\"):\n",
        "            try:\n",
        "                shutil.copy(img_file, target_images_dir / img_file.name)\n",
        "            except Exception as e:\n",
        "                print(f\"è¤‡è£½åœ–ç‰‡å¤±æ•—: {img_file} -> {target_images_dir}. éŒ¯èª¤: {e}\")\n",
        "\n",
        "        # è®€å–ã€æ˜ å°„ä¸¦å¯«å…¥æ¨™ç±¤æª”æ¡ˆ\n",
        "        for label_file in source_labels_dir.glob(\"*.txt\"):\n",
        "            new_label_lines = []\n",
        "            try:\n",
        "                with open(label_file, 'r') as f_in:\n",
        "                    lines = f_in.readlines()\n",
        "\n",
        "                for line in lines:\n",
        "                    parts = line.strip().split()\n",
        "                    if not parts:\n",
        "                        continue\n",
        "\n",
        "                    original_class_index = int(parts[0])\n",
        "\n",
        "                    # æª¢æŸ¥åŸå§‹ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ\n",
        "                    if 0 <= original_class_index < len(original_class_names):\n",
        "                        original_class_name = original_class_names[original_class_index]\n",
        "\n",
        "                        # æª¢æŸ¥æ­¤é¡åˆ¥æ˜¯å¦éœ€è¦è¢«æ˜ å°„åˆ°ç›®æ¨™é¡åˆ¥\n",
        "                        if original_class_name in class_mapping:\n",
        "                            target_class_name = class_mapping[original_class_name]\n",
        "\n",
        "                            # æª¢æŸ¥ç›®æ¨™é¡åˆ¥æ˜¯å¦å­˜åœ¨æ–¼çµ±ä¸€å®šç¾©ä¸­\n",
        "                            if target_class_name in UNIFIED_CLASSES:\n",
        "                                target_class_index = UNIFIED_CLASSES[target_class_name]\n",
        "                                # æ›´æ–°æ¨™ç±¤è¡Œï¼Œä½¿ç”¨æ–°çš„çµ±ä¸€ç´¢å¼•\n",
        "                                new_label_lines.append(f\"{target_class_index} {' '.join(parts[1:])}\\n\")\n",
        "                            else:\n",
        "                                print(f\"è­¦å‘Š: ç›®æ¨™é¡åˆ¥ '{target_class_name}' æœªåœ¨ UNIFIED_CLASSES ä¸­å®šç¾©ã€‚ä¾†è‡ªæª”æ¡ˆ: {label_file}\")\n",
        "                        # else: # å¦‚æœåŸå§‹é¡åˆ¥ä¸éœ€è¦æ˜ å°„ï¼Œå¯ä»¥é¸æ“‡å¿½ç•¥æˆ–æ˜ å°„åˆ° 'other'\n",
        "                        #     print(f\"è³‡è¨Š: åŸå§‹é¡åˆ¥ '{original_class_name}' åœ¨ {label_file} ä¸­æœªè¢«æ˜ å°„ï¼Œå·²å¿½ç•¥ã€‚\")\n",
        "                    else:\n",
        "                        print(f\"è­¦å‘Š: åœ¨ {label_file} ä¸­ç™¼ç¾ç„¡æ•ˆçš„åŸå§‹é¡åˆ¥ç´¢å¼•: {original_class_index}\")\n",
        "\n",
        "                # å°‡ä¿®æ”¹å¾Œçš„æ¨™ç±¤å¯«å…¥åˆä½µè³‡æ–™å¤¾\n",
        "                if new_label_lines:\n",
        "                    target_label_file = target_labels_dir / label_file.name\n",
        "                    with open(target_label_file, 'w') as f_out:\n",
        "                        f_out.writelines(new_label_lines)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"è™•ç†æ¨™ç±¤æª”æ¡ˆå¤±æ•—: {label_file}. éŒ¯èª¤: {e}\")\n",
        "\n",
        "print(\"è³‡æ–™é›†åˆä½µèˆ‡æ¨™ç±¤é‡æ–°æ˜ å°„å®Œæˆã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J06XCciKLo0j",
        "outputId": "cbabed89-4999-4ccf-ea26-f2e0d17c1bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "é–‹å§‹åˆä½µè³‡æ–™é›†ä¸¦é‡æ–°æ˜ å°„æ¨™ç±¤...\n",
            "è™•ç†è³‡æ–™é›†: package-at-front-door-2\n",
            "è™•ç†è³‡æ–™é›†: find-luggage-1\n",
            "è™•ç†è³‡æ–™é›†: person-2\n",
            "è™•ç†è³‡æ–™é›†: deliverman-1\n",
            "è™•ç†è³‡æ–™é›†: food_deliverman-1\n",
            "è³‡æ–™é›†åˆä½µèˆ‡æ¨™ç±¤é‡æ–°æ˜ å°„å®Œæˆã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -h -o '^[0-9]' merged_dataset/train/labels/*.txt | sort | uniq -c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmJlz5AAMYXx",
        "outputId": "ed5b7799-ac5b-4cbb-81ce-5eb4be567f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   1905 0\n",
            "     71 1\n",
            "    168 2\n",
            "    135 3\n",
            "     56 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_dataset_dir='/content/merged_dataset'\n",
        "print(merged_dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzmyuu-4PFHJ",
        "outputId": "17de67fa-4541-4f1c-ca6e-1b1c2c0c73c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/merged_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKL8eDtLUj0t",
        "outputId": "ea6488eb-4be8-4e4b-ac4f-188dbb2dca65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å‰µå»ºåˆä½µè³‡æ–™é›†çš„ data.yaml...\n",
            "åˆä½µå¾Œçš„ data.yaml å·²å‰µå»º: /content/merged_dataset/data.yaml\n"
          ]
        }
      ],
      "source": [
        "# --- 3. å‰µå»ºåˆä½µè³‡æ–™é›†çš„ data.yaml ---\n",
        "\n",
        "print(\"å‰µå»ºåˆä½µè³‡æ–™é›†çš„ data.yaml...\")\n",
        "# Convert merged_dataset_dir to a Path object\n",
        "merged_dataset_dir = Path(merged_dataset_dir)\n",
        "\n",
        "merged_data_yaml_content = {\n",
        "    'train': str(merged_dataset_dir / 'train' / 'images'),\n",
        "    'val': str(merged_dataset_dir / 'valid' / 'images'),\n",
        "    'test': str(merged_dataset_dir / 'test' / 'images'),\n",
        "    'nc': len(UNIFIED_CLASSES),\n",
        "    'names': {index: name for name, index in UNIFIED_CLASSES.items()} # ä½¿ç”¨ç´¢å¼•ä½œç‚ºéµ\n",
        "}\n",
        "\n",
        "merged_data_yaml_path = merged_dataset_dir / \"data.yaml\"\n",
        "\n",
        "try:\n",
        "    with open(merged_data_yaml_path, 'w') as f:\n",
        "        yaml.dump(merged_data_yaml_content, f, default_flow_style=False, sort_keys=False)\n",
        "    print(f\"åˆä½µå¾Œçš„ data.yaml å·²å‰µå»º: {merged_data_yaml_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"å‰µå»ºåˆä½µå¾Œçš„ data.yaml å¤±æ•—: {e}\")\n",
        "    exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data_yaml_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WSXNaqjN_od",
        "outputId": "3e59235b-9b69-4103-ecdd-6101aaf4f1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/merged_dataset/data.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3g0VEVrSrl-",
        "outputId": "d08c803f-4463-43e2-83f5-de137e04f279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "é–‹å§‹æ¨¡å‹è¨“ç·´...\n",
            "Ultralytics 8.3.124 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/content/merged_dataset/data.yaml, epochs=30, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=multi_class_detection, name=merge_run4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, cutmix=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=multi_class_detection/merge_run4\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3778591  ultralytics.nn.modules.head.Detect           [5, [192, 384, 576]]          \n",
            "Model summary: 169 layers, 25,859,215 parameters, 25,859,199 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 469/475 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 991.8Â±309.9 MB/s, size: 24.3 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/merged_dataset/train/labels.cache... 1644 images, 3 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1647/1647 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 344.8Â±139.5 MB/s, size: 35.4 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/merged_dataset/valid/labels.cache... 131 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to multi_class_detection/merge_run4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mmulti_class_detection/merge_run4\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/30      5.93G     0.9783      1.824      1.332         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:57<00:00,  1.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.283      0.474      0.385      0.271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/30      6.44G      1.199      1.533      1.473         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:56<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137       0.18      0.462      0.226      0.119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/30       6.5G       1.29      1.643      1.565         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:55<00:00,  1.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.444      0.398      0.278      0.144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/30       6.5G      1.246      1.539      1.513         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.733      0.346      0.452      0.235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/30      6.57G      1.191      1.412      1.481         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.577      0.306      0.379      0.221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/30      7.12G      1.121      1.272      1.427         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.728       0.28      0.336      0.179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/30      7.19G      1.071      1.184       1.38         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.518      0.532      0.638      0.413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/30      7.26G      1.028      1.085      1.346         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.788      0.565       0.76      0.505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/30      7.32G     0.9871      1.046      1.331         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.635      0.719       0.79      0.541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/30      7.32G     0.9575      0.996      1.305         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.539       0.67      0.635      0.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/30      7.32G     0.9324     0.9236      1.294         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.602       0.78      0.689      0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/30      7.32G     0.8966     0.9124       1.27         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137       0.52      0.798      0.733       0.54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/30      7.32G     0.8722     0.8521      1.239         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.915      0.643      0.844        0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/30      7.32G     0.8502      0.802       1.23         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.732      0.865      0.887      0.587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/30      7.32G     0.8242     0.7621      1.205         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.819      0.802      0.868      0.636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/30      7.32G     0.7987     0.7074      1.193         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.699      0.808      0.837      0.603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/30      7.32G     0.7815     0.7324      1.186         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.854      0.832      0.913      0.691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/30      7.32G     0.7567      0.675      1.159         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.919       0.85      0.936      0.731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/30      7.32G     0.7243      0.635      1.146         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.885      0.862      0.923      0.699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/30      7.32G     0.7108     0.6213      1.136         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.871      0.913      0.885      0.654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/30      7.32G     0.6086     0.4823      1.067         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137       0.85      0.895      0.924      0.739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/30      7.32G     0.5938     0.4542      1.048         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:53<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.891      0.908      0.951      0.774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/30      7.32G     0.5456     0.4008      1.029         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:53<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137       0.82       0.89      0.949      0.732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/30      7.32G     0.5412     0.3931      1.014         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:53<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.918      0.811      0.954      0.758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/30      7.32G     0.5271     0.3608      1.002         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:53<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.824      0.917      0.931      0.767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/30      7.32G     0.4914      0.343     0.9822         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:53<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.811      0.939      0.968        0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/30      7.32G     0.4689     0.3212      0.966         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.952      0.919      0.958      0.803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/30      7.32G     0.4554     0.3119     0.9594         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.891      0.888       0.91      0.772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/30      7.32G     0.4338     0.2931     0.9451         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:53<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.899      0.888      0.946      0.821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/30      7.32G     0.4192     0.2869     0.9372         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.917      0.897      0.953      0.819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "30 epochs completed in 0.497 hours.\n",
            "Optimizer stripped from multi_class_detection/merge_run4/weights/last.pt, 52.0MB\n",
            "Optimizer stripped from multi_class_detection/merge_run4/weights/best.pt, 52.0MB\n",
            "\n",
            "Validating multi_class_detection/merge_run4/weights/best.pt...\n",
            "Ultralytics 8.3.124 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 92 layers, 25,842,655 parameters, 0 gradients, 78.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.898      0.888      0.946       0.82\n",
            "               package         47         51      0.907      0.863      0.938      0.761\n",
            "                   bag         19         21      0.992      0.905      0.949      0.905\n",
            "          other_person         47         47          1      0.899      0.995      0.922\n",
            "       delivery_worker         14         14          1      0.773      0.937      0.712\n",
            "         food_delivery          4          4      0.594          1      0.912      0.801\n",
            "Speed: 0.5ms preprocess, 11.0ms inference, 0.0ms loss, 7.2ms postprocess per image\n",
            "Results saved to \u001b[1mmulti_class_detection/merge_run4\u001b[0m\n",
            "æ¨¡å‹è¨“ç·´å®Œæˆã€‚çµæœä¿å­˜åœ¨ 'multi_class_detection/merge_run' è³‡æ–™å¤¾ä¸­ã€‚\n",
            "é–‹å§‹æ¨¡å‹é©—è­‰...\n",
            "Ultralytics 8.3.124 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 92 layers, 25,842,655 parameters, 0 gradients, 78.7 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1401.5Â±469.4 MB/s, size: 39.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/merged_dataset/valid/labels.cache... 131 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:04<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.899      0.888      0.946      0.822\n",
            "               package         47         51      0.907      0.863      0.938      0.766\n",
            "                   bag         19         21      0.992      0.905      0.949      0.905\n",
            "          other_person         47         47          1      0.899      0.995      0.923\n",
            "       delivery_worker         14         14          1      0.771      0.937      0.713\n",
            "         food_delivery          4          4      0.598          1      0.912      0.801\n",
            "Speed: 2.6ms preprocess, 21.4ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
            "Results saved to \u001b[1mmulti_class_detection/merge_run42\u001b[0m\n",
            "æ¨¡å‹é©—è­‰å®Œæˆã€‚\n"
          ]
        }
      ],
      "source": [
        "# --- 4. æ¨¡å‹è¨“ç·´ ---\n",
        "\n",
        "print(\"é–‹å§‹æ¨¡å‹è¨“ç·´...\")\n",
        "\n",
        "# è¼‰å…¥é è¨“ç·´æ¨¡å‹ (ä¾‹å¦‚ yolov8m.pt)\n",
        "model = YOLO('yolov8m.pt')\n",
        "\n",
        "# é–‹å§‹è¨“ç·´\n",
        "try:\n",
        "    results = model.train(\n",
        "        data=str(merged_data_yaml_path), # ä½¿ç”¨ä¿®æ­£å¾Œçš„ data.yaml è·¯å¾‘\n",
        "        epochs=30,  # æ ¹æ“šéœ€è¦èª¿æ•´è¨“ç·´è¼ªæ•¸\n",
        "        imgsz=640,\n",
        "        batch=16,   # æ ¹æ“šæ‚¨çš„ GPU è¨˜æ†¶é«”èª¿æ•´\n",
        "        patience=10,\n",
        "        save=True,\n",
        "        project=\"multi_class_detection\", # å°ˆæ¡ˆåç¨±\n",
        "        name=\"merge_run\",   # è¨“ç·´é‹è¡Œåç¨±\n",
        "        # device=0, # å¦‚æœæœ‰ GPUï¼Œå–æ¶ˆè¨»è§£ä¸¦æŒ‡å®š GPU ç´¢å¼•\n",
        "        # workers=8 # æ ¹æ“šæ‚¨çš„ CPU æ ¸å¿ƒæ•¸èª¿æ•´\n",
        "    )\n",
        "    print(\"æ¨¡å‹è¨“ç·´å®Œæˆã€‚çµæœä¿å­˜åœ¨ 'multi_class_detection/merge_run' è³‡æ–™å¤¾ä¸­ã€‚\")\n",
        "\n",
        "    # (å¯é¸) é€²è¡Œé©—è­‰\n",
        "    print(\"é–‹å§‹æ¨¡å‹é©—è­‰...\")\n",
        "    validation_results = model.val()\n",
        "    print(\"æ¨¡å‹é©—è­‰å®Œæˆã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"æ¨¡å‹è¨“ç·´æˆ–é©—è­‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVXeL9cJJ-8R",
        "outputId": "c49ce072-1e8f-4ff8-b099-ccb22af54f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class package             : \tPrecision = 0.9069, \tRecall = 0.8627\n",
            "Class bag                 : \tPrecision = 0.9921, \tRecall = 0.9048\n",
            "Class other_person        : \tPrecision = 1.0000, \tRecall = 0.8985\n",
            "Class delivery_worker     : \tPrecision = 1.0000, \tRecall = 0.7715\n",
            "Class food_delivery       : \tPrecision = 0.5980, \tRecall = 1.0000\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "# è®€å– data.yaml æª”æ¡ˆä»¥ç²å–é¡åˆ¥åç¨±\n",
        "with open(f\"{merged_dataset_dir}/data.yaml\", 'r') as f:\n",
        "    data = yaml.safe_load(f)\n",
        "class_names = data['names']  # ç²å–é¡åˆ¥åç¨±å­—å…¸\n",
        "\n",
        "# Get precision and recall for each class\n",
        "precision = validation_results.box.p\n",
        "recall = validation_results.box.r\n",
        "\n",
        "# Print precision and recall for each class with names\n",
        "for i, (p, r) in enumerate(zip(precision, recall)):\n",
        "    class_name = class_names[i]  # ç²å–é¡åˆ¥åç¨±\n",
        "    print(f\"Class {class_name:<20}: \\tPrecision = {p:.4f}, \\tRecall = {r:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovKdyEe-g5mJ"
      },
      "source": [
        "## æ¨¡å‹éƒ¨ç½²\n",
        "\n",
        "### 1. æ¨¡å‹å°å‡º"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dThIYydAg8Bv",
        "outputId": "73418b7f-f3d1-421a-d79d-c0356e9ed66d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.124 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'multi_class_detection/merge_run4/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 9, 8400) (49.6 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim>=0.1.46', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxslim>=0.1.46\n",
            "  Downloading onnxslim-0.1.51-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.12.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.12.0) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxslim>=0.1.46) (1.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxslim>=0.1.46) (24.2)\n",
            "Collecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxslim>=0.1.46) (1.3.0)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 16.0/16.0 MB 321.9 MB/s eta 0:00:00\n",
            "Downloading onnxslim-0.1.51-py3-none-any.whl (145 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 145.6/145.6 kB 326.0 MB/s eta 0:00:00\n",
            "Downloading onnxruntime_gpu-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (280.8 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 280.8/280.8 MB 3.6 MB/s eta 0:00:00\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46.0/46.0 kB 201.8 MB/s eta 0:00:00\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 86.8/86.8 kB 306.3 MB/s eta 0:00:00\n",
            "Installing collected packages: onnx, humanfriendly, onnxslim, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-gpu-1.21.1 onnxslim-0.1.51\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 22.0s, installed 3 packages: ['onnx>=1.12.0', 'onnxslim>=0.1.46', 'onnxruntime-gpu']\n",
            "WARNING âš ï¸ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.51...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 45.5s, saved as 'multi_class_detection/merge_run4/weights/best.onnx' (98.7 MB)\n",
            "\n",
            "Export complete (48.8s)\n",
            "Results saved to \u001b[1m/content/multi_class_detection/merge_run4/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=multi_class_detection/merge_run4/weights/best.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=multi_class_detection/merge_run4/weights/best.onnx imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "Ultralytics 8.3.124 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'multi_class_detection/merge_run4/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 9, 8400) (49.6 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.51...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 26.8s, saved as 'multi_class_detection/merge_run4/weights/best.onnx' (98.7 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['tensorrt>7.0.0,!=10.1.0'] not found, attempting AutoUpdate...\n",
            "Collecting tensorrt!=10.1.0,>7.0.0\n",
            "  Downloading tensorrt-10.10.0.31.tar.gz (40 kB)\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40.7/40.7 kB 5.7 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tensorrt_cu12==10.10.0.31 (from tensorrt!=10.1.0,>7.0.0)\n",
            "  Downloading tensorrt_cu12-10.10.0.31.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tensorrt_cu12_libs==10.10.0.31 (from tensorrt_cu12==10.10.0.31->tensorrt!=10.1.0,>7.0.0)\n",
            "  Downloading tensorrt_cu12_libs-10.10.0.31.tar.gz (708 bytes)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting tensorrt_cu12_bindings==10.10.0.31 (from tensorrt_cu12==10.10.0.31->tensorrt!=10.1.0,>7.0.0)\n",
            "  Downloading tensorrt_cu12_bindings-10.10.0.31-cp311-none-manylinux_2_28_x86_64.whl.metadata (607 bytes)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from tensorrt_cu12_libs==10.10.0.31->tensorrt_cu12==10.10.0.31->tensorrt!=10.1.0,>7.0.0) (12.4.127)\n",
            "Downloading tensorrt_cu12_bindings-10.10.0.31-cp311-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.2/1.2 MB 44.6 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: tensorrt, tensorrt_cu12, tensorrt_cu12_libs\n",
            "  Building wheel for tensorrt (setup.py): started\n",
            "  Building wheel for tensorrt (setup.py): finished with status 'done'\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.10.0.31-py2.py3-none-any.whl size=46636 sha256=4dfb630fc7bee795addf6b0fdb2936628a24751886b76f5641f0d9d16e0d7e31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2bqwe2kx/wheels/01/04/6b/680eb64bea852e9183d35f8112879c19d02a37c4a7a3e9ab19\n",
            "  Building wheel for tensorrt_cu12 (setup.py): started\n",
            "  Building wheel for tensorrt_cu12 (setup.py): finished with status 'done'\n",
            "  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.10.0.31-py2.py3-none-any.whl size=17483 sha256=e2dc662e7c73bf9404e3fa793e376e9414041c7d6f5633924c490f520f588e98\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2bqwe2kx/wheels/ef/6b/47/10dbe13e12085c48a6b521b5c62b7c4d27e9192fbc7069e27d\n",
            "  Building wheel for tensorrt_cu12_libs (pyproject.toml): started\n",
            "  Building wheel for tensorrt_cu12_libs (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for tensorrt_cu12_libs: filename=tensorrt_cu12_libs-10.10.0.31-py2.py3-none-manylinux_2_28_x86_64.whl size=3404991480 sha256=02a17ce9d84979d325a085e6492cfb71fae1039d4e7f55efe4de8e96d0410255\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2bqwe2kx/wheels/ff/db/8e/03e6696471217bb7beca639a1f62de8c3b77dac3a53cafb068\n",
            "Successfully built tensorrt tensorrt_cu12 tensorrt_cu12_libs\n",
            "Installing collected packages: tensorrt_cu12_bindings, tensorrt_cu12_libs, tensorrt_cu12, tensorrt\n",
            "Successfully installed tensorrt-10.10.0.31 tensorrt_cu12-10.10.0.31 tensorrt_cu12_bindings-10.10.0.31 tensorrt_cu12_libs-10.10.0.31\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 284.1s, installed 1 package: ['tensorrt>7.0.0,!=10.1.0']\n",
            "WARNING âš ï¸ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(-1, 9, -1) DataType.FLOAT\n",
            "WARNING âš ï¸ \u001b[34m\u001b[1mTensorRT:\u001b[0m 'dynamic=True' model requires max batch size, i.e. 'batch=16'\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as multi_class_detection/merge_run4/weights/best.engine\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success âœ… 429.8s, saved as 'multi_class_detection/merge_run4/weights/best.engine' (127.2 MB)\n",
            "\n",
            "Export complete (430.0s)\n",
            "Results saved to \u001b[1m/content/multi_class_detection/merge_run4/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=multi_class_detection/merge_run4/weights/best.engine imgsz=640  \n",
            "Validate:        yolo val task=detect model=multi_class_detection/merge_run4/weights/best.engine imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "Ultralytics 8.3.124 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'multi_class_detection/merge_run4/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 9, 8400) (49.6 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['coremltools>=8.0'] not found, attempting AutoUpdate...\n",
            "Collecting coremltools>=8.0\n",
            "  Downloading coremltools-8.3.0-cp311-none-manylinux1_x86_64.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (24.2)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (25.3.0)\n",
            "Collecting cattrs (from coremltools>=8.0)\n",
            "  Downloading cattrs-24.1.3-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pyaml (from coremltools>=8.0)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml->coremltools>=8.0) (6.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->coremltools>=8.0) (1.3.0)\n",
            "Downloading coremltools-8.3.0-cp311-none-manylinux1_x86_64.whl (2.3 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.3/2.3 MB 50.5 MB/s eta 0:00:00\n",
            "Downloading cattrs-24.1.3-py3-none-any.whl (66 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 66.5/66.5 kB 78.4 MB/s eta 0:00:00\n",
            "Downloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, cattrs, coremltools\n",
            "Successfully installed cattrs-24.1.3 coremltools-8.3.0 pyaml-25.1.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 5.1s, installed 1 package: ['coremltools>=8.0']\n",
            "WARNING âš ï¸ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:coremltools:scikit-learn version 1.6.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\n",
            "WARNING:coremltools:XGBoost version 2.1.4 has not been tested with coremltools. You may run into unexpected errors. XGBoost 1.4.2 is the most recent version that has been tested.\n",
            "WARNING:coremltools:TensorFlow version 2.18.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
            "WARNING:coremltools:Torch version 2.6.0+cu124 has not been tested with coremltools. You may run into unexpected errors. Torch 2.5.0 is the most recent version that has been tested.\n",
            "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLCPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLGPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLNeuralEngineComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLComputePlanProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLModelAssetProxy: No module named 'coremltools.libcoremlpython'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mCoreML:\u001b[0m starting export with coremltools 8.3.0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 720/721 [00:00<00:00, 3064.27 ops/s]\n",
            "Running MIL frontend_pytorch pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 87.16 passes/s]\n",
            "Running MIL default pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [00:03<00:00, 24.75 passes/s]\n",
            "Running MIL backend_mlprogram pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 88.08 passes/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mCoreML:\u001b[0m export success âœ… 27.4s, saved as 'multi_class_detection/merge_run4/weights/best.mlpackage' (49.5 MB)\n",
            "\n",
            "Export complete (29.5s)\n",
            "Results saved to \u001b[1m/content/multi_class_detection/merge_run4/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=multi_class_detection/merge_run4/weights/best.mlpackage imgsz=640  \n",
            "Validate:        yolo val task=detect model=multi_class_detection/merge_run4/weights/best.mlpackage imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "Ultralytics 8.3.124 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'multi_class_detection/merge_run4/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 9, 8400) (49.6 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'ai-edge-litert>=1.2.0', 'onnx2tf>=1.26.3'] not found, attempting AutoUpdate...\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting sng4onnx>=1.0.1\n",
            "  Downloading sng4onnx-1.0.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting onnx_graphsurgeon>=0.3.26\n",
            "  Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting ai-edge-litert>=1.2.0\n",
            "  Downloading ai_edge_litert-1.2.0-cp311-cp311-manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting onnx2tf>=1.26.3\n",
            "  Downloading onnx2tf-1.27.2-py3-none-any.whl.metadata (147 kB)\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 147.7/147.7 kB 8.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from onnx_graphsurgeon>=0.3.26) (2.0.2)\n",
            "Requirement already satisfied: onnx>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from onnx_graphsurgeon>=0.3.26) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from ai-edge-litert>=1.2.0) (25.2.10)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.14.0->onnx_graphsurgeon>=0.3.26) (5.29.4)\n",
            "Downloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\n",
            "Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl (57 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 57.9/57.9 kB 314.5 MB/s eta 0:00:00\n",
            "Downloading ai_edge_litert-1.2.0-cp311-cp311-manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.5/3.5 MB 101.0 MB/s eta 0:00:00\n",
            "Downloading onnx2tf-1.27.2-py3-none-any.whl (446 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 446.6/446.6 kB 225.6 MB/s eta 0:00:00\n",
            "Installing collected packages: sng4onnx, onnx2tf, ai-edge-litert, onnx_graphsurgeon\n",
            "Successfully installed ai-edge-litert-1.2.0 onnx2tf-1.27.2 onnx_graphsurgeon-0.5.8 sng4onnx-1.0.4\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 5.5s, installed 4 packages: ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'ai-edge-litert>=1.2.0', 'onnx2tf>=1.26.3']\n",
            "WARNING âš ï¸ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.18.0...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/calibration_image_sample_data_20x128x128x3_float32.npy.zip to 'calibration_image_sample_data_20x128x128x3_float32.npy.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.11M/1.11M [00:00<00:00, 29.8MB/s]\n",
            "Unzipping calibration_image_sample_data_20x128x128x3_float32.npy.zip to /content/calibration_image_sample_data_20x128x128x3_float32.npy...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 38.40file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.51...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 8.1s, saved as 'multi_class_detection/merge_run4/weights/best.onnx' (98.8 MB)\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.27.2...\n",
            "Saved artifact at 'multi_class_detection/merge_run4/weights/best_saved_model'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serving_default'\n",
            "  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 640, 640, 3), dtype=tf.float32, name='images')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(1, 9, 8400), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136326922341456: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326922341264: TensorSpec(shape=(3, 3, 3, 48), dtype=tf.float32, name=None)\n",
            "  136326922341840: TensorSpec(shape=(48,), dtype=tf.float32, name=None)\n",
            "  136326922345296: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326922344912: TensorSpec(shape=(3, 3, 48, 96), dtype=tf.float32, name=None)\n",
            "  136326922345872: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922341648: TensorSpec(shape=(1, 1, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922346256: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922342992: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922346640: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922348560: TensorSpec(shape=(3, 3, 48, 48), dtype=tf.float32, name=None)\n",
            "  136326922348944: TensorSpec(shape=(48,), dtype=tf.float32, name=None)\n",
            "  136326922347600: TensorSpec(shape=(3, 3, 48, 48), dtype=tf.float32, name=None)\n",
            "  136326922349328: TensorSpec(shape=(48,), dtype=tf.float32, name=None)\n",
            "  136326922349136: TensorSpec(shape=(3, 3, 48, 48), dtype=tf.float32, name=None)\n",
            "  136326922349520: TensorSpec(shape=(48,), dtype=tf.float32, name=None)\n",
            "  136326922347216: TensorSpec(shape=(3, 3, 48, 48), dtype=tf.float32, name=None)\n",
            "  136326922349712: TensorSpec(shape=(48,), dtype=tf.float32, name=None)\n",
            "  136326922346448: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922346832: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922350096: TensorSpec(shape=(1, 1, 192, 96), dtype=tf.float32, name=None)\n",
            "  136326922350288: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922350480: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326922347792: TensorSpec(shape=(3, 3, 96, 192), dtype=tf.float32, name=None)\n",
            "  136326922349904: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326922350672: TensorSpec(shape=(1, 1, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326922351056: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326922351440: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922351248: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922352016: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922353360: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922352592: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922353744: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922352784: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922353936: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922352208: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922354128: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922353552: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922354320: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326755566032: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326755565840: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922354512: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326755565648: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326755566608: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326755566416: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922351632: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922350864: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755567184: TensorSpec(shape=(1, 1, 576, 192), dtype=tf.float32, name=None)\n",
            "  136326755566992: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755567376: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326755566224: TensorSpec(shape=(3, 3, 192, 384), dtype=tf.float32, name=None)\n",
            "  136326755566800: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326755567568: TensorSpec(shape=(1, 1, 384, 384), dtype=tf.float32, name=None)\n",
            "  136326755567952: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326755568336: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755568144: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755568912: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755570256: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755569488: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755570640: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755569680: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755570832: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755569104: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755571024: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755571408: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755571600: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755570448: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755571792: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755572176: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755572368: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755571216: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755572560: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755568528: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755567760: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755573136: TensorSpec(shape=(1, 1, 1152, 384), dtype=tf.float32, name=None)\n",
            "  136326755572944: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326755573328: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326755571984: TensorSpec(shape=(3, 3, 384, 576), dtype=tf.float32, name=None)\n",
            "  136326755572752: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326755573520: TensorSpec(shape=(1, 1, 576, 576), dtype=tf.float32, name=None)\n",
            "  136326755574288: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326755573712: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755573904: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755574864: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326755576208: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326755575440: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326755576592: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326755575632: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326755576784: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326755575056: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326755576976: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326755574480: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755574096: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755577552: TensorSpec(shape=(1, 1, 1152, 576), dtype=tf.float32, name=None)\n",
            "  136326755577360: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326755576400: TensorSpec(shape=(1, 1, 576, 288), dtype=tf.float32, name=None)\n",
            "  136326755577744: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326755577936: TensorSpec(shape=(1, 1, 1152, 576), dtype=tf.float32, name=None)\n",
            "  136326755578128: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326755578512: TensorSpec(shape=(1, 1, 960, 384), dtype=tf.float32, name=None)\n",
            "  136326755578320: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326755578896: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755578704: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755579472: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755580816: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755580048: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755581200: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755581008: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755580240: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755581392: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755581584: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755579088: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755577168: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755579664: TensorSpec(shape=(1, 1, 768, 384), dtype=tf.float32, name=None)\n",
            "  136326755581776: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326751404112: TensorSpec(shape=(1, 1, 576, 192), dtype=tf.float32, name=None)\n",
            "  136326751404496: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751404880: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751404688: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751405456: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326751406800: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326751406032: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326751407184: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326751406992: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326751406224: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326751407376: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326751407568: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326751405072: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751404304: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751407952: TensorSpec(shape=(1, 1, 384, 192), dtype=tf.float32, name=None)\n",
            "  136326751407760: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751408336: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326751408144: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751405648: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751410064: TensorSpec(shape=(1, 1, 576, 384), dtype=tf.float32, name=None)\n",
            "  136326751410256: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326751411216: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751411408: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751412944: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751414096: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751414288: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751413904: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751412752: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751412368: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751413712: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751412176: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751411600: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751411792: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751414672: TensorSpec(shape=(1, 1, 768, 384), dtype=tf.float32, name=None)\n",
            "  136326751413520: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326751415056: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326751414864: TensorSpec(shape=(3, 3, 384, 384), dtype=tf.float32, name=None)\n",
            "  136326751414480: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326751416784: TensorSpec(shape=(1, 1, 960, 576), dtype=tf.float32, name=None)\n",
            "  136326751416976: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326751417936: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751418128: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326748619024: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326748619216: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326751419856: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326751419472: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326751420240: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326751419664: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326751418896: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326751419088: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326751418320: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751418512: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326748619792: TensorSpec(shape=(1, 1, 1152, 576), dtype=tf.float32, name=None)\n",
            "  136326748619408: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326748620368: TensorSpec(shape=(3, 3, 576, 192), dtype=tf.float32, name=None)\n",
            "  136326748619984: TensorSpec(shape=(3, 3, 576, 64), dtype=tf.float32, name=None)\n",
            "  136326751415632: TensorSpec(shape=(3, 3, 384, 192), dtype=tf.float32, name=None)\n",
            "  136326751415248: TensorSpec(shape=(3, 3, 384, 64), dtype=tf.float32, name=None)\n",
            "  136326751408912: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751408528: TensorSpec(shape=(3, 3, 192, 64), dtype=tf.float32, name=None)\n",
            "  136326748619600: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326748620176: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751415824: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751415440: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751409104: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751408720: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326748620560: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326748620944: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326751416016: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751416208: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326751409296: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751409488: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326748620752: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326748621136: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751416400: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751416592: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751409680: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751409872: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326748621328: TensorSpec(shape=(1, 1, 192, 5), dtype=tf.float32, name=None)\n",
            "  136326748621712: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326751417168: TensorSpec(shape=(1, 1, 192, 5), dtype=tf.float32, name=None)\n",
            "  136326751417744: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326751410448: TensorSpec(shape=(1, 1, 192, 5), dtype=tf.float32, name=None)\n",
            "  136326751411024: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326748621904: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326748621520: TensorSpec(shape=(5,), dtype=tf.float32, name=None)\n",
            "  136326751417360: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751417552: TensorSpec(shape=(5,), dtype=tf.float32, name=None)\n",
            "  136326751410640: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751410832: TensorSpec(shape=(5,), dtype=tf.float32, name=None)\n",
            "  136326748624016: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748623824: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748624400: TensorSpec(shape=(1, 1, 16, 1), dtype=tf.float32, name=None)\n",
            "  136326748622288: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748623056: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748625360: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748625552: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748626704: TensorSpec(shape=(1, 2, 8400), dtype=tf.float32, name=None)\n",
            "  136326748626128: TensorSpec(shape=(1, 2, 8400), dtype=tf.float32, name=None)\n",
            "  136326748623248: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748623440: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 50.0s, saved as 'multi_class_detection/merge_run4/weights/best_saved_model' (247.3 MB)\n",
            "\n",
            "Export complete (51.8s)\n",
            "Results saved to \u001b[1m/content/multi_class_detection/merge_run4/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=multi_class_detection/merge_run4/weights/best_saved_model imgsz=640  \n",
            "Validate:        yolo val task=detect model=multi_class_detection/merge_run4/weights/best_saved_model imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'multi_class_detection/merge_run4/weights/best_saved_model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# å°å‡ºç‚º ONNX æ ¼å¼ï¼ˆé©ç”¨æ–¼å¤šç¨®æ¨ç†æ¡†æ¶ï¼‰\n",
        "model.export(format='onnx', dynamic=True, simplify=True)\n",
        "\n",
        "# å°å‡ºç‚º TensorRT æ ¼å¼ï¼ˆNVIDIA GPU ä¸Šçš„é«˜æ€§èƒ½æ¨ç†ï¼‰\n",
        "model.export(format='engine', dynamic=True, simplify=True, device=0)\n",
        "\n",
        "# å°å‡ºç‚º CoreML æ ¼å¼ï¼ˆé©ç”¨æ–¼ iOS è¨­å‚™ï¼‰\n",
        "model.export(format='coreml', simplify=True)\n",
        "\n",
        "# å°å‡ºç‚º TensorFlow SavedModel æ ¼å¼\n",
        "model.export(format='saved_model', simplify=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmSxAhPnhBhU"
      },
      "source": [
        "### 2. éƒ¨ç½²åˆ°å‡ºå…¥å£å³æ™‚åŒ…è£¹èˆ‡äººç‰©æ„åœ–åµæ¸¬ç³»çµ±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qdiAxzcUOQN"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output  # << é‡è¦ï¼šæ¸…é™¤ä¸Šå€‹frameçš„é¡¯ç¤º"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# è¼‰å…¥æ¨¡å‹\n",
        "model = YOLO('/content/multi_class_detection/merge_run4/weights/best.pt')\n",
        "\n",
        "# è¨­å®šåƒæ•¸\n",
        "conf_threshold = 0.5\n",
        "video_source = 1\n",
        "detect_interval = 15  # æ¯15å¼µæ¨è«–ä¸€æ¬¡\n",
        "snapshot_interval_sec = 5  # æ¯5ç§’å­˜ä¸€å¼µå¿«ç…§\n",
        "snapshot_dir = \"/content/snapshots\"\n",
        "os.makedirs(snapshot_dir, exist_ok=True)\n",
        "\n",
        "if video_source == 0:\n",
        "    def get_camera_stream(ip, username, password, channel=1):\n",
        "        rtsp_url = f\"rtsp://{username}:{password}@{ip}/Streaming/Channels/{channel}01\"\n",
        "        cap = cv2.VideoCapture(rtsp_url)\n",
        "        return cap\n",
        "    camera = get_camera_stream(\"106.107.183.134\", \"admin\", \"Qazwsx1122\")\n",
        "elif video_source == 1:\n",
        "    video_path = \"videoplayback.mp4\"  # æ›¿æ›æˆä½ çš„å½±ç‰‡è·¯å¾‘\n",
        "    camera = cv2.VideoCapture(video_path)\n",
        "else:\n",
        "    print(\"Invalid video source selected.\")\n",
        "    exit()\n",
        "\n",
        "# å–å¾—å½±ç‰‡è³‡è¨Š\n",
        "fps = camera.get(cv2.CAP_PROP_FPS) or 30  # é è¨­30fps\n",
        "frame_width = int(camera.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "snapshot_interval_frames = int(fps * snapshot_interval_sec)\n",
        "\n",
        "# è¨­å®šåŸ·è¡Œæ™‚é–“\n",
        "end_time = time.time() + 300  # æœ€é•·5åˆ†é˜\n",
        "\n",
        "frame_count = 0\n",
        "inference_count = 0\n",
        "latest_detected_classes = defaultdict(int)   # æœ€æ–°ä¸€è¼ªæ¨è«–çµæœ\n",
        "cumulative_detected_classes = defaultdict(int)  # ç´¯ç©æ‰€æœ‰æ¨è«–çµæœ\n",
        "latest_names = []\n",
        "frames = []  # ä¿å­˜æ‰€æœ‰è™•ç†å¾Œçš„frame\n",
        "\n",
        "# ä¸»å¾ªç’°\n",
        "while time.time() < end_time:\n",
        "    ret, frame = camera.read()\n",
        "    if not ret:\n",
        "        print(\"ç„¡æ³•è®€å–å½±åƒï¼Œå¯èƒ½æ˜¯å½±ç‰‡çµæŸäº†ã€‚\")\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    # æ¯detect_intervalå¼µæ¨è«–ä¸€æ¬¡\n",
        "    if frame_count % detect_interval == 0:\n",
        "        results = model(frame, conf=conf_threshold)\n",
        "        boxes = results[0].boxes.data.cpu().numpy() if results[0].boxes.data is not None else []\n",
        "        latest_names = results[0].names\n",
        "        inference_count += 1\n",
        "\n",
        "        # æ›´æ–°æœ€æ–°æ¨è«–çµæœ + ç´¯ç©æ¨è«–çµ±è¨ˆ\n",
        "        latest_detected_classes = defaultdict(int)\n",
        "        for det in boxes:\n",
        "            _, _, _, _, score, class_id = det\n",
        "            if score >= conf_threshold:\n",
        "                class_name = latest_names[int(class_id)]\n",
        "                latest_detected_classes[class_name] += 1\n",
        "                cumulative_detected_classes[class_name] += 1\n",
        "\n",
        "    # ç•«æ¨è«–è³‡è¨Šæ–‡å­—åœ¨frameä¸Š\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    # å·¦ä¸Šè§’æ¨è«–æ¬¡æ•¸\n",
        "    cv2.putText(annotated_frame, f\"Inference Count: {inference_count}\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "    # å³ä¸Šè§’ç›®å‰frameç·¨è™Ÿ\n",
        "    cv2.putText(annotated_frame, f\"Frame: {frame_count}\", (frame_width - 250, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2)\n",
        "\n",
        "    # å·¦ä¸‹è§’é¡¯ç¤ºç´¯ç©åµæ¸¬çµ±è¨ˆ\n",
        "    y_offset = 70\n",
        "    for cls_name, count in cumulative_detected_classes.items():\n",
        "        text = f\"{cls_name}: {count}\"\n",
        "        cv2.putText(annotated_frame, text, (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "        y_offset += 40\n",
        "\n",
        "    # å­˜é€²framesæ¸…å–®\n",
        "    frames.append(annotated_frame)\n",
        "\n",
        "    # æ¯éš”snapshot_interval_frameså„²å­˜ä¸€å¼µå¿«ç…§\n",
        "    if frame_count % snapshot_interval_frames == 0:\n",
        "        snapshot_path = os.path.join(snapshot_dir, f\"snapshot_frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(snapshot_path, annotated_frame)\n",
        "        print(f\"å„²å­˜å¿«ç…§: {snapshot_path}\")\n",
        "\n",
        "camera.release()\n",
        "print(f\"ç¸½å…±è™•ç†äº† {len(frames)} å¼µframeã€‚\")\n",
        "\n",
        "# --- åˆæˆå½±ç‰‡ ---\n",
        "output_video_path = '/content/inference_output.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "for frame in frames:\n",
        "    out.write(frame)\n",
        "out.release()\n",
        "\n",
        "print(f\"å½±ç‰‡å·²å„²å­˜åˆ° {output_video_path}\")\n",
        "\n",
        "# ğŸ”¥ ğŸ”¥ ğŸ”¥ åœ¨æœ€å¾Œåˆ—å°ç´¯ç©æ¨è«–çµæœ ğŸ”¥ ğŸ”¥ ğŸ”¥\n",
        "print(\"\\n===== æœ€å¾Œç´¯ç©æ¨è«–çµæœ =====\")\n",
        "for cls_name, count in cumulative_detected_classes.items():\n",
        "    print(f\"{cls_name}: {count} æ¬¡\")\n",
        "print(\"==============================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FBdG0vk7ip3C",
        "outputId": "4e47791f-f379-489c-919b-6a8b9366fc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 (no detections), 240.4ms\n",
            "Speed: 8.6ms preprocess, 240.4ms inference, 15.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 80.0ms\n",
            "Speed: 10.6ms preprocess, 80.0ms inference, 14.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 2.8ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.5ms\n",
            "Speed: 3.5ms preprocess, 29.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 39.7ms\n",
            "Speed: 2.5ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 5.1ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 6.6ms preprocess, 24.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.1ms\n",
            "Speed: 2.6ms preprocess, 26.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_149.jpg\n",
            "\n",
            "0: 384x640 (no detections), 48.4ms\n",
            "Speed: 4.7ms preprocess, 48.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.9ms\n",
            "Speed: 2.5ms preprocess, 31.9ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 41.1ms\n",
            "Speed: 2.6ms preprocess, 41.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 30.9ms\n",
            "Speed: 4.6ms preprocess, 30.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 48.3ms\n",
            "Speed: 6.0ms preprocess, 48.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 44.4ms\n",
            "Speed: 2.5ms preprocess, 44.4ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 33.6ms\n",
            "Speed: 2.5ms preprocess, 33.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 35.8ms\n",
            "Speed: 7.8ms preprocess, 35.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 35.1ms\n",
            "Speed: 3.8ms preprocess, 35.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_298.jpg\n",
            "\n",
            "0: 384x640 (no detections), 44.9ms\n",
            "Speed: 4.9ms preprocess, 44.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 57.0ms\n",
            "Speed: 10.9ms preprocess, 57.0ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 65.2ms\n",
            "Speed: 4.4ms preprocess, 65.2ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.6ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 28.4ms\n",
            "Speed: 2.8ms preprocess, 28.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 24.7ms\n",
            "Speed: 2.4ms preprocess, 24.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.9ms\n",
            "Speed: 2.6ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.3ms\n",
            "Speed: 4.7ms preprocess, 31.3ms inference, 15.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_447.jpg\n",
            "\n",
            "0: 384x640 1 other_person, 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 30.7ms\n",
            "Speed: 2.9ms preprocess, 30.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 28.0ms\n",
            "Speed: 2.6ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 26.4ms\n",
            "Speed: 2.6ms preprocess, 26.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.8ms\n",
            "Speed: 2.5ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 30.9ms\n",
            "Speed: 3.4ms preprocess, 30.9ms inference, 20.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 42.0ms\n",
            "Speed: 2.8ms preprocess, 42.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.7ms\n",
            "Speed: 4.0ms preprocess, 27.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_596.jpg\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 2.6ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.6ms\n",
            "Speed: 8.0ms preprocess, 29.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.9ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 35.4ms\n",
            "Speed: 2.6ms preprocess, 35.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 31.8ms\n",
            "Speed: 2.8ms preprocess, 31.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 55.8ms\n",
            "Speed: 3.9ms preprocess, 55.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 33.5ms\n",
            "Speed: 21.1ms preprocess, 33.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 40.5ms\n",
            "Speed: 2.4ms preprocess, 40.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.4ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.8ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_745.jpg\n",
            "\n",
            "0: 384x640 (no detections), 29.2ms\n",
            "Speed: 5.0ms preprocess, 29.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.6ms\n",
            "Speed: 2.7ms preprocess, 32.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 3.7ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.7ms preprocess, 24.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.7ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 2.4ms preprocess, 24.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 3.3ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.2ms preprocess, 24.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_894.jpg\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.4ms preprocess, 24.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.8ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 3.5ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.8ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.5ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.9ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 3.3ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.9ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 24.7ms\n",
            "Speed: 3.7ms preprocess, 24.7ms inference, 11.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.1ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_1043.jpg\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.7ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 packages, 24.6ms\n",
            "Speed: 2.9ms preprocess, 24.6ms inference, 8.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.0ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 3.0ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 3.6ms preprocess, 24.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 24.6ms\n",
            "Speed: 2.6ms preprocess, 24.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 25.4ms\n",
            "Speed: 2.8ms preprocess, 25.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 24.6ms\n",
            "Speed: 2.4ms preprocess, 24.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_1192.jpg\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.0ms\n",
            "Speed: 2.3ms preprocess, 25.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.9ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 bag, 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.8ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 3.0ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 36.9ms\n",
            "Speed: 34.4ms preprocess, 36.9ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 311.3ms\n",
            "Speed: 172.4ms preprocess, 311.3ms inference, 63.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 3.2ms preprocess, 24.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_1341.jpg\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.4ms preprocess, 24.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 4.3ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 2.7ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 4.1ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.8ms preprocess, 24.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 3.4ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "ç„¡æ³•è®€å–å½±åƒï¼Œå¯èƒ½æ˜¯å½±ç‰‡çµæŸäº†ã€‚\n",
            "ç¸½å…±è™•ç†äº† 1469 å¼µframeã€‚\n",
            "å½±ç‰‡å·²å„²å­˜åˆ° /content/inference_output.mp4\n",
            "\n",
            "===== æœ€å¾Œç´¯ç©æ¨è«–çµæœ =====\n",
            "package: 10 æ¬¡\n",
            "other_person: 12 æ¬¡\n",
            "bag: 1 æ¬¡\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# è¼‰å…¥æ¨¡å‹\n",
        "# è«‹ç¢ºä¿æ¨¡å‹è·¯å¾‘æ­£ç¢º\n",
        "model_path = '/content/multi_class_detection/merge_run4/weights/best.pt'\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ {model_path}ã€‚è«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚\")\n",
        "    # åœ¨ Colab ç’°å¢ƒä¸­ï¼Œå¦‚æœæ¨¡å‹åœ¨ Google Driveï¼Œéœ€è¦å…ˆæ›è¼‰ Drive\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    # model_path = '/content/drive/MyDrive/path/to/your/model/best.pt' # ä¿®æ”¹ç‚ºæ‚¨åœ¨ Drive ä¸­çš„å¯¦éš›è·¯å¾‘\n",
        "    # if not os.path.exists(model_path):\n",
        "    #     exit()\n",
        "    exit() # å¦‚æœä¸åœ¨ Colab æˆ– Drive ä¸­æ‰¾ä¸åˆ°ï¼Œå‰‡é€€å‡º\n",
        "\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# è¨­å®šåƒæ•¸\n",
        "conf_threshold = 0.5\n",
        "video_source = 1  # 0: RTSP, 1: æœ¬åœ°å½±ç‰‡æª”æ¡ˆ\n",
        "detect_interval = 15  # æ¯15å¼µæ¨è«–ä¸€æ¬¡\n",
        "snapshot_interval_sec = 5  # æ¯5ç§’å­˜ä¸€å¼µå¿«ç…§\n",
        "snapshot_dir = \"/content/snapshots\"\n",
        "os.makedirs(snapshot_dir, exist_ok=True)\n",
        "\n",
        "# --- æ”å½±æ©Ÿ/å½±ç‰‡ä¾†æºè¨­å®š ---\n",
        "if video_source == 0:\n",
        "    # RTSP ä¾†æºè¨­å®š (è«‹æ›¿æ›æˆæ‚¨çš„å¯¦éš›è³‡è¨Š)\n",
        "    rtsp_ip = \"106.107.183.134\"\n",
        "    rtsp_user = \"admin\"\n",
        "    rtsp_pass = \"Qazwsx1122\"\n",
        "    rtsp_channel = 1\n",
        "    rtsp_url = f\"rtsp://{rtsp_user}:{rtsp_pass}@{rtsp_ip}/Streaming/Channels/{rtsp_channel}01\"\n",
        "    print(f\"å˜—è©¦é€£æ¥ RTSP: {rtsp_url}\")\n",
        "    camera = cv2.VideoCapture(rtsp_url)\n",
        "elif video_source == 1:\n",
        "    # æœ¬åœ°å½±ç‰‡æª”æ¡ˆè¨­å®š (è«‹æ›¿æ›æˆæ‚¨çš„å½±ç‰‡è·¯å¾‘)\n",
        "    video_path = \"videoplayback.mp4\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°å½±ç‰‡æª”æ¡ˆ {video_path}ã€‚è«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚\")\n",
        "        # åœ¨ Colab ç’°å¢ƒä¸­ï¼Œå¦‚æœå½±ç‰‡åœ¨ Google Driveï¼Œéœ€è¦å…ˆæ›è¼‰ Drive\n",
        "        # from google.colab import drive\n",
        "        # drive.mount('/content/drive')\n",
        "        # video_path = '/content/drive/MyDrive/path/to/your/video.mp4' # ä¿®æ”¹ç‚ºæ‚¨åœ¨ Drive ä¸­çš„å¯¦éš›è·¯å¾‘\n",
        "        # if not os.path.exists(video_path):\n",
        "        #     exit()\n",
        "        exit() # å¦‚æœä¸åœ¨ Colab æˆ– Drive ä¸­æ‰¾ä¸åˆ°ï¼Œå‰‡é€€å‡º\n",
        "    print(f\"è®€å–å½±ç‰‡æª”æ¡ˆ: {video_path}\")\n",
        "    camera = cv2.VideoCapture(video_path)\n",
        "else:\n",
        "    print(\"éŒ¯èª¤ï¼šç„¡æ•ˆçš„å½±åƒä¾†æºé¸æ“‡ã€‚\")\n",
        "    exit()\n",
        "\n",
        "# æª¢æŸ¥æ”å½±æ©Ÿ/å½±ç‰‡æ˜¯å¦æˆåŠŸé–‹å•Ÿ\n",
        "if not camera.isOpened():\n",
        "    print(\"éŒ¯èª¤ï¼šç„¡æ³•é–‹å•Ÿå½±åƒä¾†æºã€‚è«‹æª¢æŸ¥è·¯å¾‘ã€ç¶²è·¯é€£ç·šæˆ– RTSP èªè­‰è³‡è¨Šã€‚\")\n",
        "    exit()\n",
        "\n",
        "# å–å¾—å½±ç‰‡è³‡è¨Š\n",
        "fps = camera.get(cv2.CAP_PROP_FPS)\n",
        "if fps is None or fps == 0:\n",
        "    print(\"è­¦å‘Šï¼šç„¡æ³•å–å¾—å½±ç‰‡ FPSï¼Œé è¨­ç‚º 30ã€‚\")\n",
        "    fps = 30\n",
        "frame_width = int(camera.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "snapshot_interval_frames = int(fps * snapshot_interval_sec)\n",
        "\n",
        "print(f\"å½±ç‰‡è³‡è¨Š: {frame_width}x{frame_height} @ {fps:.2f} FPS\")\n",
        "\n",
        "# è¨­å®šåŸ·è¡Œæ™‚é–“ (ä¾‹å¦‚ï¼šæœ€é•·5åˆ†é˜)\n",
        "max_duration_sec = 300\n",
        "end_time = time.time() + max_duration_sec\n",
        "\n",
        "# åˆå§‹åŒ–è¨ˆæ•¸å™¨å’Œå„²å­˜è®Šæ•¸\n",
        "frame_count = 0\n",
        "inference_count = 0\n",
        "latest_detected_classes = defaultdict(int)   # æœ€æ–°ä¸€è¼ªæ¨è«–çµæœ\n",
        "cumulative_detected_classes = defaultdict(int)  # ç´¯ç©æ‰€æœ‰æ¨è«–çµæœ\n",
        "latest_boxes_for_drawing = [] # ç”¨æ–¼å„²å­˜æœ€æ–°åµæ¸¬æ¡†ï¼Œä»¥ä¾¿åœ¨éæ¨è«–å¹€ä¸Šç¹ªè£½\n",
        "latest_names = model.names # å¾æ¨¡å‹ç›´æ¥ç²å–é¡åˆ¥åç¨±\n",
        "frames = []  # ä¿å­˜æ‰€æœ‰è™•ç†å¾Œçš„frame\n",
        "\n",
        "print(\"é–‹å§‹è™•ç†å½±åƒ...\")\n",
        "# ä¸»å¾ªç’°\n",
        "while time.time() < end_time:\n",
        "    ret, frame = camera.read()\n",
        "    if not ret:\n",
        "        print(\"ç„¡æ³•è®€å–å½±åƒï¼Œå¯èƒ½æ˜¯å½±ç‰‡çµæŸæˆ–é€£ç·šä¸­æ–·ã€‚\")\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "    annotated_frame = frame.copy() # è¤‡è£½åŸå§‹å¹€ç”¨æ–¼ç¹ªåœ–\n",
        "\n",
        "    # æ¯detect_intervalå¼µæ¨è«–ä¸€æ¬¡\n",
        "    if frame_count % detect_interval == 0:\n",
        "        inference_count += 1\n",
        "        print(f\"åŸ·è¡Œç¬¬ {inference_count} æ¬¡æ¨è«– (Frame: {frame_count})\")\n",
        "        results = model(frame, conf=conf_threshold)\n",
        "        # ç²å–åµæ¸¬æ¡†æ•¸æ“šï¼Œå¦‚æœæ²’æœ‰åµæ¸¬åˆ°å‰‡ç‚ºç©ºåˆ—è¡¨\n",
        "        boxes = results[0].boxes.data.cpu().numpy() if results[0].boxes.data is not None else []\n",
        "        latest_boxes_for_drawing = boxes # æ›´æ–°ç”¨æ–¼ç¹ªè£½çš„åµæ¸¬æ¡†\n",
        "\n",
        "        # æ›´æ–°æœ€æ–°æ¨è«–çµæœ + ç´¯ç©æ¨è«–çµ±è¨ˆ\n",
        "        latest_detected_classes = defaultdict(int)\n",
        "        for det in boxes:\n",
        "            # boxes æ ¼å¼: [x1, y1, x2, y2, score, class_id]\n",
        "            score = det[4]\n",
        "            class_id = int(det[5])\n",
        "            if score >= conf_threshold:\n",
        "                class_name = latest_names[class_id]\n",
        "                latest_detected_classes[class_name] += 1\n",
        "                cumulative_detected_classes[class_name] += 1\n",
        "\n",
        "    # --- åœ¨ç•«å¸ƒä¸Šç¹ªè£½åµæ¸¬æ¡†å’Œæ¨™ç±¤ (ä½¿ç”¨æœ€æ–°åµæ¸¬çµæœ) ---\n",
        "    if latest_boxes_for_drawing is not None:\n",
        "        for det in latest_boxes_for_drawing:\n",
        "            x1, y1, x2, y2, score, class_id = det\n",
        "            if score >= conf_threshold:\n",
        "                class_id = int(class_id)\n",
        "                class_name = latest_names[class_id]\n",
        "\n",
        "                # ç¹ªè£½åµæ¸¬æ¡† (ç¶ è‰²)\n",
        "                color = (0, 255, 0)\n",
        "                cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
        "\n",
        "                # æº–å‚™æ¨™ç±¤æ–‡å­—\n",
        "                label = f\"{class_name}: {score:.2f}\"\n",
        "\n",
        "                # è¨ˆç®—æ–‡å­—å¤§å°å’Œä½ç½®\n",
        "                (label_width, label_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
        "                # ç¢ºä¿æ¨™ç±¤åœ¨åœ–ç‰‡ç¯„åœå…§\n",
        "                label_y = max(label_height + 10, int(y1) - 10)\n",
        "                label_x = int(x1)\n",
        "\n",
        "                # ç¹ªè£½æ¨™ç±¤èƒŒæ™¯\n",
        "                cv2.rectangle(annotated_frame,\n",
        "                              (label_x, label_y - label_height - baseline),\n",
        "                              (label_x + label_width, label_y + baseline),\n",
        "                              color, cv2.FILLED)\n",
        "                # ç¹ªè£½æ¨™ç±¤æ–‡å­— (é»‘è‰²)\n",
        "                cv2.putText(annotated_frame, label, (label_x, label_y),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
        "    # --- ç¹ªè£½çµæŸ ---\n",
        "\n",
        "    # --- åœ¨ç•«å¸ƒä¸Šç¹ªè£½å…¶ä»–è³‡è¨Šæ–‡å­— ---\n",
        "    # å·¦ä¸Šè§’æ¨è«–æ¬¡æ•¸ (ç´…è‰²)\n",
        "    cv2.putText(annotated_frame, f\"Inference Count: {inference_count}\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "    # å³ä¸Šè§’ç›®å‰frameç·¨è™Ÿ (æ´‹ç´…è‰²)\n",
        "    cv2.putText(annotated_frame, f\"Frame: {frame_count}\", (frame_width - 250, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2)\n",
        "\n",
        "    # å·¦ä¸‹è§’é¡¯ç¤ºç´¯ç©åµæ¸¬çµ±è¨ˆ (è—è‰²)\n",
        "    y_offset = frame_height - 30 # å¾åº•éƒ¨å¾€ä¸Šç•«\n",
        "    sorted_cumulative = sorted(cumulative_detected_classes.items(), key=lambda item: item[1], reverse=True)\n",
        "    for cls_name, count in sorted_cumulative:\n",
        "        text = f\"{cls_name}: {count}\"\n",
        "        cv2.putText(annotated_frame, text, (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "        y_offset -= 30 # å¾€ä¸Šç§»å‹•ç¹ªè£½ä½ç½®\n",
        "        if y_offset < 60: # é¿å…è¦†è“‹é ‚éƒ¨æ–‡å­—\n",
        "            break\n",
        "    # --- ç¹ªè£½çµæŸ ---\n",
        "\n",
        "    # å°‡è™•ç†å¾Œçš„å¹€åŠ å…¥åˆ—è¡¨\n",
        "    frames.append(annotated_frame)\n",
        "\n",
        "    # æ¯éš” snapshot_interval_frames å„²å­˜ä¸€å¼µå¿«ç…§\n",
        "    if frame_count % snapshot_interval_frames == 0:\n",
        "        snapshot_path = os.path.join(snapshot_dir, f\"snapshot_frame_{frame_count}.jpg\")\n",
        "        try:\n",
        "            cv2.imwrite(snapshot_path, annotated_frame)\n",
        "            print(f\"å„²å­˜å¿«ç…§: {snapshot_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"éŒ¯èª¤ï¼šç„¡æ³•å„²å­˜å¿«ç…§ {snapshot_path}: {e}\")\n",
        "\n",
        "    # (å¯é¸) åœ¨ Colab ä¸­é¡¯ç¤ºå³æ™‚å½±åƒ (å¯èƒ½å½±éŸ¿æ€§èƒ½)\n",
        "    # from google.colab.patches import cv2_imshow\n",
        "    # cv2_imshow(cv2.resize(annotated_frame, (frame_width // 2, frame_height // 2))) # ç¸®å°é¡¯ç¤º\n",
        "    # if cv2.waitKey(1) & 0xFF == ord('q'): # åœ¨ Colab ä¸­ waitKey å¯èƒ½ç„¡æ³•æ­£å¸¸å·¥ä½œ\n",
        "    #    break\n",
        "\n",
        "camera.release()\n",
        "print(f\"å½±åƒä¾†æºå·²é—œé–‰ã€‚ç¸½å…±è™•ç†äº† {len(frames)} å¼µ frameã€‚\")\n",
        "\n",
        "# --- åˆæˆå½±ç‰‡ ---\n",
        "if frames:\n",
        "    output_video_path = '/content/inference_output_with_boxes.mp4'\n",
        "    print(f\"é–‹å§‹åˆæˆå½±ç‰‡åˆ° {output_video_path}...\")\n",
        "    try:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v') # æˆ–è€…ä½¿ç”¨ 'XVID'\n",
        "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "        for i, frame_to_write in enumerate(frames):\n",
        "            out.write(frame_to_write)\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"å·²å¯«å…¥ {i + 1}/{len(frames)} å¹€åˆ°å½±ç‰‡...\")\n",
        "\n",
        "        out.release()\n",
        "        print(f\"å½±ç‰‡å·²æˆåŠŸå„²å­˜åˆ° {output_video_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"éŒ¯èª¤ï¼šç„¡æ³•åˆæˆæˆ–å„²å­˜å½±ç‰‡: {e}\")\n",
        "else:\n",
        "    print(\"æ²’æœ‰è™•ç†ä»»ä½•å¹€ï¼Œç„¡æ³•åˆæˆå½±ç‰‡ã€‚\")\n",
        "\n",
        "# --- åœ¨æœ€å¾Œåˆ—å°ç´¯ç©æ¨è«–çµæœ ---\n",
        "print(\"\\n===== æœ€å¾Œç´¯ç©æ¨è«–çµæœ =====\")\n",
        "if cumulative_detected_classes:\n",
        "    sorted_final_cumulative = sorted(cumulative_detected_classes.items(), key=lambda item: item[1], reverse=True)\n",
        "    for cls_name, count in sorted_final_cumulative:\n",
        "        print(f\"{cls_name}: {count} æ¬¡\")\n",
        "else:\n",
        "    print(\"æ²’æœ‰åµæ¸¬åˆ°ä»»ä½•ç‰©ä»¶ã€‚\")\n",
        "print(\"==============================\")\n",
        "\n",
        "print(\"è…³æœ¬åŸ·è¡Œå®Œç•¢ã€‚\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmZ1ucj5AbVq",
        "outputId": "24023f07-bae5-4b09-a28d-37fd4438fdd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "è®€å–å½±ç‰‡æª”æ¡ˆ: videoplayback.mp4\n",
            "å½±ç‰‡è³‡è¨Š: 1280x720 @ 29.97 FPS\n",
            "é–‹å§‹è™•ç†å½±åƒ...\n",
            "åŸ·è¡Œç¬¬ 1 æ¬¡æ¨è«– (Frame: 15)\n",
            "\n",
            "0: 384x640 (no detections), 111.4ms\n",
            "Speed: 18.6ms preprocess, 111.4ms inference, 117.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 2 æ¬¡æ¨è«– (Frame: 30)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 4.2ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 3 æ¬¡æ¨è«– (Frame: 45)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.8ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 4 æ¬¡æ¨è«– (Frame: 60)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 5 æ¬¡æ¨è«– (Frame: 75)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.9ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 6 æ¬¡æ¨è«– (Frame: 90)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.6ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 7 æ¬¡æ¨è«– (Frame: 105)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 8 æ¬¡æ¨è«– (Frame: 120)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 9 æ¬¡æ¨è«– (Frame: 135)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_149.jpg\n",
            "åŸ·è¡Œç¬¬ 10 æ¬¡æ¨è«– (Frame: 150)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 11 æ¬¡æ¨è«– (Frame: 165)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.4ms preprocess, 25.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 12 æ¬¡æ¨è«– (Frame: 180)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 13 æ¬¡æ¨è«– (Frame: 195)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 14 æ¬¡æ¨è«– (Frame: 210)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.7ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 15 æ¬¡æ¨è«– (Frame: 225)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.0ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 16 æ¬¡æ¨è«– (Frame: 240)\n",
            "\n",
            "0: 384x640 1 package, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 227.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 17 æ¬¡æ¨è«– (Frame: 255)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 18 æ¬¡æ¨è«– (Frame: 270)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 19 æ¬¡æ¨è«– (Frame: 285)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_298.jpg\n",
            "åŸ·è¡Œç¬¬ 20 æ¬¡æ¨è«– (Frame: 300)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 21 æ¬¡æ¨è«– (Frame: 315)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 22 æ¬¡æ¨è«– (Frame: 330)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.4ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 23 æ¬¡æ¨è«– (Frame: 345)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.9ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 24 æ¬¡æ¨è«– (Frame: 360)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.8ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 25 æ¬¡æ¨è«– (Frame: 375)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 26 æ¬¡æ¨è«– (Frame: 390)\n",
            "\n",
            "0: 384x640 1 other_person, 25.1ms\n",
            "Speed: 3.7ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 27 æ¬¡æ¨è«– (Frame: 405)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 28 æ¬¡æ¨è«– (Frame: 420)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.6ms preprocess, 25.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 29 æ¬¡æ¨è«– (Frame: 435)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.8ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_447.jpg\n",
            "åŸ·è¡Œç¬¬ 30 æ¬¡æ¨è«– (Frame: 450)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 31 æ¬¡æ¨è«– (Frame: 465)\n",
            "\n",
            "0: 384x640 1 other_person, 25.1ms\n",
            "Speed: 3.6ms preprocess, 25.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 32 æ¬¡æ¨è«– (Frame: 480)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 33 æ¬¡æ¨è«– (Frame: 495)\n",
            "\n",
            "0: 384x640 1 other_person, 25.8ms\n",
            "Speed: 4.7ms preprocess, 25.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 34 æ¬¡æ¨è«– (Frame: 510)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 35 æ¬¡æ¨è«– (Frame: 525)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 36 æ¬¡æ¨è«– (Frame: 540)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 4.6ms preprocess, 25.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 37 æ¬¡æ¨è«– (Frame: 555)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 38 æ¬¡æ¨è«– (Frame: 570)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.8ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 39 æ¬¡æ¨è«– (Frame: 585)\n",
            "\n",
            "0: 384x640 1 other_person, 25.6ms\n",
            "Speed: 5.6ms preprocess, 25.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_596.jpg\n",
            "åŸ·è¡Œç¬¬ 40 æ¬¡æ¨è«– (Frame: 600)\n",
            "\n",
            "0: 384x640 (no detections), 37.5ms\n",
            "Speed: 3.6ms preprocess, 37.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 41 æ¬¡æ¨è«– (Frame: 615)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 42 æ¬¡æ¨è«– (Frame: 630)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 43 æ¬¡æ¨è«– (Frame: 645)\n",
            "\n",
            "0: 384x640 1 other_person, 32.8ms\n",
            "Speed: 4.7ms preprocess, 32.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 44 æ¬¡æ¨è«– (Frame: 660)\n",
            "\n",
            "0: 384x640 1 other_person, 29.3ms\n",
            "Speed: 6.2ms preprocess, 29.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 45 æ¬¡æ¨è«– (Frame: 675)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 46 æ¬¡æ¨è«– (Frame: 690)\n",
            "\n",
            "0: 384x640 (no detections), 26.3ms\n",
            "Speed: 5.1ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 47 æ¬¡æ¨è«– (Frame: 705)\n",
            "\n",
            "0: 384x640 (no detections), 43.0ms\n",
            "Speed: 3.9ms preprocess, 43.0ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 48 æ¬¡æ¨è«– (Frame: 720)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.7ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 49 æ¬¡æ¨è«– (Frame: 735)\n",
            "\n",
            "0: 384x640 (no detections), 27.3ms\n",
            "Speed: 5.4ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_745.jpg\n",
            "åŸ·è¡Œç¬¬ 50 æ¬¡æ¨è«– (Frame: 750)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.0ms preprocess, 25.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 51 æ¬¡æ¨è«– (Frame: 765)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 52 æ¬¡æ¨è«– (Frame: 780)\n",
            "\n",
            "0: 384x640 (no detections), 33.9ms\n",
            "Speed: 3.9ms preprocess, 33.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 53 æ¬¡æ¨è«– (Frame: 795)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 5.0ms preprocess, 25.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 54 æ¬¡æ¨è«– (Frame: 810)\n",
            "\n",
            "0: 384x640 (no detections), 25.6ms\n",
            "Speed: 3.9ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 55 æ¬¡æ¨è«– (Frame: 825)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 56 æ¬¡æ¨è«– (Frame: 840)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 57 æ¬¡æ¨è«– (Frame: 855)\n",
            "\n",
            "0: 384x640 (no detections), 26.2ms\n",
            "Speed: 6.5ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 58 æ¬¡æ¨è«– (Frame: 870)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 59 æ¬¡æ¨è«– (Frame: 885)\n",
            "\n",
            "0: 384x640 (no detections), 29.0ms\n",
            "Speed: 12.8ms preprocess, 29.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_894.jpg\n",
            "åŸ·è¡Œç¬¬ 60 æ¬¡æ¨è«– (Frame: 900)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 61 æ¬¡æ¨è«– (Frame: 915)\n",
            "\n",
            "0: 384x640 (no detections), 33.2ms\n",
            "Speed: 4.6ms preprocess, 33.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 62 æ¬¡æ¨è«– (Frame: 930)\n",
            "\n",
            "0: 384x640 (no detections), 43.4ms\n",
            "Speed: 3.7ms preprocess, 43.4ms inference, 10.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 63 æ¬¡æ¨è«– (Frame: 945)\n",
            "\n",
            "0: 384x640 (no detections), 35.8ms\n",
            "Speed: 3.5ms preprocess, 35.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 64 æ¬¡æ¨è«– (Frame: 960)\n",
            "\n",
            "0: 384x640 (no detections), 50.4ms\n",
            "Speed: 5.6ms preprocess, 50.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 65 æ¬¡æ¨è«– (Frame: 975)\n",
            "\n",
            "0: 384x640 (no detections), 32.8ms\n",
            "Speed: 3.3ms preprocess, 32.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 66 æ¬¡æ¨è«– (Frame: 990)\n",
            "\n",
            "0: 384x640 (no detections), 36.8ms\n",
            "Speed: 3.4ms preprocess, 36.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 67 æ¬¡æ¨è«– (Frame: 1005)\n",
            "\n",
            "0: 384x640 (no detections), 39.4ms\n",
            "Speed: 7.9ms preprocess, 39.4ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 68 æ¬¡æ¨è«– (Frame: 1020)\n",
            "\n",
            "0: 384x640 1 package, 47.6ms\n",
            "Speed: 7.7ms preprocess, 47.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 69 æ¬¡æ¨è«– (Frame: 1035)\n",
            "\n",
            "0: 384x640 (no detections), 42.5ms\n",
            "Speed: 3.4ms preprocess, 42.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_1043.jpg\n",
            "åŸ·è¡Œç¬¬ 70 æ¬¡æ¨è«– (Frame: 1050)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 5.2ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 71 æ¬¡æ¨è«– (Frame: 1065)\n",
            "\n",
            "0: 384x640 4 packages, 29.8ms\n",
            "Speed: 5.8ms preprocess, 29.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 72 æ¬¡æ¨è«– (Frame: 1080)\n",
            "\n",
            "0: 384x640 1 package, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 73 æ¬¡æ¨è«– (Frame: 1095)\n",
            "\n",
            "0: 384x640 (no detections), 25.7ms\n",
            "Speed: 12.3ms preprocess, 25.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 74 æ¬¡æ¨è«– (Frame: 1110)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 75 æ¬¡æ¨è«– (Frame: 1125)\n",
            "\n",
            "0: 384x640 (no detections), 32.0ms\n",
            "Speed: 11.9ms preprocess, 32.0ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 76 æ¬¡æ¨è«– (Frame: 1140)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 77 æ¬¡æ¨è«– (Frame: 1155)\n",
            "\n",
            "0: 384x640 1 package, 25.7ms\n",
            "Speed: 4.0ms preprocess, 25.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 78 æ¬¡æ¨è«– (Frame: 1170)\n",
            "\n",
            "0: 384x640 1 package, 33.1ms\n",
            "Speed: 8.4ms preprocess, 33.1ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 79 æ¬¡æ¨è«– (Frame: 1185)\n",
            "\n",
            "0: 384x640 1 package, 25.1ms\n",
            "Speed: 3.7ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_1192.jpg\n",
            "åŸ·è¡Œç¬¬ 80 æ¬¡æ¨è«– (Frame: 1200)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 81 æ¬¡æ¨è«– (Frame: 1215)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.4ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 82 æ¬¡æ¨è«– (Frame: 1230)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.6ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 83 æ¬¡æ¨è«– (Frame: 1245)\n",
            "\n",
            "0: 384x640 1 bag, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 84 æ¬¡æ¨è«– (Frame: 1260)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.2ms preprocess, 25.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 85 æ¬¡æ¨è«– (Frame: 1275)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 86 æ¬¡æ¨è«– (Frame: 1290)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 2.9ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 87 æ¬¡æ¨è«– (Frame: 1305)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 88 æ¬¡æ¨è«– (Frame: 1320)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.4ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 89 æ¬¡æ¨è«– (Frame: 1335)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "å„²å­˜å¿«ç…§: /content/snapshots/snapshot_frame_1341.jpg\n",
            "åŸ·è¡Œç¬¬ 90 æ¬¡æ¨è«– (Frame: 1350)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.7ms preprocess, 25.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 91 æ¬¡æ¨è«– (Frame: 1365)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 92 æ¬¡æ¨è«– (Frame: 1380)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.8ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 93 æ¬¡æ¨è«– (Frame: 1395)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 94 æ¬¡æ¨è«– (Frame: 1410)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 95 æ¬¡æ¨è«– (Frame: 1425)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.7ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 96 æ¬¡æ¨è«– (Frame: 1440)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.2ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "åŸ·è¡Œç¬¬ 97 æ¬¡æ¨è«– (Frame: 1455)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.3ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "ç„¡æ³•è®€å–å½±åƒï¼Œå¯èƒ½æ˜¯å½±ç‰‡çµæŸæˆ–é€£ç·šä¸­æ–·ã€‚\n",
            "å½±åƒä¾†æºå·²é—œé–‰ã€‚ç¸½å…±è™•ç†äº† 1469 å¼µ frameã€‚\n",
            "é–‹å§‹åˆæˆå½±ç‰‡åˆ° /content/inference_output_with_boxes.mp4...\n",
            "å·²å¯«å…¥ 100/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 200/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 300/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 400/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 500/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 600/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 700/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 800/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 900/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 1000/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 1100/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 1200/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 1300/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å·²å¯«å…¥ 1400/1469 å¹€åˆ°å½±ç‰‡...\n",
            "å½±ç‰‡å·²æˆåŠŸå„²å­˜åˆ° /content/inference_output_with_boxes.mp4\n",
            "\n",
            "===== æœ€å¾Œç´¯ç©æ¨è«–çµæœ =====\n",
            "other_person: 12 æ¬¡\n",
            "package: 10 æ¬¡\n",
            "bag: 1 æ¬¡\n",
            "==============================\n",
            "è…³æœ¬åŸ·è¡Œå®Œç•¢ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/multi_class_detection_merge_run4.zip /content/multi_class_detection/merge_run4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SJDJBOqVkTDh",
        "outputId": "802c5399-c82e-4445-8b8e-b9e2174b350f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/multi_class_detection/merge_run4/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/R_curve.png (deflated 10%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch0_pred.jpg (deflated 6%)\n",
            "  adding: content/multi_class_detection/merge_run4/labels_correlogram.jpg (deflated 33%)\n",
            "  adding: content/multi_class_detection/merge_run4/results.png (deflated 7%)\n",
            "  adding: content/multi_class_detection/merge_run4/confusion_matrix_normalized.png (deflated 23%)\n",
            "  adding: content/multi_class_detection/merge_run4/confusion_matrix.png (deflated 28%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch2062.jpg (deflated 10%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch2_pred.jpg (deflated 11%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.pt (deflated 8%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.onnx (deflated 17%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/last.pt (deflated 8%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Data/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Data/com.apple.CoreML/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Data/com.apple.CoreML/weights/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Data/com.apple.CoreML/weights/weight.bin (deflated 8%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Data/com.apple.CoreML/model.mlmodel (deflated 89%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Manifest.json (deflated 59%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.engine (deflated 10%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/variables/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/variables/variables.data-00000-of-00001 (deflated 87%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/variables/variables.index (deflated 33%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/best_float32.tflite (deflated 17%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/saved_model.pb (deflated 8%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/best_float16.tflite (deflated 8%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/metadata.yaml (deflated 37%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/fingerprint.pb (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/assets/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch2061.jpg (deflated 7%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch0_labels.jpg (deflated 6%)\n",
            "  adding: content/multi_class_detection/merge_run4/P_curve.png (deflated 10%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch1_labels.jpg (deflated 6%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch2060.jpg (deflated 7%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch1_pred.jpg (deflated 6%)\n",
            "  adding: content/multi_class_detection/merge_run4/F1_curve.png (deflated 7%)\n",
            "  adding: content/multi_class_detection/merge_run4/args.yaml (deflated 53%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch2.jpg (deflated 3%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch0.jpg (deflated 2%)\n",
            "  adding: content/multi_class_detection/merge_run4/results.csv (deflated 60%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch1.jpg (deflated 5%)\n",
            "  adding: content/multi_class_detection/merge_run4/labels.jpg (deflated 20%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch2_labels.jpg (deflated 12%)\n",
            "  adding: content/multi_class_detection/merge_run4/PR_curve.png (deflated 16%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/multi_class_detection_merge_run4.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Uhda2yUDkhP1",
        "outputId": "ae40a323-1399-4fd4-fff3-fd7e36280e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0696cb4b-cf60-4e52-bcbf-1aa863f5ed7b\", \"multi_class_detection_merge_run4.zip\", 586895320)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k3Hw0mIhGuO"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOxEpf2wD/0nX8W5UmP2/c2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}