{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kevinlo937/objects_story/blob/main/cv_MultipleObjectDection3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k3FFkJ-w1_B",
        "outputId": "e6619572-541c-446d-aea0-d5f08d2afb63",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.124-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.124-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.124 ultralytics-thop-2.0.14\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.63-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Downloading roboflow-1.1.63-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.63\n"
          ]
        }
      ],
      "source": [
        "# 安裝 Ultralytics YOLOv8\n",
        "!pip install ultralytics\n",
        "\n",
        "# 安裝 Roboflow 套件（用於資料集下載）\n",
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXjz3PwNXQOT",
        "outputId": "66f197d3-749f-4e9d-a10a-efdc1566e907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# 檢查 CUDA 是否可用\n",
        "!python -c \"import torch; print(torch.cuda.is_available())\"\n",
        "\n",
        "# 檢查可用的 GPU 數量\n",
        "!python -c \"import torch; print(torch.cuda.device_count())\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaUfQsdE-7W8",
        "outputId": "d8083ea9-972c-4f8b-ae29-1e2cbb53b27b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "開始下載資料集...\n",
            "正在下載: package-detection/package-at-front-door v2\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in package-at-front-door-2 to yolov8:: 100%|██████████| 31292/31292 [00:00<00:00, 43875.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to package-at-front-door-2 in yolov8:: 100%|██████████| 2598/2598 [00:00<00:00, 10772.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "下載完成: /content/package-at-front-door-2\n",
            "正在下載: findluggage/find-luggage v1\n",
            "\rloading Roboflow workspace...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in find-luggage-1 to yolov8:: 100%|██████████| 2852/2852 [00:00<00:00, 19268.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to find-luggage-1 in yolov8:: 100%|██████████| 206/206 [00:00<00:00, 10676.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "下載完成: /content/find-luggage-1\n",
            "正在下載: project-ii/person-7y27w v2\n",
            "\rloading Roboflow workspace...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in person-2 to yolov8:: 100%|██████████| 8413/8413 [00:00<00:00, 19105.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to person-2 in yolov8:: 100%|██████████| 482/482 [00:00<00:00, 8820.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "下載完成: /content/person-2\n",
            "正在下載: kevinlo937/deliverman v1\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in deliverman-1 to yolov8:: 100%|██████████| 10326/10326 [00:00<00:00, 24191.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to deliverman-1 in yolov8:: 100%|██████████| 328/328 [00:00<00:00, 6198.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "下載完成: /content/deliverman-1\n",
            "正在下載: kevinlo937/food_deliverman v1\n",
            "\rloading Roboflow workspace...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in food_deliverman-1 to yolov8:: 100%|██████████| 4262/4262 [00:00<00:00, 11596.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to food_deliverman-1 in yolov8:: 100%|██████████| 132/132 [00:00<00:00, 6921.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "下載完成: /content/food_deliverman-1\n",
            "所有資料集下載完成。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "修正後的資料集合併與訓練腳本\n",
        "\n",
        "修正了原始腳本中未處理類別索引映射的問題，確保合併後的多個資料集\n",
        "能夠正確訓練出多類別物件偵測模型。\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import roboflow\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# --- 1. 環境設定與資料集下載 ---\n",
        "\n",
        "# 設定 Roboflow API Key (請替換成您的金鑰)\n",
        "API_KEY = \"NKaOxmYYLFRPaf6w0uF6\" # 請務必替換成您的有效金鑰\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = API_KEY\n",
        "\n",
        "rf = roboflow.Roboflow(api_key=API_KEY)\n",
        "\n",
        "# 定義要下載的資料集及其目標類別名稱\n",
        "# 格式: (workspace, project, version, {original_class_name: target_class_name})\n",
        "datasets_to_download = [\n",
        "    (\"package-detection\", \"package-at-front-door\", 2, {\"package\": \"package\"}),\n",
        "    (\"findluggage\", \"find-luggage\", 1, {\"paper bag\": \"bag\"}), # 根據需要取消註解\n",
        "    (\"project-ii\", \"person-7y27w\", 2, {\"person\": \"other_person\"}),\n",
        "    (\"kevinlo937\", \"deliverman\", 1, {\"mailman\": \"delivery_worker\"}),\n",
        "    (\"kevinlo937\", \"food_deliverman\", 1, {\"Foodpenda\": \"food_delivery\"})\n",
        "]\n",
        "\n",
        "# 統一定義最終合併資料集的類別名稱和索引\n",
        "# **這是關鍵步驟，確保所有來源的類別被映射到一致的索引**\n",
        "UNIFIED_CLASSES = {\n",
        "    \"package\": 0,          # 包裹 (來自 package-at-front-door)\n",
        "    \"bag\": 1,              # 提袋 (如果使用 find-luggage)\n",
        "    \"other_person\": 2,     # 其他人員 (來自 person-7y27w)\n",
        "    \"delivery_worker\": 3,  # 郵差快遞員 (來自 deliverman)\n",
        "    \"food_delivery\": 4     # 送餐員 (來自 food_deliverman)\n",
        "}\n",
        "# 反向映射，方便查找名稱\n",
        "UNIFIED_CLASS_NAMES = {v: k for k, v in UNIFIED_CLASSES.items()}\n",
        "\n",
        "print(\"開始下載資料集...\")\n",
        "downloaded_datasets_info = []\n",
        "for workspace, project, version, class_mapping in datasets_to_download:\n",
        "    try:\n",
        "        print(f\"正在下載: {workspace}/{project} v{version}\")\n",
        "        dataset = rf.workspace(workspace).project(project).version(version).download(\"yolov8\")\n",
        "        downloaded_datasets_info.append({\n",
        "            \"location\": dataset.location,\n",
        "            \"original_yaml\": Path(dataset.location) / \"data.yaml\",\n",
        "            \"class_mapping\": class_mapping\n",
        "        })\n",
        "        print(f\"下載完成: {dataset.location}\")\n",
        "    except Exception as e:\n",
        "        print(f\"下載失敗: {workspace}/{project} v{version}. 錯誤: {e}\")\n",
        "\n",
        "if not downloaded_datasets_info:\n",
        "    print(\"沒有成功下載任何資料集，腳本終止。\")\n",
        "    exit()\n",
        "\n",
        "print(\"所有資料集下載完成。\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. 資料集合併與標籤重新映射 ---\n",
        "\n",
        "print(\"開始合併資料集並重新映射標籤...\")\n",
        "merged_dataset_dir = Path(\"merged_dataset\")\n",
        "\n",
        "# 創建合併資料集的目錄結構\n",
        "for split in [\"train\", \"valid\", \"test\"]:\n",
        "    os.makedirs(merged_dataset_dir / split / \"images\", exist_ok=True)\n",
        "    os.makedirs(merged_dataset_dir / split / \"labels\", exist_ok=True)\n",
        "\n",
        "# 遍歷每個下載的資料集進行合併和映射\n",
        "for dataset_info in downloaded_datasets_info:\n",
        "    dataset_path = Path(dataset_info[\"location\"])\n",
        "    original_yaml_path = dataset_info[\"original_yaml\"]\n",
        "    class_mapping = dataset_info[\"class_mapping\"]\n",
        "\n",
        "    print(f\"處理資料集: {dataset_path.name}\")\n",
        "\n",
        "    # 讀取原始資料集的 data.yaml 以獲取原始類別名稱\n",
        "    try:\n",
        "        with open(original_yaml_path, 'r') as f:\n",
        "            original_data_config = yaml.safe_load(f)\n",
        "        original_class_names = original_data_config.get('names', [])\n",
        "        if not original_class_names:\n",
        "            print(f\"警告: 無法從 {original_yaml_path} 讀取原始類別名稱，跳過此資料集。\")\n",
        "            continue\n",
        "    except Exception as e:\n",
        "        print(f\"錯誤: 無法讀取或解析 {original_yaml_path}: {e}，跳過此資料集。\")\n",
        "        continue\n",
        "\n",
        "    # 遍歷 train/valid/test 分割\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        source_images_dir = dataset_path / split / \"images\"\n",
        "        source_labels_dir = dataset_path / split / \"labels\"\n",
        "        target_images_dir = merged_dataset_dir / split / \"images\"\n",
        "        target_labels_dir = merged_dataset_dir / split / \"labels\"\n",
        "\n",
        "        if not source_images_dir.exists() or not source_labels_dir.exists():\n",
        "            print(f\"警告: 資料集 {dataset_path.name} 的 {split} 分割不完整，跳過。\")\n",
        "            continue\n",
        "\n",
        "        # 複製圖片檔案\n",
        "        for img_file in source_images_dir.glob(\"*.*\"):\n",
        "            try:\n",
        "                shutil.copy(img_file, target_images_dir / img_file.name)\n",
        "            except Exception as e:\n",
        "                print(f\"複製圖片失敗: {img_file} -> {target_images_dir}. 錯誤: {e}\")\n",
        "\n",
        "        # 讀取、映射並寫入標籤檔案\n",
        "        for label_file in source_labels_dir.glob(\"*.txt\"):\n",
        "            new_label_lines = []\n",
        "            try:\n",
        "                with open(label_file, 'r') as f_in:\n",
        "                    lines = f_in.readlines()\n",
        "\n",
        "                for line in lines:\n",
        "                    parts = line.strip().split()\n",
        "                    if not parts:\n",
        "                        continue\n",
        "\n",
        "                    original_class_index = int(parts[0])\n",
        "\n",
        "                    # 檢查原始索引是否有效\n",
        "                    if 0 <= original_class_index < len(original_class_names):\n",
        "                        original_class_name = original_class_names[original_class_index]\n",
        "\n",
        "                        # 檢查此類別是否需要被映射到目標類別\n",
        "                        if original_class_name in class_mapping:\n",
        "                            target_class_name = class_mapping[original_class_name]\n",
        "\n",
        "                            # 檢查目標類別是否存在於統一定義中\n",
        "                            if target_class_name in UNIFIED_CLASSES:\n",
        "                                target_class_index = UNIFIED_CLASSES[target_class_name]\n",
        "                                # 更新標籤行，使用新的統一索引\n",
        "                                new_label_lines.append(f\"{target_class_index} {' '.join(parts[1:])}\\n\")\n",
        "                            else:\n",
        "                                print(f\"警告: 目標類別 '{target_class_name}' 未在 UNIFIED_CLASSES 中定義。來自檔案: {label_file}\")\n",
        "                        # else: # 如果原始類別不需要映射，可以選擇忽略或映射到 'other'\n",
        "                        #     print(f\"資訊: 原始類別 '{original_class_name}' 在 {label_file} 中未被映射，已忽略。\")\n",
        "                    else:\n",
        "                        print(f\"警告: 在 {label_file} 中發現無效的原始類別索引: {original_class_index}\")\n",
        "\n",
        "                # 將修改後的標籤寫入合併資料夾\n",
        "                if new_label_lines:\n",
        "                    target_label_file = target_labels_dir / label_file.name\n",
        "                    with open(target_label_file, 'w') as f_out:\n",
        "                        f_out.writelines(new_label_lines)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"處理標籤檔案失敗: {label_file}. 錯誤: {e}\")\n",
        "\n",
        "print(\"資料集合併與標籤重新映射完成。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J06XCciKLo0j",
        "outputId": "cbabed89-4999-4ccf-ea26-f2e0d17c1bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始合併資料集並重新映射標籤...\n",
            "處理資料集: package-at-front-door-2\n",
            "處理資料集: find-luggage-1\n",
            "處理資料集: person-2\n",
            "處理資料集: deliverman-1\n",
            "處理資料集: food_deliverman-1\n",
            "資料集合併與標籤重新映射完成。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -h -o '^[0-9]' merged_dataset/train/labels/*.txt | sort | uniq -c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmJlz5AAMYXx",
        "outputId": "ed5b7799-ac5b-4cbb-81ce-5eb4be567f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   1905 0\n",
            "     71 1\n",
            "    168 2\n",
            "    135 3\n",
            "     56 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_dataset_dir='/content/merged_dataset'\n",
        "print(merged_dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzmyuu-4PFHJ",
        "outputId": "17de67fa-4541-4f1c-ca6e-1b1c2c0c73c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/merged_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKL8eDtLUj0t",
        "outputId": "ea6488eb-4be8-4e4b-ac4f-188dbb2dca65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "創建合併資料集的 data.yaml...\n",
            "合併後的 data.yaml 已創建: /content/merged_dataset/data.yaml\n"
          ]
        }
      ],
      "source": [
        "# --- 3. 創建合併資料集的 data.yaml ---\n",
        "\n",
        "print(\"創建合併資料集的 data.yaml...\")\n",
        "# Convert merged_dataset_dir to a Path object\n",
        "merged_dataset_dir = Path(merged_dataset_dir)\n",
        "\n",
        "merged_data_yaml_content = {\n",
        "    'train': str(merged_dataset_dir / 'train' / 'images'),\n",
        "    'val': str(merged_dataset_dir / 'valid' / 'images'),\n",
        "    'test': str(merged_dataset_dir / 'test' / 'images'),\n",
        "    'nc': len(UNIFIED_CLASSES),\n",
        "    'names': {index: name for name, index in UNIFIED_CLASSES.items()} # 使用索引作為鍵\n",
        "}\n",
        "\n",
        "merged_data_yaml_path = merged_dataset_dir / \"data.yaml\"\n",
        "\n",
        "try:\n",
        "    with open(merged_data_yaml_path, 'w') as f:\n",
        "        yaml.dump(merged_data_yaml_content, f, default_flow_style=False, sort_keys=False)\n",
        "    print(f\"合併後的 data.yaml 已創建: {merged_data_yaml_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"創建合併後的 data.yaml 失敗: {e}\")\n",
        "    exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data_yaml_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WSXNaqjN_od",
        "outputId": "3e59235b-9b69-4103-ecdd-6101aaf4f1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/merged_dataset/data.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3g0VEVrSrl-",
        "outputId": "d08c803f-4463-43e2-83f5-de137e04f279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始模型訓練...\n",
            "Ultralytics 8.3.124 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/content/merged_dataset/data.yaml, epochs=30, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=multi_class_detection, name=merge_run4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, cutmix=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=multi_class_detection/merge_run4\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3778591  ultralytics.nn.modules.head.Detect           [5, [192, 384, 576]]          \n",
            "Model summary: 169 layers, 25,859,215 parameters, 25,859,199 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 469/475 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 991.8±309.9 MB/s, size: 24.3 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/merged_dataset/train/labels.cache... 1644 images, 3 backgrounds, 0 corrupt: 100%|██████████| 1647/1647 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 344.8±139.5 MB/s, size: 35.4 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/merged_dataset/valid/labels.cache... 131 images, 1 backgrounds, 0 corrupt: 100%|██████████| 132/132 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to multi_class_detection/merge_run4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mmulti_class_detection/merge_run4\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/30      5.93G     0.9783      1.824      1.332         38        640: 100%|██████████| 103/103 [00:57<00:00,  1.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  2.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.283      0.474      0.385      0.271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/30      6.44G      1.199      1.533      1.473         34        640: 100%|██████████| 103/103 [00:56<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137       0.18      0.462      0.226      0.119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/30       6.5G       1.29      1.643      1.565         41        640: 100%|██████████| 103/103 [00:55<00:00,  1.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.444      0.398      0.278      0.144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/30       6.5G      1.246      1.539      1.513         29        640: 100%|██████████| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.733      0.346      0.452      0.235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/30      6.57G      1.191      1.412      1.481         45        640: 100%|██████████| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.577      0.306      0.379      0.221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/30      7.12G      1.121      1.272      1.427         31        640: 100%|██████████| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.728       0.28      0.336      0.179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/30      7.19G      1.071      1.184       1.38         42        640: 100%|██████████| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.518      0.532      0.638      0.413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/30      7.26G      1.028      1.085      1.346         44        640: 100%|██████████| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.788      0.565       0.76      0.505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/30      7.32G     0.9871      1.046      1.331         36        640: 100%|██████████| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.635      0.719       0.79      0.541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/30      7.32G     0.9575      0.996      1.305         41        640: 100%|██████████| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.539       0.67      0.635      0.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/30      7.32G     0.9324     0.9236      1.294         51        640: 100%|██████████| 103/103 [00:54<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.602       0.78      0.689      0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/30      7.32G     0.8966     0.9124       1.27         39        640: 100%|██████████| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137       0.52      0.798      0.733       0.54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/30      7.32G     0.8722     0.8521      1.239         45        640: 100%|██████████| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.915      0.643      0.844        0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/30      7.32G     0.8502      0.802       1.23         63        640: 100%|██████████| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.732      0.865      0.887      0.587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/30      7.32G     0.8242     0.7621      1.205         39        640: 100%|██████████| 103/103 [00:54<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.819      0.802      0.868      0.636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/30      7.32G     0.7987     0.7074      1.193         31        640: 100%|██████████| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.699      0.808      0.837      0.603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/30      7.32G     0.7815     0.7324      1.186         42        640: 100%|██████████| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.854      0.832      0.913      0.691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/30      7.32G     0.7567      0.675      1.159         40        640: 100%|██████████| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.919       0.85      0.936      0.731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/30      7.32G     0.7243      0.635      1.146         36        640: 100%|██████████| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.885      0.862      0.923      0.699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/30      7.32G     0.7108     0.6213      1.136         30        640: 100%|██████████| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.871      0.913      0.885      0.654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/30      7.32G     0.6086     0.4823      1.067         21        640: 100%|██████████| 103/103 [00:54<00:00,  1.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137       0.85      0.895      0.924      0.739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/30      7.32G     0.5938     0.4542      1.048         35        640: 100%|██████████| 103/103 [00:53<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.891      0.908      0.951      0.774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/30      7.32G     0.5456     0.4008      1.029         19        640: 100%|██████████| 103/103 [00:53<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137       0.82       0.89      0.949      0.732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/30      7.32G     0.5412     0.3931      1.014         19        640: 100%|██████████| 103/103 [00:53<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.918      0.811      0.954      0.758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/30      7.32G     0.5271     0.3608      1.002         24        640: 100%|██████████| 103/103 [00:53<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.824      0.917      0.931      0.767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/30      7.32G     0.4914      0.343     0.9822         23        640: 100%|██████████| 103/103 [00:53<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.811      0.939      0.968        0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/30      7.32G     0.4689     0.3212      0.966         18        640: 100%|██████████| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.952      0.919      0.958      0.803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/30      7.32G     0.4554     0.3119     0.9594         17        640: 100%|██████████| 103/103 [00:54<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.891      0.888       0.91      0.772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/30      7.32G     0.4338     0.2931     0.9451         21        640: 100%|██████████| 103/103 [00:53<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.899      0.888      0.946      0.821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/30      7.32G     0.4192     0.2869     0.9372         28        640: 100%|██████████| 103/103 [00:54<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.917      0.897      0.953      0.819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "30 epochs completed in 0.497 hours.\n",
            "Optimizer stripped from multi_class_detection/merge_run4/weights/last.pt, 52.0MB\n",
            "Optimizer stripped from multi_class_detection/merge_run4/weights/best.pt, 52.0MB\n",
            "\n",
            "Validating multi_class_detection/merge_run4/weights/best.pt...\n",
            "Ultralytics 8.3.124 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 92 layers, 25,842,655 parameters, 0 gradients, 78.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:04<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.898      0.888      0.946       0.82\n",
            "               package         47         51      0.907      0.863      0.938      0.761\n",
            "                   bag         19         21      0.992      0.905      0.949      0.905\n",
            "          other_person         47         47          1      0.899      0.995      0.922\n",
            "       delivery_worker         14         14          1      0.773      0.937      0.712\n",
            "         food_delivery          4          4      0.594          1      0.912      0.801\n",
            "Speed: 0.5ms preprocess, 11.0ms inference, 0.0ms loss, 7.2ms postprocess per image\n",
            "Results saved to \u001b[1mmulti_class_detection/merge_run4\u001b[0m\n",
            "模型訓練完成。結果保存在 'multi_class_detection/merge_run' 資料夾中。\n",
            "開始模型驗證...\n",
            "Ultralytics 8.3.124 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 92 layers, 25,842,655 parameters, 0 gradients, 78.7 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1401.5±469.4 MB/s, size: 39.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/merged_dataset/valid/labels.cache... 131 images, 1 backgrounds, 0 corrupt: 100%|██████████| 132/132 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        132        137      0.899      0.888      0.946      0.822\n",
            "               package         47         51      0.907      0.863      0.938      0.766\n",
            "                   bag         19         21      0.992      0.905      0.949      0.905\n",
            "          other_person         47         47          1      0.899      0.995      0.923\n",
            "       delivery_worker         14         14          1      0.771      0.937      0.713\n",
            "         food_delivery          4          4      0.598          1      0.912      0.801\n",
            "Speed: 2.6ms preprocess, 21.4ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
            "Results saved to \u001b[1mmulti_class_detection/merge_run42\u001b[0m\n",
            "模型驗證完成。\n"
          ]
        }
      ],
      "source": [
        "# --- 4. 模型訓練 ---\n",
        "\n",
        "print(\"開始模型訓練...\")\n",
        "\n",
        "# 載入預訓練模型 (例如 yolov8m.pt)\n",
        "model = YOLO('yolov8m.pt')\n",
        "\n",
        "# 開始訓練\n",
        "try:\n",
        "    results = model.train(\n",
        "        data=str(merged_data_yaml_path), # 使用修正後的 data.yaml 路徑\n",
        "        epochs=30,  # 根據需要調整訓練輪數\n",
        "        imgsz=640,\n",
        "        batch=16,   # 根據您的 GPU 記憶體調整\n",
        "        patience=10,\n",
        "        save=True,\n",
        "        project=\"multi_class_detection\", # 專案名稱\n",
        "        name=\"merge_run\",   # 訓練運行名稱\n",
        "        # device=0, # 如果有 GPU，取消註解並指定 GPU 索引\n",
        "        # workers=8 # 根據您的 CPU 核心數調整\n",
        "    )\n",
        "    print(\"模型訓練完成。結果保存在 'multi_class_detection/merge_run' 資料夾中。\")\n",
        "\n",
        "    # (可選) 進行驗證\n",
        "    print(\"開始模型驗證...\")\n",
        "    validation_results = model.val()\n",
        "    print(\"模型驗證完成。\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"模型訓練或驗證過程中發生錯誤: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVXeL9cJJ-8R",
        "outputId": "c49ce072-1e8f-4ff8-b099-ccb22af54f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class package             : \tPrecision = 0.9069, \tRecall = 0.8627\n",
            "Class bag                 : \tPrecision = 0.9921, \tRecall = 0.9048\n",
            "Class other_person        : \tPrecision = 1.0000, \tRecall = 0.8985\n",
            "Class delivery_worker     : \tPrecision = 1.0000, \tRecall = 0.7715\n",
            "Class food_delivery       : \tPrecision = 0.5980, \tRecall = 1.0000\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "# 讀取 data.yaml 檔案以獲取類別名稱\n",
        "with open(f\"{merged_dataset_dir}/data.yaml\", 'r') as f:\n",
        "    data = yaml.safe_load(f)\n",
        "class_names = data['names']  # 獲取類別名稱字典\n",
        "\n",
        "# Get precision and recall for each class\n",
        "precision = validation_results.box.p\n",
        "recall = validation_results.box.r\n",
        "\n",
        "# Print precision and recall for each class with names\n",
        "for i, (p, r) in enumerate(zip(precision, recall)):\n",
        "    class_name = class_names[i]  # 獲取類別名稱\n",
        "    print(f\"Class {class_name:<20}: \\tPrecision = {p:.4f}, \\tRecall = {r:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovKdyEe-g5mJ"
      },
      "source": [
        "## 模型部署\n",
        "\n",
        "### 1. 模型導出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dThIYydAg8Bv",
        "outputId": "73418b7f-f3d1-421a-d79d-c0356e9ed66d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.124 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'multi_class_detection/merge_run4/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 9, 8400) (49.6 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim>=0.1.46', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxslim>=0.1.46\n",
            "  Downloading onnxslim-0.1.51-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.12.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.12.0) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxslim>=0.1.46) (1.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxslim>=0.1.46) (24.2)\n",
            "Collecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxslim>=0.1.46) (1.3.0)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/16.0 MB 321.9 MB/s eta 0:00:00\n",
            "Downloading onnxslim-0.1.51-py3-none-any.whl (145 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145.6/145.6 kB 326.0 MB/s eta 0:00:00\n",
            "Downloading onnxruntime_gpu-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (280.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.8/280.8 MB 3.6 MB/s eta 0:00:00\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 201.8 MB/s eta 0:00:00\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 306.3 MB/s eta 0:00:00\n",
            "Installing collected packages: onnx, humanfriendly, onnxslim, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-gpu-1.21.1 onnxslim-0.1.51\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 22.0s, installed 3 packages: ['onnx>=1.12.0', 'onnxslim>=0.1.46', 'onnxruntime-gpu']\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.51...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 45.5s, saved as 'multi_class_detection/merge_run4/weights/best.onnx' (98.7 MB)\n",
            "\n",
            "Export complete (48.8s)\n",
            "Results saved to \u001b[1m/content/multi_class_detection/merge_run4/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=multi_class_detection/merge_run4/weights/best.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=multi_class_detection/merge_run4/weights/best.onnx imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "Ultralytics 8.3.124 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'multi_class_detection/merge_run4/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 9, 8400) (49.6 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.51...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 26.8s, saved as 'multi_class_detection/merge_run4/weights/best.onnx' (98.7 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['tensorrt>7.0.0,!=10.1.0'] not found, attempting AutoUpdate...\n",
            "Collecting tensorrt!=10.1.0,>7.0.0\n",
            "  Downloading tensorrt-10.10.0.31.tar.gz (40 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.7/40.7 kB 5.7 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tensorrt_cu12==10.10.0.31 (from tensorrt!=10.1.0,>7.0.0)\n",
            "  Downloading tensorrt_cu12-10.10.0.31.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tensorrt_cu12_libs==10.10.0.31 (from tensorrt_cu12==10.10.0.31->tensorrt!=10.1.0,>7.0.0)\n",
            "  Downloading tensorrt_cu12_libs-10.10.0.31.tar.gz (708 bytes)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting tensorrt_cu12_bindings==10.10.0.31 (from tensorrt_cu12==10.10.0.31->tensorrt!=10.1.0,>7.0.0)\n",
            "  Downloading tensorrt_cu12_bindings-10.10.0.31-cp311-none-manylinux_2_28_x86_64.whl.metadata (607 bytes)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from tensorrt_cu12_libs==10.10.0.31->tensorrt_cu12==10.10.0.31->tensorrt!=10.1.0,>7.0.0) (12.4.127)\n",
            "Downloading tensorrt_cu12_bindings-10.10.0.31-cp311-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 44.6 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: tensorrt, tensorrt_cu12, tensorrt_cu12_libs\n",
            "  Building wheel for tensorrt (setup.py): started\n",
            "  Building wheel for tensorrt (setup.py): finished with status 'done'\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.10.0.31-py2.py3-none-any.whl size=46636 sha256=4dfb630fc7bee795addf6b0fdb2936628a24751886b76f5641f0d9d16e0d7e31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2bqwe2kx/wheels/01/04/6b/680eb64bea852e9183d35f8112879c19d02a37c4a7a3e9ab19\n",
            "  Building wheel for tensorrt_cu12 (setup.py): started\n",
            "  Building wheel for tensorrt_cu12 (setup.py): finished with status 'done'\n",
            "  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.10.0.31-py2.py3-none-any.whl size=17483 sha256=e2dc662e7c73bf9404e3fa793e376e9414041c7d6f5633924c490f520f588e98\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2bqwe2kx/wheels/ef/6b/47/10dbe13e12085c48a6b521b5c62b7c4d27e9192fbc7069e27d\n",
            "  Building wheel for tensorrt_cu12_libs (pyproject.toml): started\n",
            "  Building wheel for tensorrt_cu12_libs (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for tensorrt_cu12_libs: filename=tensorrt_cu12_libs-10.10.0.31-py2.py3-none-manylinux_2_28_x86_64.whl size=3404991480 sha256=02a17ce9d84979d325a085e6492cfb71fae1039d4e7f55efe4de8e96d0410255\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2bqwe2kx/wheels/ff/db/8e/03e6696471217bb7beca639a1f62de8c3b77dac3a53cafb068\n",
            "Successfully built tensorrt tensorrt_cu12 tensorrt_cu12_libs\n",
            "Installing collected packages: tensorrt_cu12_bindings, tensorrt_cu12_libs, tensorrt_cu12, tensorrt\n",
            "Successfully installed tensorrt-10.10.0.31 tensorrt_cu12-10.10.0.31 tensorrt_cu12_bindings-10.10.0.31 tensorrt_cu12_libs-10.10.0.31\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 284.1s, installed 1 package: ['tensorrt>7.0.0,!=10.1.0']\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(-1, 9, -1) DataType.FLOAT\n",
            "WARNING ⚠️ \u001b[34m\u001b[1mTensorRT:\u001b[0m 'dynamic=True' model requires max batch size, i.e. 'batch=16'\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as multi_class_detection/merge_run4/weights/best.engine\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 429.8s, saved as 'multi_class_detection/merge_run4/weights/best.engine' (127.2 MB)\n",
            "\n",
            "Export complete (430.0s)\n",
            "Results saved to \u001b[1m/content/multi_class_detection/merge_run4/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=multi_class_detection/merge_run4/weights/best.engine imgsz=640  \n",
            "Validate:        yolo val task=detect model=multi_class_detection/merge_run4/weights/best.engine imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "Ultralytics 8.3.124 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'multi_class_detection/merge_run4/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 9, 8400) (49.6 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['coremltools>=8.0'] not found, attempting AutoUpdate...\n",
            "Collecting coremltools>=8.0\n",
            "  Downloading coremltools-8.3.0-cp311-none-manylinux1_x86_64.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (24.2)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.11/dist-packages (from coremltools>=8.0) (25.3.0)\n",
            "Collecting cattrs (from coremltools>=8.0)\n",
            "  Downloading cattrs-24.1.3-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pyaml (from coremltools>=8.0)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml->coremltools>=8.0) (6.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->coremltools>=8.0) (1.3.0)\n",
            "Downloading coremltools-8.3.0-cp311-none-manylinux1_x86_64.whl (2.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 50.5 MB/s eta 0:00:00\n",
            "Downloading cattrs-24.1.3-py3-none-any.whl (66 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 kB 78.4 MB/s eta 0:00:00\n",
            "Downloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, cattrs, coremltools\n",
            "Successfully installed cattrs-24.1.3 coremltools-8.3.0 pyaml-25.1.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 5.1s, installed 1 package: ['coremltools>=8.0']\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:coremltools:scikit-learn version 1.6.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\n",
            "WARNING:coremltools:XGBoost version 2.1.4 has not been tested with coremltools. You may run into unexpected errors. XGBoost 1.4.2 is the most recent version that has been tested.\n",
            "WARNING:coremltools:TensorFlow version 2.18.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
            "WARNING:coremltools:Torch version 2.6.0+cu124 has not been tested with coremltools. You may run into unexpected errors. Torch 2.5.0 is the most recent version that has been tested.\n",
            "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLCPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLGPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLNeuralEngineComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLComputePlanProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
            "WARNING:coremltools:Failed to load _MLModelAssetProxy: No module named 'coremltools.libcoremlpython'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mCoreML:\u001b[0m starting export with coremltools 8.3.0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 720/721 [00:00<00:00, 3064.27 ops/s]\n",
            "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 87.16 passes/s]\n",
            "Running MIL default pipeline: 100%|██████████| 89/89 [00:03<00:00, 24.75 passes/s]\n",
            "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 88.08 passes/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mCoreML:\u001b[0m export success ✅ 27.4s, saved as 'multi_class_detection/merge_run4/weights/best.mlpackage' (49.5 MB)\n",
            "\n",
            "Export complete (29.5s)\n",
            "Results saved to \u001b[1m/content/multi_class_detection/merge_run4/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=multi_class_detection/merge_run4/weights/best.mlpackage imgsz=640  \n",
            "Validate:        yolo val task=detect model=multi_class_detection/merge_run4/weights/best.mlpackage imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "Ultralytics 8.3.124 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'multi_class_detection/merge_run4/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 9, 8400) (49.6 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'ai-edge-litert>=1.2.0', 'onnx2tf>=1.26.3'] not found, attempting AutoUpdate...\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting sng4onnx>=1.0.1\n",
            "  Downloading sng4onnx-1.0.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting onnx_graphsurgeon>=0.3.26\n",
            "  Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting ai-edge-litert>=1.2.0\n",
            "  Downloading ai_edge_litert-1.2.0-cp311-cp311-manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting onnx2tf>=1.26.3\n",
            "  Downloading onnx2tf-1.27.2-py3-none-any.whl.metadata (147 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.7/147.7 kB 8.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from onnx_graphsurgeon>=0.3.26) (2.0.2)\n",
            "Requirement already satisfied: onnx>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from onnx_graphsurgeon>=0.3.26) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from ai-edge-litert>=1.2.0) (25.2.10)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.14.0->onnx_graphsurgeon>=0.3.26) (5.29.4)\n",
            "Downloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\n",
            "Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl (57 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.9/57.9 kB 314.5 MB/s eta 0:00:00\n",
            "Downloading ai_edge_litert-1.2.0-cp311-cp311-manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 101.0 MB/s eta 0:00:00\n",
            "Downloading onnx2tf-1.27.2-py3-none-any.whl (446 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 446.6/446.6 kB 225.6 MB/s eta 0:00:00\n",
            "Installing collected packages: sng4onnx, onnx2tf, ai-edge-litert, onnx_graphsurgeon\n",
            "Successfully installed ai-edge-litert-1.2.0 onnx2tf-1.27.2 onnx_graphsurgeon-0.5.8 sng4onnx-1.0.4\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 5.5s, installed 4 packages: ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'ai-edge-litert>=1.2.0', 'onnx2tf>=1.26.3']\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.18.0...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/calibration_image_sample_data_20x128x128x3_float32.npy.zip to 'calibration_image_sample_data_20x128x128x3_float32.npy.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.11M/1.11M [00:00<00:00, 29.8MB/s]\n",
            "Unzipping calibration_image_sample_data_20x128x128x3_float32.npy.zip to /content/calibration_image_sample_data_20x128x128x3_float32.npy...: 100%|██████████| 1/1 [00:00<00:00, 38.40file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.51...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 8.1s, saved as 'multi_class_detection/merge_run4/weights/best.onnx' (98.8 MB)\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.27.2...\n",
            "Saved artifact at 'multi_class_detection/merge_run4/weights/best_saved_model'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serving_default'\n",
            "  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 640, 640, 3), dtype=tf.float32, name='images')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(1, 9, 8400), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136326922341456: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326922341264: TensorSpec(shape=(3, 3, 3, 48), dtype=tf.float32, name=None)\n",
            "  136326922341840: TensorSpec(shape=(48,), dtype=tf.float32, name=None)\n",
            "  136326922345296: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326922344912: TensorSpec(shape=(3, 3, 48, 96), dtype=tf.float32, name=None)\n",
            "  136326922345872: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922341648: TensorSpec(shape=(1, 1, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922346256: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922342992: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922346640: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922348560: TensorSpec(shape=(3, 3, 48, 48), dtype=tf.float32, name=None)\n",
            "  136326922348944: TensorSpec(shape=(48,), dtype=tf.float32, name=None)\n",
            "  136326922347600: TensorSpec(shape=(3, 3, 48, 48), dtype=tf.float32, name=None)\n",
            "  136326922349328: TensorSpec(shape=(48,), dtype=tf.float32, name=None)\n",
            "  136326922349136: TensorSpec(shape=(3, 3, 48, 48), dtype=tf.float32, name=None)\n",
            "  136326922349520: TensorSpec(shape=(48,), dtype=tf.float32, name=None)\n",
            "  136326922347216: TensorSpec(shape=(3, 3, 48, 48), dtype=tf.float32, name=None)\n",
            "  136326922349712: TensorSpec(shape=(48,), dtype=tf.float32, name=None)\n",
            "  136326922346448: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922346832: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922350096: TensorSpec(shape=(1, 1, 192, 96), dtype=tf.float32, name=None)\n",
            "  136326922350288: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922350480: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326922347792: TensorSpec(shape=(3, 3, 96, 192), dtype=tf.float32, name=None)\n",
            "  136326922349904: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326922350672: TensorSpec(shape=(1, 1, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326922351056: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326922351440: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922351248: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922352016: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922353360: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922352592: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922353744: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922352784: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922353936: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922352208: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922354128: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922353552: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326922354320: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326755566032: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326755565840: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922354512: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326755565648: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326755566608: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326755566416: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326922351632: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326922350864: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755567184: TensorSpec(shape=(1, 1, 576, 192), dtype=tf.float32, name=None)\n",
            "  136326755566992: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755567376: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326755566224: TensorSpec(shape=(3, 3, 192, 384), dtype=tf.float32, name=None)\n",
            "  136326755566800: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326755567568: TensorSpec(shape=(1, 1, 384, 384), dtype=tf.float32, name=None)\n",
            "  136326755567952: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326755568336: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755568144: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755568912: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755570256: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755569488: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755570640: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755569680: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755570832: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755569104: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755571024: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755571408: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755571600: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755570448: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755571792: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755572176: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755572368: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755571216: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755572560: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755568528: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755567760: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755573136: TensorSpec(shape=(1, 1, 1152, 384), dtype=tf.float32, name=None)\n",
            "  136326755572944: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326755573328: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326755571984: TensorSpec(shape=(3, 3, 384, 576), dtype=tf.float32, name=None)\n",
            "  136326755572752: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326755573520: TensorSpec(shape=(1, 1, 576, 576), dtype=tf.float32, name=None)\n",
            "  136326755574288: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326755573712: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755573904: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755574864: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326755576208: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326755575440: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326755576592: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326755575632: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326755576784: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326755575056: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326755576976: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326755574480: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755574096: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755577552: TensorSpec(shape=(1, 1, 1152, 576), dtype=tf.float32, name=None)\n",
            "  136326755577360: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326755576400: TensorSpec(shape=(1, 1, 576, 288), dtype=tf.float32, name=None)\n",
            "  136326755577744: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326755577936: TensorSpec(shape=(1, 1, 1152, 576), dtype=tf.float32, name=None)\n",
            "  136326755578128: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326755578512: TensorSpec(shape=(1, 1, 960, 384), dtype=tf.float32, name=None)\n",
            "  136326755578320: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326755578896: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755578704: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755579472: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755580816: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755580048: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755581200: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755581008: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755580240: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755581392: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326755581584: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326755579088: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755577168: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326755579664: TensorSpec(shape=(1, 1, 768, 384), dtype=tf.float32, name=None)\n",
            "  136326755581776: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326751404112: TensorSpec(shape=(1, 1, 576, 192), dtype=tf.float32, name=None)\n",
            "  136326751404496: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751404880: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751404688: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751405456: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326751406800: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326751406032: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326751407184: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326751406992: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326751406224: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326751407376: TensorSpec(shape=(3, 3, 96, 96), dtype=tf.float32, name=None)\n",
            "  136326751407568: TensorSpec(shape=(96,), dtype=tf.float32, name=None)\n",
            "  136326751405072: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751404304: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751407952: TensorSpec(shape=(1, 1, 384, 192), dtype=tf.float32, name=None)\n",
            "  136326751407760: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751408336: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326751408144: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751405648: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751410064: TensorSpec(shape=(1, 1, 576, 384), dtype=tf.float32, name=None)\n",
            "  136326751410256: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326751411216: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751411408: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751412944: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751414096: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751414288: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751413904: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751412752: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751412368: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751413712: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751412176: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751411600: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751411792: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751414672: TensorSpec(shape=(1, 1, 768, 384), dtype=tf.float32, name=None)\n",
            "  136326751413520: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326751415056: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136326751414864: TensorSpec(shape=(3, 3, 384, 384), dtype=tf.float32, name=None)\n",
            "  136326751414480: TensorSpec(shape=(384,), dtype=tf.float32, name=None)\n",
            "  136326751416784: TensorSpec(shape=(1, 1, 960, 576), dtype=tf.float32, name=None)\n",
            "  136326751416976: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326751417936: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751418128: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326748619024: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326748619216: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326751419856: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326751419472: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326751420240: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326751419664: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326751418896: TensorSpec(shape=(3, 3, 288, 288), dtype=tf.float32, name=None)\n",
            "  136326751419088: TensorSpec(shape=(288,), dtype=tf.float32, name=None)\n",
            "  136326751418320: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326751418512: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136326748619792: TensorSpec(shape=(1, 1, 1152, 576), dtype=tf.float32, name=None)\n",
            "  136326748619408: TensorSpec(shape=(576,), dtype=tf.float32, name=None)\n",
            "  136326748620368: TensorSpec(shape=(3, 3, 576, 192), dtype=tf.float32, name=None)\n",
            "  136326748619984: TensorSpec(shape=(3, 3, 576, 64), dtype=tf.float32, name=None)\n",
            "  136326751415632: TensorSpec(shape=(3, 3, 384, 192), dtype=tf.float32, name=None)\n",
            "  136326751415248: TensorSpec(shape=(3, 3, 384, 64), dtype=tf.float32, name=None)\n",
            "  136326751408912: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751408528: TensorSpec(shape=(3, 3, 192, 64), dtype=tf.float32, name=None)\n",
            "  136326748619600: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326748620176: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751415824: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751415440: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751409104: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751408720: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326748620560: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326748620944: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326751416016: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751416208: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326751409296: TensorSpec(shape=(3, 3, 192, 192), dtype=tf.float32, name=None)\n",
            "  136326751409488: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326748620752: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326748621136: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751416400: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751416592: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751409680: TensorSpec(shape=(192,), dtype=tf.float32, name=None)\n",
            "  136326751409872: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326748621328: TensorSpec(shape=(1, 1, 192, 5), dtype=tf.float32, name=None)\n",
            "  136326748621712: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326751417168: TensorSpec(shape=(1, 1, 192, 5), dtype=tf.float32, name=None)\n",
            "  136326751417744: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326751410448: TensorSpec(shape=(1, 1, 192, 5), dtype=tf.float32, name=None)\n",
            "  136326751411024: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  136326748621904: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326748621520: TensorSpec(shape=(5,), dtype=tf.float32, name=None)\n",
            "  136326751417360: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751417552: TensorSpec(shape=(5,), dtype=tf.float32, name=None)\n",
            "  136326751410640: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136326751410832: TensorSpec(shape=(5,), dtype=tf.float32, name=None)\n",
            "  136326748624016: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748623824: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748624400: TensorSpec(shape=(1, 1, 16, 1), dtype=tf.float32, name=None)\n",
            "  136326748622288: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748623056: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748625360: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748625552: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748626704: TensorSpec(shape=(1, 2, 8400), dtype=tf.float32, name=None)\n",
            "  136326748626128: TensorSpec(shape=(1, 2, 8400), dtype=tf.float32, name=None)\n",
            "  136326748623248: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136326748623440: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success ✅ 50.0s, saved as 'multi_class_detection/merge_run4/weights/best_saved_model' (247.3 MB)\n",
            "\n",
            "Export complete (51.8s)\n",
            "Results saved to \u001b[1m/content/multi_class_detection/merge_run4/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=multi_class_detection/merge_run4/weights/best_saved_model imgsz=640  \n",
            "Validate:        yolo val task=detect model=multi_class_detection/merge_run4/weights/best_saved_model imgsz=640 data=/content/merged_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'multi_class_detection/merge_run4/weights/best_saved_model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# 導出為 ONNX 格式（適用於多種推理框架）\n",
        "model.export(format='onnx', dynamic=True, simplify=True)\n",
        "\n",
        "# 導出為 TensorRT 格式（NVIDIA GPU 上的高性能推理）\n",
        "model.export(format='engine', dynamic=True, simplify=True, device=0)\n",
        "\n",
        "# 導出為 CoreML 格式（適用於 iOS 設備）\n",
        "model.export(format='coreml', simplify=True)\n",
        "\n",
        "# 導出為 TensorFlow SavedModel 格式\n",
        "model.export(format='saved_model', simplify=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmSxAhPnhBhU"
      },
      "source": [
        "### 2. 部署到出入口即時包裹與人物意圖偵測系統"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qdiAxzcUOQN"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output  # << 重要：清除上個frame的顯示"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 載入模型\n",
        "model = YOLO('/content/multi_class_detection/merge_run4/weights/best.pt')\n",
        "\n",
        "# 設定參數\n",
        "conf_threshold = 0.5\n",
        "video_source = 1\n",
        "detect_interval = 15  # 每15張推論一次\n",
        "snapshot_interval_sec = 5  # 每5秒存一張快照\n",
        "snapshot_dir = \"/content/snapshots\"\n",
        "os.makedirs(snapshot_dir, exist_ok=True)\n",
        "\n",
        "if video_source == 0:\n",
        "    def get_camera_stream(ip, username, password, channel=1):\n",
        "        rtsp_url = f\"rtsp://{username}:{password}@{ip}/Streaming/Channels/{channel}01\"\n",
        "        cap = cv2.VideoCapture(rtsp_url)\n",
        "        return cap\n",
        "    camera = get_camera_stream(\"106.107.183.134\", \"admin\", \"Qazwsx1122\")\n",
        "elif video_source == 1:\n",
        "    video_path = \"videoplayback.mp4\"  # 替換成你的影片路徑\n",
        "    camera = cv2.VideoCapture(video_path)\n",
        "else:\n",
        "    print(\"Invalid video source selected.\")\n",
        "    exit()\n",
        "\n",
        "# 取得影片資訊\n",
        "fps = camera.get(cv2.CAP_PROP_FPS) or 30  # 預設30fps\n",
        "frame_width = int(camera.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "snapshot_interval_frames = int(fps * snapshot_interval_sec)\n",
        "\n",
        "# 設定執行時間\n",
        "end_time = time.time() + 300  # 最長5分鐘\n",
        "\n",
        "frame_count = 0\n",
        "inference_count = 0\n",
        "latest_detected_classes = defaultdict(int)   # 最新一輪推論結果\n",
        "cumulative_detected_classes = defaultdict(int)  # 累積所有推論結果\n",
        "latest_names = []\n",
        "frames = []  # 保存所有處理後的frame\n",
        "\n",
        "# 主循環\n",
        "while time.time() < end_time:\n",
        "    ret, frame = camera.read()\n",
        "    if not ret:\n",
        "        print(\"無法讀取影像，可能是影片結束了。\")\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    # 每detect_interval張推論一次\n",
        "    if frame_count % detect_interval == 0:\n",
        "        results = model(frame, conf=conf_threshold)\n",
        "        boxes = results[0].boxes.data.cpu().numpy() if results[0].boxes.data is not None else []\n",
        "        latest_names = results[0].names\n",
        "        inference_count += 1\n",
        "\n",
        "        # 更新最新推論結果 + 累積推論統計\n",
        "        latest_detected_classes = defaultdict(int)\n",
        "        for det in boxes:\n",
        "            _, _, _, _, score, class_id = det\n",
        "            if score >= conf_threshold:\n",
        "                class_name = latest_names[int(class_id)]\n",
        "                latest_detected_classes[class_name] += 1\n",
        "                cumulative_detected_classes[class_name] += 1\n",
        "\n",
        "    # 畫推論資訊文字在frame上\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    # 左上角推論次數\n",
        "    cv2.putText(annotated_frame, f\"Inference Count: {inference_count}\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "    # 右上角目前frame編號\n",
        "    cv2.putText(annotated_frame, f\"Frame: {frame_count}\", (frame_width - 250, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2)\n",
        "\n",
        "    # 左下角顯示累積偵測統計\n",
        "    y_offset = 70\n",
        "    for cls_name, count in cumulative_detected_classes.items():\n",
        "        text = f\"{cls_name}: {count}\"\n",
        "        cv2.putText(annotated_frame, text, (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "        y_offset += 40\n",
        "\n",
        "    # 存進frames清單\n",
        "    frames.append(annotated_frame)\n",
        "\n",
        "    # 每隔snapshot_interval_frames儲存一張快照\n",
        "    if frame_count % snapshot_interval_frames == 0:\n",
        "        snapshot_path = os.path.join(snapshot_dir, f\"snapshot_frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(snapshot_path, annotated_frame)\n",
        "        print(f\"儲存快照: {snapshot_path}\")\n",
        "\n",
        "camera.release()\n",
        "print(f\"總共處理了 {len(frames)} 張frame。\")\n",
        "\n",
        "# --- 合成影片 ---\n",
        "output_video_path = '/content/inference_output.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "for frame in frames:\n",
        "    out.write(frame)\n",
        "out.release()\n",
        "\n",
        "print(f\"影片已儲存到 {output_video_path}\")\n",
        "\n",
        "# 🔥 🔥 🔥 在最後列印累積推論結果 🔥 🔥 🔥\n",
        "print(\"\\n===== 最後累積推論結果 =====\")\n",
        "for cls_name, count in cumulative_detected_classes.items():\n",
        "    print(f\"{cls_name}: {count} 次\")\n",
        "print(\"==============================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FBdG0vk7ip3C",
        "outputId": "4e47791f-f379-489c-919b-6a8b9366fc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 (no detections), 240.4ms\n",
            "Speed: 8.6ms preprocess, 240.4ms inference, 15.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 80.0ms\n",
            "Speed: 10.6ms preprocess, 80.0ms inference, 14.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 2.8ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.5ms\n",
            "Speed: 3.5ms preprocess, 29.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 39.7ms\n",
            "Speed: 2.5ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 5.1ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 6.6ms preprocess, 24.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 26.1ms\n",
            "Speed: 2.6ms preprocess, 26.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_149.jpg\n",
            "\n",
            "0: 384x640 (no detections), 48.4ms\n",
            "Speed: 4.7ms preprocess, 48.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.9ms\n",
            "Speed: 2.5ms preprocess, 31.9ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 41.1ms\n",
            "Speed: 2.6ms preprocess, 41.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 30.9ms\n",
            "Speed: 4.6ms preprocess, 30.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 48.3ms\n",
            "Speed: 6.0ms preprocess, 48.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 44.4ms\n",
            "Speed: 2.5ms preprocess, 44.4ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 33.6ms\n",
            "Speed: 2.5ms preprocess, 33.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 35.8ms\n",
            "Speed: 7.8ms preprocess, 35.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 35.1ms\n",
            "Speed: 3.8ms preprocess, 35.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_298.jpg\n",
            "\n",
            "0: 384x640 (no detections), 44.9ms\n",
            "Speed: 4.9ms preprocess, 44.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 57.0ms\n",
            "Speed: 10.9ms preprocess, 57.0ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 65.2ms\n",
            "Speed: 4.4ms preprocess, 65.2ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.6ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 28.4ms\n",
            "Speed: 2.8ms preprocess, 28.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 24.7ms\n",
            "Speed: 2.4ms preprocess, 24.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.9ms\n",
            "Speed: 2.6ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.3ms\n",
            "Speed: 4.7ms preprocess, 31.3ms inference, 15.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_447.jpg\n",
            "\n",
            "0: 384x640 1 other_person, 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 30.7ms\n",
            "Speed: 2.9ms preprocess, 30.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 28.0ms\n",
            "Speed: 2.6ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 26.4ms\n",
            "Speed: 2.6ms preprocess, 26.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.8ms\n",
            "Speed: 2.5ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 30.9ms\n",
            "Speed: 3.4ms preprocess, 30.9ms inference, 20.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 42.0ms\n",
            "Speed: 2.8ms preprocess, 42.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 27.7ms\n",
            "Speed: 4.0ms preprocess, 27.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_596.jpg\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 2.6ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.6ms\n",
            "Speed: 8.0ms preprocess, 29.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.9ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 35.4ms\n",
            "Speed: 2.6ms preprocess, 35.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 31.8ms\n",
            "Speed: 2.8ms preprocess, 31.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 other_person, 55.8ms\n",
            "Speed: 3.9ms preprocess, 55.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 33.5ms\n",
            "Speed: 21.1ms preprocess, 33.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 40.5ms\n",
            "Speed: 2.4ms preprocess, 40.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.4ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.8ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_745.jpg\n",
            "\n",
            "0: 384x640 (no detections), 29.2ms\n",
            "Speed: 5.0ms preprocess, 29.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.6ms\n",
            "Speed: 2.7ms preprocess, 32.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 3.7ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.7ms preprocess, 24.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.7ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.9ms\n",
            "Speed: 2.4ms preprocess, 24.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 3.3ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.2ms preprocess, 24.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_894.jpg\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.4ms preprocess, 24.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.8ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 3.5ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.8ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.5ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.9ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 3.3ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.9ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 24.7ms\n",
            "Speed: 3.7ms preprocess, 24.7ms inference, 11.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.1ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_1043.jpg\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 2.7ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 packages, 24.6ms\n",
            "Speed: 2.9ms preprocess, 24.6ms inference, 8.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.0ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.6ms\n",
            "Speed: 3.0ms preprocess, 24.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 3.6ms preprocess, 24.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 24.6ms\n",
            "Speed: 2.6ms preprocess, 24.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 25.4ms\n",
            "Speed: 2.8ms preprocess, 25.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 package, 24.6ms\n",
            "Speed: 2.4ms preprocess, 24.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_1192.jpg\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.0ms\n",
            "Speed: 2.3ms preprocess, 25.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.9ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 bag, 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.8ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 3.0ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 36.9ms\n",
            "Speed: 34.4ms preprocess, 36.9ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 311.3ms\n",
            "Speed: 172.4ms preprocess, 311.3ms inference, 63.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 3.2ms preprocess, 24.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_1341.jpg\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.4ms preprocess, 24.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 4.3ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.8ms\n",
            "Speed: 2.7ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 4.1ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.8ms preprocess, 24.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 24.7ms\n",
            "Speed: 3.4ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "無法讀取影像，可能是影片結束了。\n",
            "總共處理了 1469 張frame。\n",
            "影片已儲存到 /content/inference_output.mp4\n",
            "\n",
            "===== 最後累積推論結果 =====\n",
            "package: 10 次\n",
            "other_person: 12 次\n",
            "bag: 1 次\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# 載入模型\n",
        "# 請確保模型路徑正確\n",
        "model_path = '/content/multi_class_detection/merge_run4/weights/best.pt'\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"錯誤：找不到模型檔案 {model_path}。請確認路徑是否正確。\")\n",
        "    # 在 Colab 環境中，如果模型在 Google Drive，需要先掛載 Drive\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    # model_path = '/content/drive/MyDrive/path/to/your/model/best.pt' # 修改為您在 Drive 中的實際路徑\n",
        "    # if not os.path.exists(model_path):\n",
        "    #     exit()\n",
        "    exit() # 如果不在 Colab 或 Drive 中找不到，則退出\n",
        "\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# 設定參數\n",
        "conf_threshold = 0.5\n",
        "video_source = 1  # 0: RTSP, 1: 本地影片檔案\n",
        "detect_interval = 15  # 每15張推論一次\n",
        "snapshot_interval_sec = 5  # 每5秒存一張快照\n",
        "snapshot_dir = \"/content/snapshots\"\n",
        "os.makedirs(snapshot_dir, exist_ok=True)\n",
        "\n",
        "# --- 攝影機/影片來源設定 ---\n",
        "if video_source == 0:\n",
        "    # RTSP 來源設定 (請替換成您的實際資訊)\n",
        "    rtsp_ip = \"106.107.183.134\"\n",
        "    rtsp_user = \"admin\"\n",
        "    rtsp_pass = \"Qazwsx1122\"\n",
        "    rtsp_channel = 1\n",
        "    rtsp_url = f\"rtsp://{rtsp_user}:{rtsp_pass}@{rtsp_ip}/Streaming/Channels/{rtsp_channel}01\"\n",
        "    print(f\"嘗試連接 RTSP: {rtsp_url}\")\n",
        "    camera = cv2.VideoCapture(rtsp_url)\n",
        "elif video_source == 1:\n",
        "    # 本地影片檔案設定 (請替換成您的影片路徑)\n",
        "    video_path = \"videoplayback.mp4\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"錯誤：找不到影片檔案 {video_path}。請確認路徑是否正確。\")\n",
        "        # 在 Colab 環境中，如果影片在 Google Drive，需要先掛載 Drive\n",
        "        # from google.colab import drive\n",
        "        # drive.mount('/content/drive')\n",
        "        # video_path = '/content/drive/MyDrive/path/to/your/video.mp4' # 修改為您在 Drive 中的實際路徑\n",
        "        # if not os.path.exists(video_path):\n",
        "        #     exit()\n",
        "        exit() # 如果不在 Colab 或 Drive 中找不到，則退出\n",
        "    print(f\"讀取影片檔案: {video_path}\")\n",
        "    camera = cv2.VideoCapture(video_path)\n",
        "else:\n",
        "    print(\"錯誤：無效的影像來源選擇。\")\n",
        "    exit()\n",
        "\n",
        "# 檢查攝影機/影片是否成功開啟\n",
        "if not camera.isOpened():\n",
        "    print(\"錯誤：無法開啟影像來源。請檢查路徑、網路連線或 RTSP 認證資訊。\")\n",
        "    exit()\n",
        "\n",
        "# 取得影片資訊\n",
        "fps = camera.get(cv2.CAP_PROP_FPS)\n",
        "if fps is None or fps == 0:\n",
        "    print(\"警告：無法取得影片 FPS，預設為 30。\")\n",
        "    fps = 30\n",
        "frame_width = int(camera.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "snapshot_interval_frames = int(fps * snapshot_interval_sec)\n",
        "\n",
        "print(f\"影片資訊: {frame_width}x{frame_height} @ {fps:.2f} FPS\")\n",
        "\n",
        "# 設定執行時間 (例如：最長5分鐘)\n",
        "max_duration_sec = 300\n",
        "end_time = time.time() + max_duration_sec\n",
        "\n",
        "# 初始化計數器和儲存變數\n",
        "frame_count = 0\n",
        "inference_count = 0\n",
        "latest_detected_classes = defaultdict(int)   # 最新一輪推論結果\n",
        "cumulative_detected_classes = defaultdict(int)  # 累積所有推論結果\n",
        "latest_boxes_for_drawing = [] # 用於儲存最新偵測框，以便在非推論幀上繪製\n",
        "latest_names = model.names # 從模型直接獲取類別名稱\n",
        "frames = []  # 保存所有處理後的frame\n",
        "\n",
        "print(\"開始處理影像...\")\n",
        "# 主循環\n",
        "while time.time() < end_time:\n",
        "    ret, frame = camera.read()\n",
        "    if not ret:\n",
        "        print(\"無法讀取影像，可能是影片結束或連線中斷。\")\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "    annotated_frame = frame.copy() # 複製原始幀用於繪圖\n",
        "\n",
        "    # 每detect_interval張推論一次\n",
        "    if frame_count % detect_interval == 0:\n",
        "        inference_count += 1\n",
        "        print(f\"執行第 {inference_count} 次推論 (Frame: {frame_count})\")\n",
        "        results = model(frame, conf=conf_threshold)\n",
        "        # 獲取偵測框數據，如果沒有偵測到則為空列表\n",
        "        boxes = results[0].boxes.data.cpu().numpy() if results[0].boxes.data is not None else []\n",
        "        latest_boxes_for_drawing = boxes # 更新用於繪製的偵測框\n",
        "\n",
        "        # 更新最新推論結果 + 累積推論統計\n",
        "        latest_detected_classes = defaultdict(int)\n",
        "        for det in boxes:\n",
        "            # boxes 格式: [x1, y1, x2, y2, score, class_id]\n",
        "            score = det[4]\n",
        "            class_id = int(det[5])\n",
        "            if score >= conf_threshold:\n",
        "                class_name = latest_names[class_id]\n",
        "                latest_detected_classes[class_name] += 1\n",
        "                cumulative_detected_classes[class_name] += 1\n",
        "\n",
        "    # --- 在畫布上繪製偵測框和標籤 (使用最新偵測結果) ---\n",
        "    if latest_boxes_for_drawing is not None:\n",
        "        for det in latest_boxes_for_drawing:\n",
        "            x1, y1, x2, y2, score, class_id = det\n",
        "            if score >= conf_threshold:\n",
        "                class_id = int(class_id)\n",
        "                class_name = latest_names[class_id]\n",
        "\n",
        "                # 繪製偵測框 (綠色)\n",
        "                color = (0, 255, 0)\n",
        "                cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
        "\n",
        "                # 準備標籤文字\n",
        "                label = f\"{class_name}: {score:.2f}\"\n",
        "\n",
        "                # 計算文字大小和位置\n",
        "                (label_width, label_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
        "                # 確保標籤在圖片範圍內\n",
        "                label_y = max(label_height + 10, int(y1) - 10)\n",
        "                label_x = int(x1)\n",
        "\n",
        "                # 繪製標籤背景\n",
        "                cv2.rectangle(annotated_frame,\n",
        "                              (label_x, label_y - label_height - baseline),\n",
        "                              (label_x + label_width, label_y + baseline),\n",
        "                              color, cv2.FILLED)\n",
        "                # 繪製標籤文字 (黑色)\n",
        "                cv2.putText(annotated_frame, label, (label_x, label_y),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
        "    # --- 繪製結束 ---\n",
        "\n",
        "    # --- 在畫布上繪製其他資訊文字 ---\n",
        "    # 左上角推論次數 (紅色)\n",
        "    cv2.putText(annotated_frame, f\"Inference Count: {inference_count}\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "    # 右上角目前frame編號 (洋紅色)\n",
        "    cv2.putText(annotated_frame, f\"Frame: {frame_count}\", (frame_width - 250, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2)\n",
        "\n",
        "    # 左下角顯示累積偵測統計 (藍色)\n",
        "    y_offset = frame_height - 30 # 從底部往上畫\n",
        "    sorted_cumulative = sorted(cumulative_detected_classes.items(), key=lambda item: item[1], reverse=True)\n",
        "    for cls_name, count in sorted_cumulative:\n",
        "        text = f\"{cls_name}: {count}\"\n",
        "        cv2.putText(annotated_frame, text, (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "        y_offset -= 30 # 往上移動繪製位置\n",
        "        if y_offset < 60: # 避免覆蓋頂部文字\n",
        "            break\n",
        "    # --- 繪製結束 ---\n",
        "\n",
        "    # 將處理後的幀加入列表\n",
        "    frames.append(annotated_frame)\n",
        "\n",
        "    # 每隔 snapshot_interval_frames 儲存一張快照\n",
        "    if frame_count % snapshot_interval_frames == 0:\n",
        "        snapshot_path = os.path.join(snapshot_dir, f\"snapshot_frame_{frame_count}.jpg\")\n",
        "        try:\n",
        "            cv2.imwrite(snapshot_path, annotated_frame)\n",
        "            print(f\"儲存快照: {snapshot_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"錯誤：無法儲存快照 {snapshot_path}: {e}\")\n",
        "\n",
        "    # (可選) 在 Colab 中顯示即時影像 (可能影響性能)\n",
        "    # from google.colab.patches import cv2_imshow\n",
        "    # cv2_imshow(cv2.resize(annotated_frame, (frame_width // 2, frame_height // 2))) # 縮小顯示\n",
        "    # if cv2.waitKey(1) & 0xFF == ord('q'): # 在 Colab 中 waitKey 可能無法正常工作\n",
        "    #    break\n",
        "\n",
        "camera.release()\n",
        "print(f\"影像來源已關閉。總共處理了 {len(frames)} 張 frame。\")\n",
        "\n",
        "# --- 合成影片 ---\n",
        "if frames:\n",
        "    output_video_path = '/content/inference_output_with_boxes.mp4'\n",
        "    print(f\"開始合成影片到 {output_video_path}...\")\n",
        "    try:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v') # 或者使用 'XVID'\n",
        "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "        for i, frame_to_write in enumerate(frames):\n",
        "            out.write(frame_to_write)\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"已寫入 {i + 1}/{len(frames)} 幀到影片...\")\n",
        "\n",
        "        out.release()\n",
        "        print(f\"影片已成功儲存到 {output_video_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"錯誤：無法合成或儲存影片: {e}\")\n",
        "else:\n",
        "    print(\"沒有處理任何幀，無法合成影片。\")\n",
        "\n",
        "# --- 在最後列印累積推論結果 ---\n",
        "print(\"\\n===== 最後累積推論結果 =====\")\n",
        "if cumulative_detected_classes:\n",
        "    sorted_final_cumulative = sorted(cumulative_detected_classes.items(), key=lambda item: item[1], reverse=True)\n",
        "    for cls_name, count in sorted_final_cumulative:\n",
        "        print(f\"{cls_name}: {count} 次\")\n",
        "else:\n",
        "    print(\"沒有偵測到任何物件。\")\n",
        "print(\"==============================\")\n",
        "\n",
        "print(\"腳本執行完畢。\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmZ1ucj5AbVq",
        "outputId": "24023f07-bae5-4b09-a28d-37fd4438fdd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "讀取影片檔案: videoplayback.mp4\n",
            "影片資訊: 1280x720 @ 29.97 FPS\n",
            "開始處理影像...\n",
            "執行第 1 次推論 (Frame: 15)\n",
            "\n",
            "0: 384x640 (no detections), 111.4ms\n",
            "Speed: 18.6ms preprocess, 111.4ms inference, 117.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 2 次推論 (Frame: 30)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 4.2ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 3 次推論 (Frame: 45)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.8ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 4 次推論 (Frame: 60)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 5 次推論 (Frame: 75)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.9ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 6 次推論 (Frame: 90)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.6ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 7 次推論 (Frame: 105)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 8 次推論 (Frame: 120)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 9 次推論 (Frame: 135)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_149.jpg\n",
            "執行第 10 次推論 (Frame: 150)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 11 次推論 (Frame: 165)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.4ms preprocess, 25.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 12 次推論 (Frame: 180)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 13 次推論 (Frame: 195)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 14 次推論 (Frame: 210)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.7ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 15 次推論 (Frame: 225)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.0ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 16 次推論 (Frame: 240)\n",
            "\n",
            "0: 384x640 1 package, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 227.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 17 次推論 (Frame: 255)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 18 次推論 (Frame: 270)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 19 次推論 (Frame: 285)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_298.jpg\n",
            "執行第 20 次推論 (Frame: 300)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 21 次推論 (Frame: 315)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 22 次推論 (Frame: 330)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.4ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 23 次推論 (Frame: 345)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.9ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 24 次推論 (Frame: 360)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.8ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 25 次推論 (Frame: 375)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 26 次推論 (Frame: 390)\n",
            "\n",
            "0: 384x640 1 other_person, 25.1ms\n",
            "Speed: 3.7ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 27 次推論 (Frame: 405)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 28 次推論 (Frame: 420)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.6ms preprocess, 25.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 29 次推論 (Frame: 435)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.8ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_447.jpg\n",
            "執行第 30 次推論 (Frame: 450)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 31 次推論 (Frame: 465)\n",
            "\n",
            "0: 384x640 1 other_person, 25.1ms\n",
            "Speed: 3.6ms preprocess, 25.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 32 次推論 (Frame: 480)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 33 次推論 (Frame: 495)\n",
            "\n",
            "0: 384x640 1 other_person, 25.8ms\n",
            "Speed: 4.7ms preprocess, 25.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 34 次推論 (Frame: 510)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 35 次推論 (Frame: 525)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 36 次推論 (Frame: 540)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 4.6ms preprocess, 25.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 37 次推論 (Frame: 555)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 38 次推論 (Frame: 570)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.8ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 39 次推論 (Frame: 585)\n",
            "\n",
            "0: 384x640 1 other_person, 25.6ms\n",
            "Speed: 5.6ms preprocess, 25.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_596.jpg\n",
            "執行第 40 次推論 (Frame: 600)\n",
            "\n",
            "0: 384x640 (no detections), 37.5ms\n",
            "Speed: 3.6ms preprocess, 37.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 41 次推論 (Frame: 615)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 42 次推論 (Frame: 630)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 43 次推論 (Frame: 645)\n",
            "\n",
            "0: 384x640 1 other_person, 32.8ms\n",
            "Speed: 4.7ms preprocess, 32.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 44 次推論 (Frame: 660)\n",
            "\n",
            "0: 384x640 1 other_person, 29.3ms\n",
            "Speed: 6.2ms preprocess, 29.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 45 次推論 (Frame: 675)\n",
            "\n",
            "0: 384x640 1 other_person, 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 46 次推論 (Frame: 690)\n",
            "\n",
            "0: 384x640 (no detections), 26.3ms\n",
            "Speed: 5.1ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 47 次推論 (Frame: 705)\n",
            "\n",
            "0: 384x640 (no detections), 43.0ms\n",
            "Speed: 3.9ms preprocess, 43.0ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 48 次推論 (Frame: 720)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.7ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 49 次推論 (Frame: 735)\n",
            "\n",
            "0: 384x640 (no detections), 27.3ms\n",
            "Speed: 5.4ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_745.jpg\n",
            "執行第 50 次推論 (Frame: 750)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.0ms preprocess, 25.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 51 次推論 (Frame: 765)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 52 次推論 (Frame: 780)\n",
            "\n",
            "0: 384x640 (no detections), 33.9ms\n",
            "Speed: 3.9ms preprocess, 33.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 53 次推論 (Frame: 795)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 5.0ms preprocess, 25.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 54 次推論 (Frame: 810)\n",
            "\n",
            "0: 384x640 (no detections), 25.6ms\n",
            "Speed: 3.9ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 55 次推論 (Frame: 825)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 56 次推論 (Frame: 840)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 57 次推論 (Frame: 855)\n",
            "\n",
            "0: 384x640 (no detections), 26.2ms\n",
            "Speed: 6.5ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 58 次推論 (Frame: 870)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 59 次推論 (Frame: 885)\n",
            "\n",
            "0: 384x640 (no detections), 29.0ms\n",
            "Speed: 12.8ms preprocess, 29.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_894.jpg\n",
            "執行第 60 次推論 (Frame: 900)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 61 次推論 (Frame: 915)\n",
            "\n",
            "0: 384x640 (no detections), 33.2ms\n",
            "Speed: 4.6ms preprocess, 33.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 62 次推論 (Frame: 930)\n",
            "\n",
            "0: 384x640 (no detections), 43.4ms\n",
            "Speed: 3.7ms preprocess, 43.4ms inference, 10.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 63 次推論 (Frame: 945)\n",
            "\n",
            "0: 384x640 (no detections), 35.8ms\n",
            "Speed: 3.5ms preprocess, 35.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 64 次推論 (Frame: 960)\n",
            "\n",
            "0: 384x640 (no detections), 50.4ms\n",
            "Speed: 5.6ms preprocess, 50.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 65 次推論 (Frame: 975)\n",
            "\n",
            "0: 384x640 (no detections), 32.8ms\n",
            "Speed: 3.3ms preprocess, 32.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 66 次推論 (Frame: 990)\n",
            "\n",
            "0: 384x640 (no detections), 36.8ms\n",
            "Speed: 3.4ms preprocess, 36.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 67 次推論 (Frame: 1005)\n",
            "\n",
            "0: 384x640 (no detections), 39.4ms\n",
            "Speed: 7.9ms preprocess, 39.4ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 68 次推論 (Frame: 1020)\n",
            "\n",
            "0: 384x640 1 package, 47.6ms\n",
            "Speed: 7.7ms preprocess, 47.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 69 次推論 (Frame: 1035)\n",
            "\n",
            "0: 384x640 (no detections), 42.5ms\n",
            "Speed: 3.4ms preprocess, 42.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_1043.jpg\n",
            "執行第 70 次推論 (Frame: 1050)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 5.2ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 71 次推論 (Frame: 1065)\n",
            "\n",
            "0: 384x640 4 packages, 29.8ms\n",
            "Speed: 5.8ms preprocess, 29.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 72 次推論 (Frame: 1080)\n",
            "\n",
            "0: 384x640 1 package, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 73 次推論 (Frame: 1095)\n",
            "\n",
            "0: 384x640 (no detections), 25.7ms\n",
            "Speed: 12.3ms preprocess, 25.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 74 次推論 (Frame: 1110)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 75 次推論 (Frame: 1125)\n",
            "\n",
            "0: 384x640 (no detections), 32.0ms\n",
            "Speed: 11.9ms preprocess, 32.0ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 76 次推論 (Frame: 1140)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 77 次推論 (Frame: 1155)\n",
            "\n",
            "0: 384x640 1 package, 25.7ms\n",
            "Speed: 4.0ms preprocess, 25.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 78 次推論 (Frame: 1170)\n",
            "\n",
            "0: 384x640 1 package, 33.1ms\n",
            "Speed: 8.4ms preprocess, 33.1ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 79 次推論 (Frame: 1185)\n",
            "\n",
            "0: 384x640 1 package, 25.1ms\n",
            "Speed: 3.7ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_1192.jpg\n",
            "執行第 80 次推論 (Frame: 1200)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 81 次推論 (Frame: 1215)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.4ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 82 次推論 (Frame: 1230)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.6ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 83 次推論 (Frame: 1245)\n",
            "\n",
            "0: 384x640 1 bag, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 84 次推論 (Frame: 1260)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.2ms preprocess, 25.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 85 次推論 (Frame: 1275)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 86 次推論 (Frame: 1290)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 2.9ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 87 次推論 (Frame: 1305)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 2.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 88 次推論 (Frame: 1320)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.4ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 89 次推論 (Frame: 1335)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "儲存快照: /content/snapshots/snapshot_frame_1341.jpg\n",
            "執行第 90 次推論 (Frame: 1350)\n",
            "\n",
            "0: 384x640 (no detections), 25.3ms\n",
            "Speed: 3.7ms preprocess, 25.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 91 次推論 (Frame: 1365)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 92 次推論 (Frame: 1380)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.8ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 93 次推論 (Frame: 1395)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 94 次推論 (Frame: 1410)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 95 次推論 (Frame: 1425)\n",
            "\n",
            "0: 384x640 (no detections), 25.1ms\n",
            "Speed: 3.7ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 96 次推論 (Frame: 1440)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.2ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "執行第 97 次推論 (Frame: 1455)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 4.3ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "無法讀取影像，可能是影片結束或連線中斷。\n",
            "影像來源已關閉。總共處理了 1469 張 frame。\n",
            "開始合成影片到 /content/inference_output_with_boxes.mp4...\n",
            "已寫入 100/1469 幀到影片...\n",
            "已寫入 200/1469 幀到影片...\n",
            "已寫入 300/1469 幀到影片...\n",
            "已寫入 400/1469 幀到影片...\n",
            "已寫入 500/1469 幀到影片...\n",
            "已寫入 600/1469 幀到影片...\n",
            "已寫入 700/1469 幀到影片...\n",
            "已寫入 800/1469 幀到影片...\n",
            "已寫入 900/1469 幀到影片...\n",
            "已寫入 1000/1469 幀到影片...\n",
            "已寫入 1100/1469 幀到影片...\n",
            "已寫入 1200/1469 幀到影片...\n",
            "已寫入 1300/1469 幀到影片...\n",
            "已寫入 1400/1469 幀到影片...\n",
            "影片已成功儲存到 /content/inference_output_with_boxes.mp4\n",
            "\n",
            "===== 最後累積推論結果 =====\n",
            "other_person: 12 次\n",
            "package: 10 次\n",
            "bag: 1 次\n",
            "==============================\n",
            "腳本執行完畢。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/multi_class_detection_merge_run4.zip /content/multi_class_detection/merge_run4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SJDJBOqVkTDh",
        "outputId": "802c5399-c82e-4445-8b8e-b9e2174b350f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/multi_class_detection/merge_run4/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/R_curve.png (deflated 10%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch0_pred.jpg (deflated 6%)\n",
            "  adding: content/multi_class_detection/merge_run4/labels_correlogram.jpg (deflated 33%)\n",
            "  adding: content/multi_class_detection/merge_run4/results.png (deflated 7%)\n",
            "  adding: content/multi_class_detection/merge_run4/confusion_matrix_normalized.png (deflated 23%)\n",
            "  adding: content/multi_class_detection/merge_run4/confusion_matrix.png (deflated 28%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch2062.jpg (deflated 10%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch2_pred.jpg (deflated 11%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.pt (deflated 8%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.onnx (deflated 17%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/last.pt (deflated 8%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Data/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Data/com.apple.CoreML/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Data/com.apple.CoreML/weights/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Data/com.apple.CoreML/weights/weight.bin (deflated 8%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Data/com.apple.CoreML/model.mlmodel (deflated 89%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.mlpackage/Manifest.json (deflated 59%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best.engine (deflated 10%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/variables/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/variables/variables.data-00000-of-00001 (deflated 87%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/variables/variables.index (deflated 33%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/best_float32.tflite (deflated 17%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/saved_model.pb (deflated 8%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/best_float16.tflite (deflated 8%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/metadata.yaml (deflated 37%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/fingerprint.pb (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/weights/best_saved_model/assets/ (stored 0%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch2061.jpg (deflated 7%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch0_labels.jpg (deflated 6%)\n",
            "  adding: content/multi_class_detection/merge_run4/P_curve.png (deflated 10%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch1_labels.jpg (deflated 6%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch2060.jpg (deflated 7%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch1_pred.jpg (deflated 6%)\n",
            "  adding: content/multi_class_detection/merge_run4/F1_curve.png (deflated 7%)\n",
            "  adding: content/multi_class_detection/merge_run4/args.yaml (deflated 53%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch2.jpg (deflated 3%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch0.jpg (deflated 2%)\n",
            "  adding: content/multi_class_detection/merge_run4/results.csv (deflated 60%)\n",
            "  adding: content/multi_class_detection/merge_run4/train_batch1.jpg (deflated 5%)\n",
            "  adding: content/multi_class_detection/merge_run4/labels.jpg (deflated 20%)\n",
            "  adding: content/multi_class_detection/merge_run4/val_batch2_labels.jpg (deflated 12%)\n",
            "  adding: content/multi_class_detection/merge_run4/PR_curve.png (deflated 16%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/multi_class_detection_merge_run4.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Uhda2yUDkhP1",
        "outputId": "ae40a323-1399-4fd4-fff3-fd7e36280e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0696cb4b-cf60-4e52-bcbf-1aa863f5ed7b\", \"multi_class_detection_merge_run4.zip\", 586895320)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k3Hw0mIhGuO"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOxEpf2wD/0nX8W5UmP2/c2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}