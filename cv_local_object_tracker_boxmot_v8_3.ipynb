{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e91c3f80",
      "metadata": {},
      "source": [
        "# Object Tracking & Intent Analysis (v7 - Interaction Distance, Full Annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1f3320bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BoxMOT and create_tracker imported successfully.\n"
          ]
        }
      ],
      "source": [
        "# Install Ultralytics (YOLOv8) and BoxMOT\n",
        "!pip install ultralytics --quiet\n",
        "!pip install boxmot --quiet\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import yaml\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, deque\n",
        "from ultralytics import YOLO\n",
        "import datetime\n",
        "# from google.colab.patches import cv2_imshow # Usually not needed if running locally or if cv2.imshow works\n",
        "\n",
        "# Define DummyTracker globally for fallback\n",
        "\n",
        "class DummyTracker:\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        print(\"Initialized DummyTracker for BoxMOT fallback.\")\n",
        "        self.frame_id = 0\n",
        "    def update(self, dets, img): # img argument is often expected by trackers\n",
        "        self.frame_id += 1\n",
        "        if dets is None or len(dets) == 0:\n",
        "            return np.empty((0, 7)) # BoxMOT ByteTrack returns 7 columns: x1,y1,x2,y2,id,cls,conf\n",
        "        \n",
        "        if hasattr(dets, 'cpu') and not isinstance(dets, np.ndarray):\n",
        "            dets_np = dets.cpu().numpy()\n",
        "        else:\n",
        "            dets_np = dets\n",
        "            \n",
        "        fake_tracks = []\n",
        "        for i, det_row in enumerate(dets_np):\n",
        "            x1, y1, x2, y2 = det_row[:4]\n",
        "            conf = det_row[4] if len(det_row) > 4 else 0.5 \n",
        "            cls = det_row[5] if len(det_row) > 5 else (i % 5) # Placeholder class\n",
        "            fake_tracks.append([x1, y1, x2, y2, self.frame_id * 1000 + i, cls, conf])\n",
        "        return np.array(fake_tracks)\n",
        "\n",
        "\n",
        "# Attempt to import BoxMOT and its utilities\n",
        "try:\n",
        "    from boxmot import create_tracker\n",
        "    from boxmot.utils import TRACKER_CONFIGS\n",
        "    print(\"BoxMOT and create_tracker imported successfully.\")\n",
        "    if TRACKER_CONFIGS is None:\n",
        "        print(\"WARNING: boxmot.utils.TRACKER_CONFIGS is None. main() will attempt to find config path manually.\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR: Failed to import BoxMOT: {e}\")\n",
        "    print(\"Please ensure BoxMOT is installed: pip install boxmot\")\n",
        "    print(\"WARNING: BoxMOT not available. Real tracking will not work. Falling back to DummyTracker.\")\n",
        "    create_tracker = lambda *args, **kwargs: DummyTracker(*args, **kwargs)\n",
        "    TRACKER_CONFIGS = Path(\"dummy_boxmot_configs\") # Relative path for dummy config\n",
        "    if not TRACKER_CONFIGS.exists():\n",
        "        TRACKER_CONFIGS.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b48f2e6",
      "metadata": {},
      "source": [
        "## Configuration Parameters & Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "70aa2721",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration & Global Variables ---\n",
        "# ROI_MODE: 0 = Manual ROI, 1 = Dynamic ROI from frame margin\n",
        "ROI_MODE = 0 \n",
        "ROI_MARGIN_PIXELS = 10 # Margin in pixels for dynamic ROI (if ROI_MODE = 1)\n",
        "MANUAL_ROI = (0, 250, 650, 720) # Manual ROI: (x1, y1, x2, y2) or None (if ROI_MODE = 0), too big ROI will cause ROI exit harder \n",
        "\n",
        "# Behavior analysis parameters\n",
        "LOITERING_THRESHOLD_SEC = 5 # Seconds of inactivity to trigger loitering\n",
        "INTERACTION_PROXIMITY_THRESHOLD = 210 # Distance in pixels for interaction proximity\n",
        "TRACK_HISTORY_LENGTH = 60 # Frames of history for behavior analysis\n",
        "EVENT_LOG_FILE = \"event_log_boxmot_v8.json\" # Relative path\n",
        "\n",
        "# Event Media Saving Parameters\n",
        "ENABLE_EVENT_CLIPS = False\n",
        "ENABLE_EVENT_SNAPSHOTS = True\n",
        "# EVENT_CLIP_OUTPUT_DIR = \"event_clips\" # Relative path, will be created if not exists\n",
        "# EVENT_SNAPSHOT_OUTPUT_DIR = \"event_snapshots\" # Relative path, will be created if not exists\n",
        "LOITERING_EVENT_CLIP_PRE_BUFFER_SEC = 2\n",
        "LOITERING_EVENT_CLIP_POST_BUFFER_SEC = 1 # Per user request, loitering clip ends at event time\n",
        "INSTANT_EVENT_CLIP_TOTAL_DURATION_SEC = 10 # Centered around event time (5s before, 5s after)\n",
        "FRAME_BUFFER_DURATION_SEC = 10 # Max duration of ANNOTATED frames to keep in memory\n",
        "\n",
        "# Global data structures\n",
        "ROI = None\n",
        "track_history = defaultdict(lambda: deque(maxlen=TRACK_HISTORY_LENGTH))\n",
        "object_loitering_start_time = defaultdict(lambda: None)\n",
        "# 新增：追蹤物件是否在ROI內的狀態字典\n",
        "object_in_roi_status = defaultdict(lambda: False)\n",
        "# 新增：追蹤物件之間的互動狀態字典\n",
        "object_interaction_status = defaultdict(lambda: {})\n",
        "event_log = [] # Will be populated by log_event, and saved at the end\n",
        "\n",
        "# Frame buffer for video clip saving (stores (ANNOTATED_frame_copy, timestamp))\n",
        "frame_buffer = deque()\n",
        "active_clip_capture_tasks = []\n",
        "\n",
        "# Statistics counters\n",
        "total_frames_read_count = 0\n",
        "total_frames_processed_count = 0\n",
        "cumulative_detected_class_counts = defaultdict(int)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 全域狀態（請放到檔案最上方一次宣告）\n",
        "pair_state = {}                    # {(id_small,id_big): 'near'/'far'}\n",
        "INTER_THRESH_RATIO = 0.15          # 中心距離 < 影像對角線 * 15% 視為 near\n",
        "\n",
        "# 允許計算互動的「類別對」；None = 所有組合都計算\n",
        "ALLOWED_INTERACTIONS = {\n",
        "    (\"other_person\", \"package\"),\n",
        "    (\"other_person\", \"bag\"),\n",
        "    (\"delivery_worker\", \"package\"),\n",
        "    (\"delivery_worker\", \"bag\"),\n",
        "    (\"food_delivery\", \"package\"),\n",
        "    (\"food_delivery\", \"bag\"),\n",
        "}\n",
        "# ------------------------------------------------------------------\n",
        "# －－ constants －－\n",
        "ARRIVAL_AWAY_DIST_RATIO = 0.10          # 兩物件同時運動時的中心距離threshold，影像對角線 * 10 %\n",
        "ROI_EVENT_WINDOW_FRAMES  = 30            # 幾幀內算「同時」\n",
        "ARRIVAL_DIST_PX   = 150   # <= 可自行調整\n",
        "AWAY_DIST_PX      = 150\n",
        "FRAME_WINDOW_SECOND      = 5    # 多少秒以內視為「同時」\n",
        "# -------------------\n",
        "\n",
        "# 存最近幾幀發生的 ROI 進出事件\n",
        "recent_roi_enters = deque(maxlen=30)    # 每項: {'id':..,'cls':..,'cent':(x,y),'frame_idx':..}\n",
        "recent_roi_exits  = deque(maxlen=30)\n",
        "approach_depature_thresh_px = 210 # 判斷的距離閾值\n",
        "arrival_away_thresh_px = ARRIVAL_DIST_PX # 事件判斷的距離閾值\n",
        "\n",
        "# ── 觸發後不再重複的配對集合\n",
        "something_arrival_pairs: set[tuple[int, int]] = set()\n",
        "\n",
        "# ── 畫面文字維持時間：pair_key ➜ expire_ts\n",
        "arrival_overlay: dict[tuple[int, int], datetime.datetime] = {}\n",
        "\n",
        "# ── 顯示多久 (秒) FRAME_WINDOW_SECOND 請與既有程式保持一致\n",
        "ARRIVAL_DISPLAY_SEC = 3\n",
        "\n",
        "\n",
        "# 取代原先 pair_key→{'pt', 'expire'} 的做法\n",
        "away_overlay: dict[int, datetime.datetime] = {}   # keep until expire\n",
        "AWAY_DISPLAY_SEC = 3                              # 顯示秒數\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b6c3cf21",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▍Path initialised\n",
            " RUN_DIR                  : output\\golden_sample\\20250601_134922\n",
            " EVENT_CLIP_OUTPUT_DIR    : output\\golden_sample\\20250601_134922\\clips\n",
            " EVENT_SNAPSHOT_OUTPUT_DIR: output\\golden_sample\\20250601_134922\\snapshots\n",
            " EVENT_LOG_FILE           : output\\golden_sample\\20250601_134922\\golden_sample_events.json\n"
          ]
        }
      ],
      "source": [
        "# ===== Path Helper (放在 Notebook 最前面即可) =====================\n",
        "# from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# ▶ 修改這兩行就能切換資料來源與輸出根目錄\n",
        "INPUT_SOURCE = \"golden_sample.mp4\"      # 或 RTSP/HTTP URL\n",
        "OUTPUT_ROOT  = Path(\"output\")               # 建議集中管理\n",
        "\n",
        "def init_paths(input_path: str | Path, add_timestamp: bool = True):\n",
        "    \"\"\"依輸入檔名自動建立版本化輸出目錄與全域變數。\"\"\"\n",
        "    global RUN_NAME, RUN_DIR, EVENT_LOG_FILE\n",
        "    global EVENT_CLIP_OUTPUT_DIR, EVENT_SNAPSHOT_OUTPUT_DIR\n",
        "\n",
        "    input_path = Path(str(input_path))\n",
        "    RUN_NAME   = input_path.stem\n",
        "    ts_layer   = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if add_timestamp else \"\"\n",
        "    RUN_DIR    = OUTPUT_ROOT / RUN_NAME / ts_layer\n",
        "\n",
        "    EVENT_CLIP_OUTPUT_DIR     = RUN_DIR / \"clips\"\n",
        "    EVENT_SNAPSHOT_OUTPUT_DIR = RUN_DIR / \"snapshots\"\n",
        "    EVENT_LOG_FILE            = RUN_DIR / f\"{RUN_NAME}_events.json\"\n",
        "\n",
        "    # 建立必要目錄\n",
        "    EVENT_CLIP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    EVENT_SNAPSHOT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"▍Path initialised\")\n",
        "    print(\" RUN_DIR                  :\", RUN_DIR)\n",
        "    print(\" EVENT_CLIP_OUTPUT_DIR    :\", EVENT_CLIP_OUTPUT_DIR)\n",
        "    print(\" EVENT_SNAPSHOT_OUTPUT_DIR:\", EVENT_SNAPSHOT_OUTPUT_DIR)\n",
        "    print(\" EVENT_LOG_FILE           :\", EVENT_LOG_FILE)\n",
        "\n",
        "# ★ 呼叫一次，之後整支 Notebook 都能用全域變數\n",
        "init_paths(INPUT_SOURCE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "80b2cdaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "LINE_CHANNEL_ACCESS_TOKEN='UZE4h+sLvPk/7ueINMMK+I/AoqyUIj9apJhk+clU0RHL2MzUu2YB9Whqt/zIvREQS8dJJxM0BEk6/T/zC6DQwfBT5xz+I2v6pmMm2996+d3r9uOj9T+4rw5RluMoSB9NqantoYxGXjfWY+oRexwTSQdB04t89/1O/w1cDnyilFU='\n",
        "LINE_TARGET_ID='U0b438da7f84a28c344738f3e3a1c3238'\n",
        "GCS_BUCKET_NAME='cv_event_image'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "82437e82",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2, os, tempfile, requests, pyimgur\n",
        "from dotenv import load_dotenv\n",
        "# from linebot import LineBotApi\n",
        "# import linebot.v3.messaging\n",
        "from linebot.v3.messaging import MessagingApi, Configuration, ApiClient\n",
        "from linebot.v3.messaging.models import ImageMessage, TextMessage, PushMessageRequest\n",
        "\n",
        "# access google storage \n",
        "from google.cloud import storage\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "storage_cred = service_account.Credentials.from_service_account_file(\n",
        "    'storage-compute-key.json')\n",
        "storage_client = storage.Client(credentials=storage_cred)\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "LINE_TOKEN   = LINE_CHANNEL_ACCESS_TOKEN\n",
        "LINE_TARGET  = LINE_TARGET_ID\n",
        "\n",
        "linebot_configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN)\n",
        "\n",
        "# line_api = MessagingApi(LINE_TOKEN) if LINE_TOKEN else None\n",
        "\n",
        "def upload_image_to_gcs(local_image_path, gcs_image_name):\n",
        "    # storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "    blob = bucket.blob(gcs_image_name)\n",
        "\n",
        "    # blob.upload_from_filename(local_image_path)\n",
        "    # blob.make_public()  # 設為公開\n",
        "    # 嘗試刪除舊文件（如果存在）\n",
        "    try:\n",
        "        blob.delete()\n",
        "    except NotFound:\n",
        "        pass  # 如果文件不存在，忽略錯誤\n",
        "    blob.upload_from_filename(local_image_path)\n",
        "    # 生成公開訪問 URL（設置為24小時後過期）\n",
        "    url = blob.generate_signed_url(\n",
        "        version=\"v4\",\n",
        "        expiration=datetime.timedelta(hours=24),\n",
        "        method=\"GET\"\n",
        "    )\n",
        "    return url\n",
        "\n",
        "def push_image_to_line(user_id, image_url,text=None):\n",
        "    with ApiClient(linebot_configuration) as api_client:\n",
        "        line_bot = MessagingApi(api_client)\n",
        "        \n",
        "        messages = []\n",
        "        if text:\n",
        "            messages.append(TextMessage(text=text))\n",
        "        messages.append(ImageMessage(\n",
        "            original_content_url=image_url,\n",
        "            preview_image_url=image_url\n",
        "        ))\n",
        "\n",
        "        push_message = PushMessageRequest(\n",
        "            to=user_id,\n",
        "            messages=messages\n",
        "        )\n",
        "        line_bot.push_message(push_message)\n",
        "\n",
        "def push_line_image(msg: str,event_name: str, frame_bgr):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    gcs_image_name = f\"event_images/{event_name}_{timestamp}.jpg\"\n",
        "\n",
        "    # 保存暫存圖\n",
        "    fd, tmpjpg = tempfile.mkstemp(suffix=\".jpg\")\n",
        "    os.close(fd)\n",
        "    cv2.imwrite(tmpjpg, frame_bgr)\n",
        "    \n",
        "    url=upload_image_to_gcs(tmpjpg, gcs_image_name)\n",
        "    push_image_to_line(LINE_TARGET, url, text=msg)\n",
        "    print(f\"[LINE-Bot] 圖片已推送到 LINE: {url}\")\n",
        "    os.remove(tmpjpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6a91e67c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------------------------\n",
        "# utils/annot_line.py  （或任一共用模組）\n",
        "# -----------------------------------------------\n",
        "import cv2\n",
        "# from line_push import push_line_image   # ← 確保已採用新 LINE Messaging API 方案\n",
        "\n",
        "EVENT_COLORS = {\n",
        "    \"arrival\": (255,   0, 255),   # 粉紫\n",
        "    \"away\"   : (255, 255,   0)    # 淡黃\n",
        "}\n",
        "\n",
        "def check_and_emit(timestamp,\n",
        "                   event_name,\n",
        "                   obj1_id,\n",
        "                   obj2_id,\n",
        "                   frame_w,\n",
        "                   frame_h,\n",
        "                   annotated_frame,\n",
        "                   line_prefix: str = \"⚠️\"):\n",
        "    \"\"\"\n",
        "    只做兩件事：\n",
        "    1) 在 annotated_frame 畫 event_name 文字\n",
        "    2) 將影像與訊息傳到 LINE\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    event_name : \"something_arrival\" | \"something_away\"\n",
        "    obj1_cent, obj2_cent : (x, y)  兩物件 centroid\n",
        "    annotated_frame : numpy.ndarray (BGR)\n",
        "    line_prefix : str  (預設 \"⚠️\")\n",
        "    \"\"\"\n",
        "    if event_name not in EVENT_COLORS:\n",
        "        raise ValueError(f\"Unknown event_name: {event_name}\")\n",
        "\n",
        "    # 取得物件 class name（如有全域 class_names_dict 可用，否則略過）\n",
        "    obj1_cls = None\n",
        "    obj2_cls = None\n",
        "    try:\n",
        "        # 嘗試從 recent_roi_enters 找 class name\n",
        "        global recent_roi_enters\n",
        "        for obj in recent_roi_enters:\n",
        "            if obj.get('id') == obj1_id:\n",
        "                obj1_cls = obj.get('cls')\n",
        "            if obj.get('id') == obj2_id:\n",
        "                obj2_cls = obj.get('cls')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 組合訊息\n",
        "    msg = f\"{line_prefix} {event_name}\\n\"\n",
        "    msg += f\"time: {timestamp}\\n\"\n",
        "    msg += f\"obj1: {obj1_id}\\n\"\n",
        "    msg += f\"obj2: {obj2_id}\"\n",
        "    # msg += f\"obj1: {obj1_id} ({obj1_cls})\\n\"\n",
        "    # msg += f\"obj2: {obj2_id} ({obj2_cls})\"\n",
        "\n",
        "    # --- LINE 推送 ---\n",
        "    push_line_image(msg, event_name, annotated_frame)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55b6f5b8",
      "metadata": {},
      "source": [
        "## Helper Functions (Analysis, Drawing, Logging, Media Saving)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "725b3c28",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_centroid(bbox):\n",
        "    x1, y1, x2, y2 = bbox[:4]\n",
        "    return int((x1 + x2) / 2), int((y1 + y2) / 2)\n",
        "\n",
        "def is_within_roi(centroid, current_roi):\n",
        "    if current_roi is None: return False\n",
        "    cx, cy = centroid; rx1, ry1, rx2, ry2 = current_roi\n",
        "    return rx1 <= cx <= rx2 and ry1 <= cy <= ry2\n",
        "\n",
        "def calculate_distance(p1, p2): return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
        "\n",
        "def save_event_snapshot(annotated_frame, event_type, track_id, event_timestamp):\n",
        "    if annotated_frame is None:\n",
        "        print(\"No frame provided for snapshot.\")\n",
        "        return\n",
        "    try:\n",
        "        snap_dir = Path(EVENT_SNAPSHOT_OUTPUT_DIR)\n",
        "        snap_dir.mkdir(parents=True, exist_ok=True)\n",
        "        ts_str = event_timestamp.strftime(\"%Y%m%d_%H%M%S_%f\")[:-3]\n",
        "        filename_parts = [event_type]\n",
        "        if track_id is not None: filename_parts.append(f\"id{track_id}\")\n",
        "        filename_parts.append(ts_str)\n",
        "        filename = \"_\".join(map(str, filename_parts)) + \".jpg\"\n",
        "        filepath = snap_dir / filename\n",
        "        cv2.imwrite(str(filepath), annotated_frame)\n",
        "        print(f\"Event snapshot saved: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR saving event snapshot: {e}\")\n",
        "\n",
        "def save_video_clip(frames_to_save, output_path_str, fps, frame_width, frame_height):\n",
        "    if not frames_to_save:\n",
        "        print(f\"No frames to save for {output_path_str}.\")\n",
        "        return\n",
        "    output_path = Path(output_path_str)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))\n",
        "    for frame in frames_to_save:\n",
        "        writer.write(frame)\n",
        "    writer.release()\n",
        "    print(f\"Event clip saved: {output_path}\")\n",
        "\n",
        "def log_event(event_data, annotated_frame_for_media):\n",
        "    global event_log, active_clip_capture_tasks, frame_buffer\n",
        "    global recent_roi_enters, recent_roi_exits,total_frames_processed_count\n",
        "    \n",
        "    # Basic log entry structure from event_data\n",
        "    log_entry = {\n",
        "        \"timestamp\": event_data['event_timestamp'].strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3],\n",
        "        \"event_type\": event_data['event_type'],\n",
        "        \"track_id\": int(event_data['track_id']) if event_data.get('track_id') is not None else None,\n",
        "        \"class_name\": event_data.get('class_name'),\n",
        "        \"details\": event_data.get('details') or {}\n",
        "    }\n",
        "    if log_entry[\"class_name\"] == \"unknown\": \n",
        "        print(f\"WARNING: Detected class names 'Unknown'.\")\n",
        "        print(f\"log_entry = {log_entry}\")\n",
        "    event_log.append(log_entry)\n",
        "       \n",
        "    if event_data['event_type'] in ('arrival', 'away'):\n",
        "        details = event_data.get('details', {})\n",
        "        # For something_arrival, use human_id/thing_id; for something_away, use id1/id2\n",
        "        id1 = details.get('id1') or details.get('human_id')\n",
        "        id2 = details.get('id2') or details.get('thing_id')\n",
        "        if id1 is not None and id2 is not None:\n",
        "            # 在 snapshot 前補畫 AWAY/ARRIVAL\n",
        "            # 預設為畫面中心點\n",
        "            frame_h, frame_w = annotated_frame_for_media.shape[:2]\n",
        "            center_pt = (frame_w // 2, frame_h // 2)\n",
        "            cent1 = center_pt\n",
        "            cent2 = center_pt\n",
        "            for obj in recent_roi_enters:\n",
        "                if obj.get('id') == id1:\n",
        "                    cent1 = obj.get('cent')\n",
        "                if obj.get('id') == id2:\n",
        "                    cent2 = obj.get('cent')\n",
        "            if cent1 and cent2:\n",
        "                mid_pt = (int((cent1[0] + cent2[0]) / 2), int((cent1[1] + cent2[1]) / 2))\n",
        "                label = \"ARRIVAL\" if event_data['event_type'] == \"arrival\" else \"AWAY\"\n",
        "                color = (0, 0, 255) if label == \"ARRIVAL\" else (0, 0, 255)\n",
        "                cv2.putText(annotated_frame_for_media, label, (mid_pt[0] - 35, mid_pt[1] - 28),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "            # Emit the event to LINE or other media\n",
        "            check_and_emit(\n",
        "                log_entry[\"timestamp\"],\n",
        "                event_data['event_type'],\n",
        "                id1,\n",
        "                id2,\n",
        "                event_data['frame_w'],\n",
        "                event_data['frame_h'],\n",
        "                annotated_frame_for_media,\n",
        "            )\n",
        "    # Save snapshot if enabled, using the provided fully annotated frame # type: ignore\n",
        "    if ENABLE_EVENT_SNAPSHOTS and annotated_frame_for_media is not None:\n",
        "        save_event_snapshot(annotated_frame_for_media, event_data['event_type'], event_data.get('track_id'), event_data['event_timestamp'])\n",
        "\n",
        "    # Create video clip task if enabled\n",
        "    if ENABLE_EVENT_CLIPS:\n",
        "        desired_clip_start_ts, desired_clip_end_ts = None, None\n",
        "        event_timestamp = event_data['event_timestamp']\n",
        "        event_type = event_data['event_type']\n",
        "        track_id = event_data.get('track_id')\n",
        "        details = event_data.get('details', {})\n",
        "        current_fps = event_data['current_fps']\n",
        "        frame_w = event_data['frame_w']\n",
        "        frame_h = event_data['frame_h']\n",
        "\n",
        "        ts_str = event_timestamp.strftime(\"%Y%m%d_%H%M%S_%f\")[:-3]\n",
        "        clip_name_parts = [event_type]\n",
        "\n",
        "        if event_type == \"loitering\":\n",
        "            loiter_start_time = object_loitering_start_time.get(track_id) # Assumes object_loitering_start_time is globally updated\n",
        "            if loiter_start_time:\n",
        "                desired_clip_start_ts = loiter_start_time - datetime.timedelta(seconds=LOITERING_EVENT_CLIP_PRE_BUFFER_SEC)\n",
        "                desired_clip_end_ts = event_timestamp + datetime.timedelta(seconds=LOITERING_EVENT_CLIP_POST_BUFFER_SEC) # POST_BUFFER_SEC is 0\n",
        "                if track_id is not None: clip_name_parts.append(f\"id{track_id}\")\n",
        "        elif event_type in [\"roi_enter\", \"roi_exit\", \"interaction\"]:\n",
        "            half_duration = datetime.timedelta(seconds=INSTANT_EVENT_CLIP_TOTAL_DURATION_SEC / 2)\n",
        "            desired_clip_start_ts = event_timestamp - half_duration\n",
        "            desired_clip_end_ts = event_timestamp + half_duration\n",
        "            if track_id is not None: clip_name_parts.append(f\"id{track_id}\")\n",
        "            if event_type == \"interaction\":\n",
        "                p_id = details.get(\"person_id\"); pkg_id = details.get(\"package_id\")\n",
        "                if p_id is not None: clip_name_parts.append(f\"p{p_id}\")\n",
        "                if pkg_id is not None: clip_name_parts.append(f\"pkg{pkg_id}\")\n",
        "        \n",
        "        if desired_clip_start_ts and desired_clip_end_ts:\n",
        "            clip_name_parts.append(ts_str)\n",
        "            filename = \"_\".join(map(str, clip_name_parts)) + \".mp4\"\n",
        "            output_filepath = Path(EVENT_CLIP_OUTPUT_DIR) / filename\n",
        "\n",
        "            task = {\n",
        "                'log_entry_ts': log_entry['timestamp'], # Use the string timestamp from log_entry\n",
        "                'desired_clip_start_ts': desired_clip_start_ts,\n",
        "                'desired_clip_end_ts': desired_clip_end_ts,\n",
        "                'collected_frames': [],\n",
        "                'output_filename': str(output_filepath),\n",
        "                'fps': current_fps, 'width': frame_w, 'height': frame_h,\n",
        "                'header_printed': False\n",
        "            }\n",
        "            # Pre-fill with ANNOTATED frames already in buffer\n",
        "            for f_in_buf, ts_in_buf in list(frame_buffer):\n",
        "                if ts_in_buf >= desired_clip_start_ts and ts_in_buf <= event_timestamp: # Collect up to current event time\n",
        "                    task['collected_frames'].append((f_in_buf, ts_in_buf))\n",
        "            active_clip_capture_tasks.append(task)\n",
        "\n",
        "\n",
        "def analyze_behavior(track_id,\n",
        "                     history,\n",
        "                     current_bbox,\n",
        "                     class_id,\n",
        "                     class_name,\n",
        "                     current_ts,\n",
        "                     fps_val,\n",
        "                     current_roi,\n",
        "                     f_w,\n",
        "                     f_h,\n",
        "                     annotated_frame=None):\n",
        "    \"\"\"\n",
        "    依單一物件行為判斷 ROI enter / exit / loitering，\n",
        "    以及離開 ROI 時觸發 something_away。  \n",
        "    觸發 away 後，AWAY 文字會隨 paired 物件位置持續顯示 AWAY_DISPLAY_SEC 秒。\n",
        "    -------------------------------------------------------------------------\n",
        "    `annotated_frame` 若為 None，函式仍能運作但不會畫標註（向下相容）。\n",
        "    \"\"\"\n",
        "    # ---------- 全域 ----------\n",
        "    global object_loitering_start_time, object_in_roi_status, recent_roi_enters\n",
        "    global total_frames_read_count, total_frames_processed_count, cumulative_detected_class_counts\n",
        "    global away_overlay                                                   # ★ 新增\n",
        "\n",
        "    events_to_log = []\n",
        "    if not history or current_roi is None:\n",
        "        return events_to_log\n",
        "\n",
        "    # ──────────────────────────────────────────────────────────────\n",
        "    # 0️⃣ 先修正 boxMOT 早期誤判的 class —— 只要 track_id 已存在，就用最新 class_name 覆蓋\n",
        "    # ──────────────────────────────────────────────────────────────\n",
        "    existing_item = next((it for it in recent_roi_enters if it['id'] == track_id), None)\n",
        "    if existing_item:\n",
        "        if existing_item['cls'] != class_name:\n",
        "            existing_item['cls'] = class_name      # ← 更新\n",
        "    already_in_recent = existing_item is not None\n",
        "    # ──────────────────────────────────────────────────────────────\n",
        "    # ── step-0：若此物件目前在 away_overlay，先畫 AWAY 且檢查是否過期 ──\n",
        "    if annotated_frame is not None and track_id in away_overlay:\n",
        "        expire_ts = away_overlay[track_id]\n",
        "        if current_ts > expire_ts:                         # 已過期 → 移除\n",
        "            del away_overlay[track_id]\n",
        "        else:\n",
        "            cx, cy = get_centroid(current_bbox)\n",
        "            cv2.putText(annotated_frame, \"AWAY\",\n",
        "                        (cx - 25, cy - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "    # ── 主要行為邏輯 ────────────────────────────────────────────────\n",
        "    centroid = get_centroid(current_bbox)\n",
        "    event_data_template = {\n",
        "        'track_id': track_id,\n",
        "        'class_name': class_name,\n",
        "        'current_fps': fps_val,\n",
        "        'frame_w': f_w,\n",
        "        'frame_h': f_h\n",
        "    }\n",
        "\n",
        "    # 目前是否在 ROI\n",
        "    is_in_roi_now = is_within_roi(centroid, current_roi)\n",
        "    was_in_roi = object_in_roi_status.get(track_id, False)\n",
        "\n",
        "    # ---------- ROI ENTER ----------\n",
        "    # already_in_recent = any(it['id'] == track_id for it in recent_roi_enters)\n",
        "    if is_in_roi_now and not was_in_roi:\n",
        "        object_loitering_start_time[track_id] = current_ts\n",
        "        object_in_roi_status[track_id] = True\n",
        "\n",
        "        if not already_in_recent:\n",
        "            events_to_log.append({\n",
        "                **event_data_template,\n",
        "                'event_timestamp': current_ts,\n",
        "                'event_type': \"roi_enter\",\n",
        "                'details': {\"roi\": current_roi, 'centroid': centroid}\n",
        "            })\n",
        "            # 新增至 recent_roi_enters\n",
        "            recent_roi_enters.append({\n",
        "                'enter_ts': current_ts,\n",
        "                'id': track_id,\n",
        "                'cls': class_name,\n",
        "                'cent': centroid,\n",
        "                'frame_idx': total_frames_processed_count,\n",
        "                'is_loitering': False,\n",
        "                'paired': None\n",
        "            })\n",
        "\n",
        "    # ---------- LOITERING ----------\n",
        "    elif is_in_roi_now and was_in_roi:\n",
        "        start_ts = object_loitering_start_time.get(track_id)\n",
        "        if start_ts:\n",
        "            dwell = (current_ts - start_ts).total_seconds()\n",
        "            if dwell >= LOITERING_THRESHOLD_SEC:\n",
        "                for it in recent_roi_enters:\n",
        "                    if it['id'] == track_id and not it['is_loitering']:\n",
        "                        it['is_loitering'] = True\n",
        "                        events_to_log.append({\n",
        "                            **event_data_template,\n",
        "                            'event_timestamp': current_ts,\n",
        "                            'event_type': \"loitering\",\n",
        "                            'details': {\n",
        "                                \"duration_sec\": round(dwell, 1),\n",
        "                                \"roi\": current_roi,\n",
        "                                'centroid': centroid\n",
        "                            }\n",
        "                        })\n",
        "                        break\n",
        "\n",
        "    # ---------- ROI EXIT ----------\n",
        "    elif not is_in_roi_now and was_in_roi:\n",
        "        events_to_log.append({\n",
        "            **event_data_template,\n",
        "            'event_timestamp': current_ts,\n",
        "            'event_type': \"roi_exit\",\n",
        "            'details': {\"roi\": current_roi, 'centroid': centroid}\n",
        "        })\n",
        "\n",
        "        # 從 recent_roi_enters 拿出此物件\n",
        "        exiting_item = None\n",
        "        for it in list(recent_roi_enters):\n",
        "            if it['id'] == track_id:\n",
        "                exiting_item = it\n",
        "                recent_roi_enters.remove(it)\n",
        "                break\n",
        "\n",
        "        # 若 leaving 物件是「人」且曾配對 → 觸發 something_away\n",
        "        if exiting_item and exiting_item['cls'] in ('other_person', 'delivery_worker', 'food_delivery'):\n",
        "            paired_id = exiting_item.get('paired')\n",
        "            if paired_id is not None:\n",
        "                # 1) log 事件\n",
        "                events_to_log.append({\n",
        "                    **event_data_template,\n",
        "                    'track_id': None,                  # away 與人分開\n",
        "                    'event_timestamp': current_ts,\n",
        "                    'event_type': \"away\",\n",
        "                    'details': {\n",
        "                        'id1': track_id,\n",
        "                        'id2': paired_id,\n",
        "                        'distance_px': None\n",
        "                    }\n",
        "                })\n",
        "                # 2) 清掉包裹條目的 paired\n",
        "                for it in recent_roi_enters:\n",
        "                    if it['id'] == paired_id:\n",
        "                        it['paired'] = None\n",
        "                        break\n",
        "                # 3) ★ 將包裹加進 away_overlay，之後自動畫 AWAY ★\n",
        "                away_overlay[track_id] = current_ts + datetime.timedelta(seconds=AWAY_DISPLAY_SEC)\n",
        "\n",
        "        # 清理狀態\n",
        "        object_in_roi_status[track_id] = False\n",
        "        object_loitering_start_time.pop(track_id, None)\n",
        "\n",
        "    # ---------- ROI 外且之前也在 ROI 外 ----------\n",
        "    # else: 不需處理\n",
        "\n",
        "    return events_to_log\n",
        "\n",
        "\n",
        "def analyze_interactions_for_frame(trk_objs,\n",
        "                                   current_ts,\n",
        "                                   annotated_frame,\n",
        "                                   cls_names,\n",
        "                                   fps_val,\n",
        "                                   f_w,\n",
        "                                   f_h,\n",
        "                                   allowed_pairs=ALLOWED_INTERACTIONS):\n",
        "    \"\"\"\n",
        "    判斷同一幀內所有物件的接近 / 離開事件；並在「approach」當下，\n",
        "    若符合時窗條件，額外觸發 something_arrival。\n",
        "    \"\"\"\n",
        "    global pair_state, approach_depature_thresh_px, recent_roi_enters\n",
        "    global something_arrival_pairs, arrival_overlay          # ★ 新增\n",
        "    events_to_log = []\n",
        "    thresh_px = approach_depature_thresh_px  # 例如 210 px\n",
        "\n",
        "    # ── step-0：先把仍在有效期內的 ARRIVAL 標註畫上去 ────────────────\n",
        "    expired = []\n",
        "    for pair_key, expire_ts in arrival_overlay.items():\n",
        "        if current_ts > expire_ts:          # 到期就等會一起移除\n",
        "            expired.append(pair_key)\n",
        "            continue\n",
        "        id_a, id_b = pair_key\n",
        "        # 兩個物件還在畫面才畫標註\n",
        "        objs_cent = {}\n",
        "        for obj in trk_objs:\n",
        "            _, _, _, _, tid, cls_id = obj[:6]\n",
        "            if tid in pair_key:\n",
        "                cx = int((obj[0] + obj[2]) / 2)\n",
        "                cy = int((obj[1] + obj[3]) / 2)\n",
        "                objs_cent[int(tid)] = (cx, cy)\n",
        "        if len(objs_cent) == 2:\n",
        "            mid_pt = (int((objs_cent[id_a][0] + objs_cent[id_b][0]) / 2),\n",
        "                      int((objs_cent[id_a][1] + objs_cent[id_b][1]) / 2))\n",
        "            cv2.putText(annotated_frame, \"ARRIVAL\",\n",
        "                        (mid_pt[0] - 35, mid_pt[1] - 28),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "    # 清理由於到期或配對消失的項目\n",
        "    for pk in expired:\n",
        "        arrival_overlay.pop(pk, None)\n",
        "\n",
        "    # ── step-1：把所有物件整理成 dict 方便後續 ─────────────────────\n",
        "    objects = {}\n",
        "    for obj in trk_objs:\n",
        "        x1, y1, x2, y2, tid, cls_id = obj[:6]\n",
        "        tid = int(tid)\n",
        "        cls_name = cls_names.get(int(cls_id), \"unknown\").lower()\n",
        "        cx = int((x1 + x2) / 2)\n",
        "        cy = int((y1 + y2) / 2)\n",
        "        objects[tid] = {\"cent\": (cx, cy), \"cls\": cls_name}\n",
        "\n",
        "    # ── step-2：兩兩配對檢查 approach / depart ─────────────────────\n",
        "    ids = list(objects.keys())\n",
        "    for i in range(len(ids)):\n",
        "        for j in range(i + 1, len(ids)):\n",
        "            id1, id2 = ids[i], ids[j]\n",
        "            o1, o2 = objects[id1], objects[id2]\n",
        "\n",
        "            # 只保留有意義的配對\n",
        "            if allowed_pairs is not None:\n",
        "                pair_cls = (o1[\"cls\"], o2[\"cls\"])\n",
        "                if pair_cls not in allowed_pairs and pair_cls[::-1] not in allowed_pairs:\n",
        "                    continue\n",
        "\n",
        "            # 距離 / 狀態\n",
        "            dist = calculate_distance(o1[\"cent\"], o2[\"cent\"])\n",
        "            now_state = \"near\" if dist < thresh_px else \"far\"\n",
        "            pair_key = (min(id1, id2), max(id1, id2))\n",
        "            prev_state = pair_state.get(pair_key, \"far\")\n",
        "\n",
        "            # ---------- ① approach ----------\n",
        "            if prev_state == \"far\" and now_state == \"near\":\n",
        "                # ── (a) 記 approach 事件\n",
        "                events_to_log.append({\n",
        "                    \"event_timestamp\": current_ts,\n",
        "                    \"event_type\": \"approach\",\n",
        "                    \"details\": {\n",
        "                        \"id1\": id1, \"class1\": o1[\"cls\"],\n",
        "                        \"id2\": id2, \"class2\": o2[\"cls\"],\n",
        "                        \"distance_px\": round(dist, 1)\n",
        "                    },\n",
        "                    \"current_fps\": fps_val,\n",
        "                    \"frame_w\": f_w, \"frame_h\": f_h\n",
        "                })\n",
        "                # 假設 a 是人、b 是物（可自行判斷翻轉）, approach 時寫入 paired\n",
        "                for it in recent_roi_enters:\n",
        "                    if 'id' in it and it['id'] == id1:\n",
        "                        it['paired'] = id2\n",
        "                    elif 'id' in it and it['id'] == id2:\n",
        "                        it['paired'] = id1\n",
        "                        \n",
        "                # ── (b) 若人-物進入 ROI 時間差 ≤ FRAME_WINDOW_SECOND → something_arrival\n",
        "                enter_ts_a = enter_ts_b = None\n",
        "                for it in recent_roi_enters:\n",
        "                    if it.get(\"id\") == id1:\n",
        "                        enter_ts_a = it.get(\"enter_ts\")\n",
        "                    elif it.get(\"id\") == id2:\n",
        "                        enter_ts_b = it.get(\"enter_ts\")\n",
        "                \n",
        "                mid_pt = (int((o1[\"cent\"][0] + o2[\"cent\"][0]) / 2),\n",
        "                          int((o1[\"cent\"][1] + o2[\"cent\"][1]) / 2))\n",
        "                # 都找到才比對\n",
        "                if (enter_ts_a and enter_ts_b and\n",
        "                    abs((enter_ts_a - enter_ts_b).total_seconds()) <= FRAME_WINDOW_SECOND and\n",
        "                    pair_key not in something_arrival_pairs):\n",
        "\n",
        "                    # 觸發事件 & 記錄不可重複\n",
        "                    something_arrival_pairs.add(pair_key)\n",
        "                    events_to_log.append({\n",
        "                        \"event_timestamp\": current_ts,\n",
        "                        \"track_id\": None,  # 到達事件不需要 track_id\n",
        "                        \"event_type\": \"arrival\",\n",
        "                        \"details\": {\n",
        "                            \"human_id\": id1 if o1[\"cls\"] in ('other_person','delivery_worker','food_delivery') else id2,\n",
        "                            \"thing_id\": id2 if o1[\"cls\"] in ('other_person','delivery_worker','food_delivery') else id1,\n",
        "                            \"delta_t_sec\": round(abs((enter_ts_a - enter_ts_b).total_seconds()), 2)\n",
        "                        },\n",
        "                        \"current_fps\": fps_val,\n",
        "                        \"frame_w\": f_w, \"frame_h\": f_h\n",
        "                    })\n",
        "                    # 畫面標註並加入 overlay\n",
        "                    \n",
        "                    cv2.putText(annotated_frame, \"ARRIVAL\",\n",
        "                                (mid_pt[0] - 35, mid_pt[1] - 28),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "                    arrival_overlay[pair_key] = current_ts + datetime.timedelta(seconds=ARRIVAL_DISPLAY_SEC)\n",
        "                '''\n",
        "                # ── (c) 原本的 approach 線段 & 文字\n",
        "                cv2.line(annotated_frame, o1[\"cent\"], o2[\"cent\"], (0, 255, 255), 2)\n",
        "                cv2.putText(annotated_frame, f\"approach {dist:.1f}px\",\n",
        "                            (mid_pt[0] - 40, mid_pt[1] - 8),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
        "                '''\n",
        "                # 根據 now_state 給不同顏色\n",
        "                if now_state == \"near\":\n",
        "                    line_color = (0, 255, 255)  # 黃色\n",
        "                else:\n",
        "                    line_color = (0, 255, 0)    # 綠色\n",
        "                cv2.line(annotated_frame, o1[\"cent\"], o2[\"cent\"], line_color, 2)\n",
        "                cv2.putText(annotated_frame, f\"dist: {dist:.1f}px\",\n",
        "                            (mid_pt[0] - 35, mid_pt[1] - 8),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, line_color, 2)\n",
        "\n",
        "            # ---------- ② depart ----------\n",
        "            elif prev_state == \"near\" and now_state == \"far\":\n",
        "                events_to_log.append({\n",
        "                    \"event_timestamp\": current_ts,\n",
        "                    \"event_type\": \"depart\",\n",
        "                    \"details\": {\n",
        "                        \"id1\": id1, \"class1\": o1[\"cls\"],\n",
        "                        \"id2\": id2, \"class2\": o2[\"cls\"],\n",
        "                        \"distance_px\": round(dist, 1)\n",
        "                    },\n",
        "                    \"current_fps\": fps_val,\n",
        "                    \"frame_w\": f_w, \"frame_h\": f_h\n",
        "                })\n",
        "                mid_pt = (int((o1[\"cent\"][0] + o2[\"cent\"][0]) / 2),\n",
        "                          int((o1[\"cent\"][1] + o2[\"cent\"][1]) / 2))\n",
        "                cv2.line(annotated_frame, o1[\"cent\"], o2[\"cent\"], (0, 255, 0), 2)\n",
        "                cv2.putText(annotated_frame, f\"depart {dist:.1f}px\",\n",
        "                            (mid_pt[0] - 35, mid_pt[1] - 8),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            else:\n",
        "                # ---------- ③ 仍保持原狀態 ----------\n",
        "                mid_pt = (int((o1[\"cent\"][0] + o2[\"cent\"][0]) / 2),\n",
        "                          int((o1[\"cent\"][1] + o2[\"cent\"][1]) / 2))\n",
        "                '''\n",
        "                cv2.line(annotated_frame, o1[\"cent\"], o2[\"cent\"], (0, 255, 0), 2)\n",
        "                cv2.putText(annotated_frame, f\"dist: {dist:.1f}px\",\n",
        "                            (mid_pt[0] - 35, mid_pt[1] - 8),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "                '''\n",
        "                # 根據 now_state 給不同顏色\n",
        "                if now_state == \"near\":\n",
        "                    line_color = (0, 255, 255)  # 黃色\n",
        "                else:\n",
        "                    line_color = (0, 255, 0)    # 綠色\n",
        "                cv2.line(annotated_frame, o1[\"cent\"], o2[\"cent\"], line_color, 2)\n",
        "                cv2.putText(annotated_frame, f\"dist: {dist:.1f}px\",\n",
        "                            (mid_pt[0] - 35, mid_pt[1] - 8),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, line_color, 2)\n",
        "\n",
        "            # 更新狀態機\n",
        "            pair_state[pair_key] = now_state\n",
        "\n",
        "    # ── step-3：移除畫面上已不再出現的 pair 狀態 ─────────────────\n",
        "    active_ids = set(ids)\n",
        "    obsolete_pairs = [p for p in pair_state if p[0] not in active_ids or p[1] not in active_ids]\n",
        "    for p in obsolete_pairs:\n",
        "        pair_state.pop(p, None)\n",
        "\n",
        "    return events_to_log\n",
        "\n",
        "\n",
        "def draw_tracked_objects_and_stats(frame_to_draw_on, trk_objs, cls_names, current_roi, fps_val, current_frame_timestamp):\n",
        "    # This function now MODIFIES frame_to_draw_on IN PLACE\n",
        "    global total_frames_read_count, total_frames_processed_count, cumulative_detected_class_counts\n",
        "    global object_loitering_start_time\n",
        "    if current_roi: cv2.rectangle(frame_to_draw_on, (current_roi[0], current_roi[1]), (current_roi[2], current_roi[3]), (255,255,0),2); cv2.putText(frame_to_draw_on,\"ROI\",(current_roi[0],current_roi[1]-10),cv2.FONT_HERSHEY_SIMPLEX,0.7,(255,255,0),2)\n",
        "    for o in trk_objs:\n",
        "        if len(o)==7:\n",
        "            x1, y1, x2, y2, tid, cid, scr = map(float, o)\n",
        "            x1, y1, x2, y2, tid, cid = int(x1), int(y1), int(x2), int(y2), int(tid), int(cid)\n",
        "            cname = cls_names.get(cid, \"Unk\")\n",
        "            clr = get_color_by_id(tid)\n",
        "            cv2.rectangle(frame_to_draw_on, (x1, y1), (x2, y2), clr, 2)\n",
        "            lbl = f\"ID:{tid} {cname} {scr:.2f}\"\n",
        "            (lw, lh), bl = cv2.getTextSize(lbl, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "            ly = max(lh + 5, y1 - 5)\n",
        "            lx = x1\n",
        "            cv2.rectangle(frame_to_draw_on, (lx, ly - lh - bl), (lx + lw, ly + bl), clr, cv2.FILLED)\n",
        "            cv2.putText(frame_to_draw_on, lbl, (lx, ly), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
        "            if object_loitering_start_time.get(tid) and isinstance(object_loitering_start_time[tid], datetime.datetime):\n",
        "                # dur = (datetime.datetime.now() - object_loitering_start_time[tid]).total_seconds()\n",
        "                dur = (current_frame_timestamp - object_loitering_start_time[tid]).total_seconds()\n",
        "                cv2.putText(frame_to_draw_on, f\"Loiter:{dur:.1f}s\", (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "    \n",
        "    return frame_to_draw_on # Return the modified frame\n",
        "\n",
        "def get_color_by_id(track_id): np.random.seed(track_id); return tuple(np.random.randint(0,255,size=3).tolist())\n",
        "\n",
        "def save_event_log_final(log_data, filepath):\n",
        "    try:\n",
        "        existing_log = []\n",
        "        if os.path.exists(filepath):\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f_in:\n",
        "                try: existing_log = json.load(f_in)\n",
        "                except json.JSONDecodeError: existing_log = []\n",
        "                if not isinstance(existing_log, list): existing_log = []\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f_out:\n",
        "            json.dump(existing_log + log_data, f_out, indent=4, ensure_ascii=False)\n",
        "        print(f\"Event log ({len(log_data)} new entries) appended to: {filepath}\")\n",
        "    except Exception as e: print(f\"ERROR saving event log: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18adb93",
      "metadata": {},
      "source": [
        "## Main Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6bee77cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "def main():\n",
        "    global event_log, track_history, object_loitering_start_time, ROI, frame_buffer, active_clip_capture_tasks\n",
        "    global total_frames_read_count, total_frames_processed_count, cumulative_detected_class_counts\n",
        "    global object_in_roi_status, object_interaction_status, approach_depature_thresh_px, arrival_away_thresh_px\n",
        "    global recent_roi_enters, recent_roi_exits\n",
        "    \n",
        "    event_log.clear(); track_history.clear(); object_loitering_start_time.clear(); ROI = None\n",
        "    frame_buffer.clear(); active_clip_capture_tasks.clear()\n",
        "    object_in_roi_status.clear(); object_interaction_status.clear()\n",
        "    total_frames_read_count = 0; total_frames_processed_count = 0; cumulative_detected_class_counts.clear()\n",
        "    recent_roi_enters.clear(); recent_roi_exits.clear()\n",
        "\n",
        "    model_path = 'best.pt'\n",
        "    local_video_path = INPUT_SOURCE # Make sure this video exists or provide a new one\n",
        "    output_video_path = RUN_DIR / 'output_tracked_intent_boxmot_v8.mp4'\n",
        "    conf_threshold = 0.3\n",
        "    max_duration_sec = None # Set to None or a large number for full video processing\n",
        "    \n",
        "    # Create output directories if they don't exist\n",
        "    if ENABLE_EVENT_CLIPS: Path(EVENT_CLIP_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    if ENABLE_EVENT_SNAPSHOTS: Path(EVENT_SNAPSHOT_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"Loading model: {model_path}\")\n",
        "    if not Path(model_path).exists():\n",
        "        print(f\"Model {model_path} not found. Downloading yolov8n.pt.\")\n",
        "        try: torch.hub.download_url_to_file('https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt', model_path); print(\"yolov8n.pt downloaded.\")\n",
        "        except Exception as e: print(f\"Error downloading default model: {e}. Upload manually.\"); return\n",
        "            \n",
        "    try: model = YOLO(model_path); class_names_dict = model.names; print(f\"Model loaded. Classes: {class_names_dict}\")\n",
        "    except Exception as e: print(f\"ERROR loading YOLO model: {e}\"); return\n",
        "\n",
        "    if not Path(local_video_path).exists():\n",
        "        print(f\"ERROR: Video {local_video_path} not found. Please upload a video named 'sample_video.mp4' or change the path.\"); return\n",
        "\n",
        "    cap = cv2.VideoCapture(local_video_path)\n",
        "    if not cap.isOpened(): print(f\"ERROR: Cannot open video: {local_video_path}\"); return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    print(f\"Video: {frame_width}x{frame_height} @ {fps:.2f} FPS\")\n",
        "    \n",
        "    diag_len = (frame_width ** 2 + frame_height ** 2) ** 0.5\n",
        "    approach_depature_thresh_px = diag_len * INTER_THRESH_RATIO\n",
        "    arrival_away_thresh_px = diag_len * ARRIVAL_AWAY_DIST_RATIO\n",
        "\n",
        "    frame_buffer = deque(maxlen=int(fps * FRAME_BUFFER_DURATION_SEC))\n",
        "    print(f\"Annotated frame buffer size: {frame_buffer.maxlen} frames ({FRAME_BUFFER_DURATION_SEC}s at {fps:.2f} FPS)\")\n",
        "\n",
        "    if ROI_MODE == 1:\n",
        "        if frame_width > 2*ROI_MARGIN_PIXELS and frame_height > 2*ROI_MARGIN_PIXELS:\n",
        "            ROI = (ROI_MARGIN_PIXELS, ROI_MARGIN_PIXELS, frame_width-ROI_MARGIN_PIXELS, frame_height-ROI_MARGIN_PIXELS)\n",
        "        else: ROI = (0,0,frame_width,frame_height); print(\"WARN: Frame too small for margin, using full frame ROI.\")\n",
        "    elif ROI_MODE == 0: ROI = MANUAL_ROI\n",
        "    else: ROI = (0,0,frame_width,frame_height); print(\"WARN: Invalid ROI_MODE, using full frame ROI.\")\n",
        "    if ROI: print(f\"Using ROI: {ROI}\")\n",
        "    else: print(\"No ROI defined (MANUAL_ROI is None and ROI_MODE is not 1 or frame too small). Processing full frame for ROI checks.\")\n",
        "\n",
        "    tracker = None; current_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    try:\n",
        "        tracker_type = 'bytetrack'\n",
        "        tracker_config_path_cand = TRACKER_CONFIGS / (tracker_type + '.yaml') if isinstance(TRACKER_CONFIGS, Path) and (TRACKER_CONFIGS / (tracker_type + '.yaml')).exists() else None\n",
        "        if not tracker_config_path_cand:\n",
        "            import boxmot; pkg_cfg_path = Path(boxmot.__file__).parent/'configs'/(tracker_type+'.yaml')\n",
        "            if pkg_cfg_path.exists(): tracker_config_path_cand = pkg_cfg_path\n",
        "            else: raise FileNotFoundError(f\"BoxMOT config for {tracker_type} not found.\")\n",
        "        print(f\"Using BoxMOT tracker config: {tracker_config_path_cand}\")\n",
        "        tracker = create_tracker(tracker_type, tracker_config_path_cand, None, current_device, False, False)\n",
        "        print(f\"BoxMOT {tracker_type} tracker initialized on {current_device}.\")\n",
        "    except Exception as e: print(f\"ERROR initializing BoxMOT: {e}. Using DummyTracker.\"); tracker = DummyTracker()\n",
        "    if tracker is None: print(\"CRITICAL: Tracker is None. Aborting.\"); return\n",
        "\n",
        "    out_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width,frame_height))\n",
        "    \n",
        "    processing_start_time = datetime.datetime.now()\n",
        "    print(\"Starting video processing...\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, original_frame = cap.read()\n",
        "        if not ret: print(\"End of video or read error.\"); break\n",
        "        total_frames_read_count += 1\n",
        "        # current_frame_timestamp = datetime.datetime.now()\n",
        "        current_frame_timestamp = processing_start_time + datetime.timedelta(seconds=total_frames_read_count / fps)\n",
        "        \n",
        "        annotated_frame = original_frame.copy()\n",
        "\n",
        "        yolo_results = model.predict(original_frame, conf=conf_threshold, verbose=False)\n",
        "        detections_tensor = yolo_results[0].boxes.data\n",
        "        if detections_tensor.numel() > 0:\n",
        "            for cls_id in detections_tensor[:, 5].int().tolist(): cumulative_detected_class_counts[cls_id] += 1\n",
        "\n",
        "        detections_np = np.empty((0,6))\n",
        "        if isinstance(detections_tensor, torch.Tensor) and detections_tensor.numel() > 0:\n",
        "            detections_np = detections_tensor.detach().cpu().numpy().astype(\"float32\")\n",
        "\n",
        "        tracked_dets_np = np.empty((0,7))\n",
        "        if detections_np.shape[0] > 0 or not isinstance(tracker, DummyTracker):\n",
        "            tracked_dets_np = tracker.update(detections_np, original_frame) \n",
        "            total_frames_processed_count +=1\n",
        "        elif isinstance(tracker, DummyTracker):\n",
        "            tracked_dets_np = tracker.update(None, original_frame); total_frames_processed_count +=1\n",
        "        else: total_frames_processed_count +=1\n",
        "\n",
        "        current_tracked_objects_list = []\n",
        "        active_ids_this_frame = set()\n",
        "        all_events_for_this_frame = []\n",
        "\n",
        "        if tracked_dets_np.shape[0] > 0:\n",
        "            for trk_data in tracked_dets_np:\n",
        "                x1,y1,x2,y2,trk_id,conf,cls_id = trk_data[:7] # [x1, y1, x2, y2, track_id, score, cls_id]   # ← score 在第 6 欄\n",
        "                # 先判斷回傳欄位數量 (7 or 8) 再正確拆包\n",
        "                if trk_data.shape[0] == 8:       # 新版 BoxMOT\n",
        "                    x1, y1, x2, y2, trk_id, conf, cls_id, _ = trk_data\n",
        "                else:                            # 舊版 (7 欄) 或其他 tracker\n",
        "                    x1, y1, x2, y2, trk_id, conf, cls_id     = trk_data\n",
        "                # safety check：class id 不在字典就直接略過或指定 fallback\n",
        "                cls_id_int = int(cls_id)\n",
        "                if cls_id_int not in class_names_dict:\n",
        "                    # 你可以 choose to:\n",
        "                    continue                     # 1. 直接不處理這筆 (建議)\n",
        "                    # or\n",
        "                    # trk_class_name = \"other\"     # 2. 全部歸到 'other'\n",
        "                else:\n",
        "                    trk_class_name = class_names_dict[cls_id_int]\n",
        "\n",
        "                current_tracked_objects_list.append([x1,y1,x2,y2,trk_id,cls_id,conf])\n",
        "                active_ids_this_frame.add(int(trk_id))\n",
        "                trk_centroid = get_centroid(trk_data); trk_class_name = class_names_dict.get(int(cls_id), \"Unknown\")\n",
        "                if trk_class_name == \"unknown\": \n",
        "                    print(f\"WARNING: Detected class ID {cls_id} not in class names dictionary. Using 'Unknown'.\")\n",
        "                   \n",
        "                track_history[int(trk_id)].append((current_frame_timestamp, trk_centroid[0], trk_centroid[1], int(cls_id), conf))\n",
        "                \n",
        "                # Analyze behavior for this object (e.g., loitering, ROI entry/exit)\n",
        "                # Pass annotated_frame here in case analyze_behavior needs to draw (though it currently doesn't)\n",
        "                behavior_events = analyze_behavior(int(trk_id), track_history, [x1,y1,x2,y2], int(cls_id), trk_class_name, current_frame_timestamp, fps, ROI, frame_width, frame_height,annotated_frame)\n",
        "                if behavior_events:\n",
        "                    all_events_for_this_frame.extend(behavior_events)\n",
        "        \n",
        "        # Analyze interactions between all currently tracked objects for this frame\n",
        "        # This function WILL draw on annotated_frame if interactions occur\n",
        "        # interaction_events = analyze_interactions_for_frame(current_tracked_objects_list, current_frame_timestamp, annotated_frame, class_names_dict, fps, frame_width, frame_height)\n",
        "        interaction_events = analyze_interactions_for_frame(\n",
        "            trk_objs=current_tracked_objects_list,\n",
        "            current_ts=current_frame_timestamp,\n",
        "            annotated_frame=annotated_frame,\n",
        "            cls_names=class_names_dict,    # id → 字串對照表\n",
        "            fps_val=fps,\n",
        "            f_w=frame_width,\n",
        "            f_h=frame_height\n",
        "        )\n",
        "        if interaction_events:\n",
        "            all_events_for_this_frame.extend(interaction_events)\n",
        "        \n",
        "        # Draw all general annotations (object boxes, stats, ROI) onto the annotated_frame\n",
        "        # This happens AFTER interaction-specific annotations might have been drawn by analyze_interactions_for_frame\n",
        "        draw_tracked_objects_and_stats(annotated_frame, current_tracked_objects_list, class_names_dict, ROI, fps, current_frame_timestamp)\n",
        "\n",
        "        # Now, log all collected events for this frame, using the fully annotated_frame for media\n",
        "        \n",
        "        for event_data_item in all_events_for_this_frame:\n",
        "            log_event(event_data_item, annotated_frame_for_media=annotated_frame)\n",
        "\n",
        "        # Clean up history for tracks that are no longer active\n",
        "        for inactive_id in list(track_history.keys() - active_ids_this_frame):\n",
        "            if inactive_id in track_history: del track_history[inactive_id]\n",
        "            if inactive_id in object_loitering_start_time: del object_loitering_start_time[inactive_id]\n",
        "            if inactive_id in object_in_roi_status: del object_in_roi_status[inactive_id]\n",
        "            \n",
        "            # 清理互動狀態字典中與此ID相關的所有項目\n",
        "            for key in list(object_interaction_status.keys()):\n",
        "                if isinstance(key, tuple) and len(key) == 2 and inactive_id in key:\n",
        "                    del object_interaction_status[key]\n",
        "                elif isinstance(key, tuple) and len(key) == 4 and (key[1] == inactive_id or key[3] == inactive_id):\n",
        "                    del object_interaction_status[key]\n",
        "            '''\n",
        "            for it in list(recent_roi_enters):\n",
        "                if it['id'] == inactive_id:\n",
        "                    recent_roi_enters.remove(it)\n",
        "            '''\n",
        "        \n",
        "        # Add the fully ANNOTATED frame to the buffer for clip saving\n",
        "        frame_buffer.append((annotated_frame.copy(), current_frame_timestamp))\n",
        "\n",
        "        # Write the ANNOTATED frame to the output video\n",
        "        out_writer.write(annotated_frame)\n",
        "\n",
        "        # --- Handle active clip capture tasks (uses ANNOTATED frames from buffer) ---\n",
        "        if ENABLE_EVENT_CLIPS:\n",
        "            remaining_tasks = []\n",
        "            for task in active_clip_capture_tasks:\n",
        "                is_complete = False\n",
        "                # Check if enough frames collected or if it's the end of the video\n",
        "                if task['collected_frames']:\n",
        "                    # Ensure frames are sorted by timestamp before checking end condition\n",
        "                    task['collected_frames'].sort(key=lambda x: x[1])\n",
        "                    last_collected_ts = task['collected_frames'][-1][1]\n",
        "                    if last_collected_ts >= task['desired_clip_end_ts']:\n",
        "                        is_complete = True\n",
        "                \n",
        "                # If processing has ended (not ret) and task has frames, consider it complete for saving\n",
        "                if (not ret and task['collected_frames']) or is_complete:\n",
        "                    frames_data_to_save = [f_data for f_data, ts_data in task['collected_frames'] \n",
        "                                           if ts_data >= task['desired_clip_start_ts'] and ts_data <= task['desired_clip_end_ts']]\n",
        "                    if frames_data_to_save:\n",
        "                         save_video_clip(frames_data_to_save, task['output_filename'], task['fps'], task['width'], task['height'])\n",
        "                    # Mark as processed by not adding to remaining_tasks\n",
        "                else:\n",
        "                    # If not complete, keep collecting frames if current frame is within desired range\n",
        "                    if current_frame_timestamp <= task['desired_clip_end_ts']:\n",
        "                         # Only add if current frame is relevant to this task's time window\n",
        "                         if current_frame_timestamp >= task['desired_clip_start_ts']:\n",
        "                            # Check if frame already added (e.g. from pre-fill)\n",
        "                            if not any(f_ts == current_frame_timestamp for _, f_ts in task['collected_frames']):\n",
        "                                task['collected_frames'].append((annotated_frame.copy(), current_frame_timestamp))\n",
        "                    remaining_tasks.append(task)\n",
        "            active_clip_capture_tasks = remaining_tasks\n",
        "\n",
        "        \n",
        "        \n",
        "        if max_duration_sec and (time.time() - processing_start_time > max_duration_sec): print(f\"Max duration {max_duration_sec}s reached.\"); break\n",
        "        if total_frames_read_count % 100 == 0: print(f\"Processed {total_frames_read_count} frames...\")\n",
        "    \n",
        "    # Cleanup and finalize\n",
        "    cap.release(); out_writer.release()\n",
        "    \n",
        "    \n",
        "    # Save event log\n",
        "    if event_log:\n",
        "        save_event_log_final(event_log, EVENT_LOG_FILE)\n",
        "        print(f\"\\n--- Event Log Summary ---\")\n",
        "        event_types = {}\n",
        "        for e in event_log: event_types[e['event_type']] = event_types.get(e['event_type'], 0) + 1\n",
        "        for et, count in sorted(event_types.items()): print(f\"{et}: {count} events\")\n",
        "    else: print(\"No events were logged.\")\n",
        "    \n",
        "    # --- Event Clips Status (原樣保留) --------------------------------------\n",
        "    print(f\"\\n--- Event Clips ({EVENT_CLIP_OUTPUT_DIR}/) Status ---\")\n",
        "    if Path(EVENT_CLIP_OUTPUT_DIR).is_dir():\n",
        "        clips = list(Path(EVENT_CLIP_OUTPUT_DIR).glob(\"*.mp4\"))\n",
        "        print(f\"Found {len(clips)} clips in {EVENT_CLIP_OUTPUT_DIR}.\")\n",
        "    else:\n",
        "        print(f\"Event clips directory {EVENT_CLIP_OUTPUT_DIR} not found.\")\n",
        "    \n",
        "    # --- Event Snapshots Status (原樣保留) --------------------------------------\n",
        "    print(f\"\\n--- Event Snapshots ({EVENT_SNAPSHOT_OUTPUT_DIR}/) Status ---\")\n",
        "    if Path(EVENT_SNAPSHOT_OUTPUT_DIR).is_dir():\n",
        "        snaps = list(Path(EVENT_SNAPSHOT_OUTPUT_DIR).glob(\"*.jpg\"))\n",
        "        print(f\"Found {len(snaps)} snapshots in {EVENT_SNAPSHOT_OUTPUT_DIR}.\")\n",
        "    else:\n",
        "        print(f\"Event snapshots directory {EVENT_SNAPSHOT_OUTPUT_DIR} not found.\")\n",
        "\n",
        "    print(f\"\\n--- Processing Complete ---\")\n",
        "    print(f\"Total frames read: {total_frames_read_count}\")\n",
        "    print(f\"Total frames processed: {total_frames_processed_count}\")\n",
        "    print(f\"Output video saved to: {output_video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "19e49b15",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: best.pt\n",
            "Model loaded. Classes: {0: 'package', 1: 'bag', 2: 'other_person', 3: 'delivery_worker', 4: 'food_delivery'}\n",
            "Video: 1280x720 @ 30.00 FPS\n",
            "Annotated frame buffer size: 300 frames (10s at 30.00 FPS)\n",
            "Using ROI: (0, 250, 650, 720)\n",
            "Using BoxMOT tracker config: C:\\Users\\user\\venv_tracker\\Lib\\site-packages\\boxmot\\configs\\bytetrack.yaml\n",
            "BoxMOT bytetrack tracker initialized on cpu.\n",
            "Starting video processing...\n",
            "Processed 100 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id100_20250601_135005_270.jpg\n",
            "Processed 200 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id102_20250601_135008_537.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135008_837.jpg\n",
            "[LINE-Bot] 圖片已推送到 LINE: https://storage.googleapis.com/cv_event_image/event_images/arrival_20250601_135208.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=644134838534-compute%40developer.gserviceaccount.com%2F20250601%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250601T055209Z&X-Goog-Expires=86400&X-Goog-SignedHeaders=host&X-Goog-Signature=373fbaecdb7d252515ecedd7e7ef0b9932e630913ccbdb5e7d0132e1024d63846a06d5648bf270ff91e76a8fac8f777ea608ba6deeb1eced2962e2dbb62665cf20880c1872b835ad865f3d66ca004b06d9c7669cc713b006bd1dde94c2e66574332fd4b501f2979daced97f57e78a0501f969f4bac16f4a67c6d755f8cfa64c9b6b6433761e5c0fab1e009ea80267a5be91e7d221cbe7805ea1596fcd744bc9c1828b51e2388fdb86621af9d3021a7121608c58d717e3be5f772c06f6f7083b612369c4019266e3675e1023df9976b2176fc1f9563db91708422ab4a434160443b78bcff53bee990ced68505756f6c4c57c08024160d5b54079251792cd4336b\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\arrival_20250601_135008_837.jpg\n",
            "Processed 300 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135010_370.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135012_070.jpg\n",
            "Processed 400 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\loitering_id100_20250601_135015_104.jpg\n",
            "Processed 500 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\loitering_id102_20250601_135018_937.jpg\n",
            "Processed 600 frames...\n",
            "Processed 700 frames...\n",
            "Processed 800 frames...\n",
            "Processed 900 frames...\n",
            "Processed 1000 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135033_537.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135034_370.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135034_537.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135034_737.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135035_504.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135035_570.jpg\n",
            "Processed 1100 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135037_670.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135037_770.jpg\n",
            "Processed 1200 frames...\n",
            "Processed 1300 frames...\n",
            "Processed 1400 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135046_570.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_exit_id102_20250601_135048_204.jpg\n",
            "[LINE-Bot] 圖片已推送到 LINE: https://storage.googleapis.com/cv_event_image/event_images/away_20250601_140338.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=644134838534-compute%40developer.gserviceaccount.com%2F20250601%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250601T060338Z&X-Goog-Expires=86400&X-Goog-SignedHeaders=host&X-Goog-Signature=1e202bc31a0ce91b7329f7e79eb354e69d6feed98251a651185784fa0ec2acfb69dfe27fbfbe7c8be125f5f72993b5d5e415c126c8a9298c5450b6a1a5dd9cdaf11a08f4a552f9554db926e14fbb837f9e2c0050098c3fc0269b894db1f11829e90c8f46242285da4746a360ba3d4b910da7f0d66737918be79da83a939d99f8c1cec2c40cdd1e6f4de0a3762deb1a03c969ed808e7e327e8ac4e3efc5a1e120368c496a25291020411050dc2d28084f057947a81e47fa7c02403ac3dba8dbac98c7b95a55ef95fc905a3af5fb0836ac7a451441b715bc66826f3eb59d880fe1a42f26cf8688420f8281a82d0fde725e5cca21c94d062c0d860bebb1934b6b8e\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\away_20250601_135048_204.jpg\n",
            "Processed 1500 frames...\n",
            "Processed 1600 frames...\n",
            "Processed 1700 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id103_20250601_135057_804.jpg\n",
            "Processed 1800 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135059_637.jpg\n",
            "Processed 1900 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\loitering_id103_20250601_135102_804.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135103_170.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_exit_id103_20250601_135104_970.jpg\n",
            "[LINE-Bot] 圖片已推送到 LINE: https://storage.googleapis.com/cv_event_image/event_images/away_20250601_140840.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=644134838534-compute%40developer.gserviceaccount.com%2F20250601%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250601T060841Z&X-Goog-Expires=86400&X-Goog-SignedHeaders=host&X-Goog-Signature=a70df7e8a8354b810d15ce4e1ce54f29036666465793cb585d1cdb08dd1141ef912fd24856a899bd0749ec97cc048f977a510c2355dfcb046036809e3897e0da134c946feb3e899a9355c608284be88f5421aed85ba5ec6fc4ae37d6695e59f10bd2fe61c4ccaf3f699d3d1956dda2a508b0f28fe94009a79a7ad2401bfb4e141227edf4c3de668f28b2ef61a34900cd8b97ff0d7caee666b22aafbb8c00fb17e28cf925e9d96bb77baffdc1c9b599da6125452c857ecdd3670c58ec43f9b9f8ba85b9adfd81ddcf0550e97e65a6f0150a30a0afb6abaeea8d12aa79326f728d30af6e1555608d95338bbfb26b0948105650094b712d1def7094ce39b161e372\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\away_20250601_135104_970.jpg\n",
            "Processed 2000 frames...\n",
            "Processed 2100 frames...\n",
            "Processed 2200 frames...\n",
            "Processed 2300 frames...\n",
            "Processed 2400 frames...\n",
            "Processed 2500 frames...\n",
            "Processed 2600 frames...\n",
            "Processed 2700 frames...\n",
            "Processed 2800 frames...\n",
            "Processed 2900 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id104_20250601_135138_304.jpg\n",
            "Processed 3000 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135140_804.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135142_370.jpg\n",
            "Processed 3100 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135142_904.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135143_870.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135144_337.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135144_437.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135144_604.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135144_637.jpg\n",
            "Processed 3200 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135146_104.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135147_237.jpg\n",
            "Processed 3300 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\loitering_id104_20250601_135150_404.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135151_337.jpg\n",
            "Processed 3400 frames...\n",
            "Processed 3500 frames...\n",
            "Processed 3600 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id108_20250601_135200_404.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id111_20250601_135201_537.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135201_537.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id112_20250601_135202_337.jpg\n",
            "Processed 3700 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_exit_id100_20250601_135204_170.jpg\n",
            "[LINE-Bot] 圖片已推送到 LINE: https://storage.googleapis.com/cv_event_image/event_images/away_20250601_142829.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=644134838534-compute%40developer.gserviceaccount.com%2F20250601%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250601T062829Z&X-Goog-Expires=86400&X-Goog-SignedHeaders=host&X-Goog-Signature=6a7f65bfb0e7e5dd03838f3e94d94d14eb957ee5861a436fd50bd86ed00fe14866c58b9befa752b40b6581090923edb50d97d7f38a36df81ba75798b851901138c95fa1b4ff04e5a904e0dd37a86a89e79728ffc2df1b545cd8d67b2a7e67b872ec3a0b2b4c7ba7e77902e177d66d29ef266b4f9672ce36616d79f8c83827edd439ee7bd15d5ae8b5cae6d9cdada47939fa5d441ca6162ed044a4bda2a910d2ba577225447315d36f1836bfd97878a9db724cae81d89f3ed5a13fe8ba34a0eee6e063117de0dbcd8dcc9a3e39259319dc4abb6ebfaa4673486dc13aec49b41544dc1eb302473083daaba2573fb9847274844e9a2cf489a38320cdb4e097c538d\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\away_20250601_135204_170.jpg\n",
            "Processed 3800 frames...\n",
            "Processed 3900 frames...\n",
            "Processed 4000 frames...\n",
            "Processed 4100 frames...\n",
            "Processed 4200 frames...\n",
            "Processed 4300 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id115_20250601_135225_370.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id116_20250601_135225_404.jpg\n",
            "Processed 4400 frames...\n",
            "Processed 4500 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\loitering_id115_20250601_135230_370.jpg\n",
            "Processed 4600 frames...\n",
            "Processed 4700 frames...\n",
            "Processed 4800 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id117_20250601_135241_604.jpg\n",
            "Processed 4900 frames...\n",
            "Processed 5000 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\loitering_id117_20250601_135246_604.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_exit_id117_20250601_135247_537.jpg\n",
            "Processed 5100 frames...\n",
            "Processed 5200 frames...\n",
            "Processed 5300 frames...\n",
            "Processed 5400 frames...\n",
            "Processed 5500 frames...\n",
            "Processed 5600 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id118_20250601_135308_870.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id119_20250601_135308_937.jpg\n",
            "Processed 5700 frames...\n",
            "Processed 5800 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\loitering_id118_20250601_135313_870.jpg\n",
            "Processed 5900 frames...\n",
            "Processed 6000 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id120_20250601_135320_037.jpg\n",
            "Processed 6100 frames...\n",
            "Processed 6200 frames...\n",
            "Processed 6300 frames...\n",
            "Processed 6400 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id123_20250601_135333_970.jpg\n",
            "Processed 6500 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\loitering_id123_20250601_135338_970.jpg\n",
            "Processed 6600 frames...\n",
            "Processed 6700 frames...\n",
            "Processed 6800 frames...\n",
            "Processed 6900 frames...\n",
            "Processed 7000 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id128_20250601_135354_370.jpg\n",
            "Processed 7100 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id129_20250601_135356_204.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\approach_20250601_135356_204.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\depart_20250601_135356_470.jpg\n",
            "Processed 7200 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id130_20250601_135400_904.jpg\n",
            "Processed 7300 frames...\n",
            "Processed 7400 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id131_20250601_135406_237.jpg\n",
            "Processed 7500 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id132_20250601_135409_504.jpg\n",
            "Processed 7600 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id133_20250601_135413_904.jpg\n",
            "Processed 7700 frames...\n",
            "Processed 7800 frames...\n",
            "Processed 7900 frames...\n",
            "Processed 8000 frames...\n",
            "Processed 8100 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id134_20250601_135430_304.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id135_20250601_135430_737.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id136_20250601_135431_170.jpg\n",
            "Processed 8200 frames...\n",
            "Processed 8300 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id137_20250601_135436_337.jpg\n",
            "Processed 8400 frames...\n",
            "Processed 8500 frames...\n",
            "Processed 8600 frames...\n",
            "Processed 8700 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id138_20250601_135449_904.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id139_20250601_135449_937.jpg\n",
            "Processed 8800 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\loitering_id139_20250601_135454_937.jpg\n",
            "Processed 8900 frames...\n",
            "Processed 9000 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id142_20250601_135459_870.jpg\n",
            "Processed 9100 frames...\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id143_20250601_135503_837.jpg\n",
            "Event snapshot saved: output\\golden_sample\\20250601_134922\\snapshots\\roi_enter_id145_20250601_135504_104.jpg\n",
            "Processed 9200 frames...\n",
            "Processed 9300 frames...\n",
            "Processed 9400 frames...\n",
            "Processed 9500 frames...\n",
            "Processed 9600 frames...\n",
            "End of video or read error.\n",
            "Event log (74 new entries) appended to: output\\golden_sample\\20250601_134922\\golden_sample_events.json\n",
            "\n",
            "--- Event Log Summary ---\n",
            "approach: 16 events\n",
            "arrival: 1 events\n",
            "away: 3 events\n",
            "depart: 12 events\n",
            "loitering: 9 events\n",
            "roi_enter: 29 events\n",
            "roi_exit: 4 events\n",
            "\n",
            "--- Event Clips (output\\golden_sample\\20250601_134922\\clips/) Status ---\n",
            "Found 0 clips in output\\golden_sample\\20250601_134922\\clips.\n",
            "\n",
            "--- Event Snapshots (output\\golden_sample\\20250601_134922\\snapshots/) Status ---\n",
            "Found 74 snapshots in output\\golden_sample\\20250601_134922\\snapshots.\n",
            "\n",
            "--- Processing Complete ---\n",
            "Total frames read: 9662\n",
            "Total frames processed: 9662\n",
            "Output video saved to: output\\golden_sample\\20250601_134922\\output_tracked_intent_boxmot_v8.mp4\n"
          ]
        }
      ],
      "source": [
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c745a22a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Event Log (output\\golden_sample\\20250601_134922\\golden_sample_events.json) Status ---\n",
            "Found 74 events in output\\golden_sample\\20250601_134922\\golden_sample_events.json.\n",
            "\n",
            "=== Object Class Distribution ===\n",
            " Object Class  Count\n",
            "      <event>     29\n",
            " other_person     25\n",
            "food_delivery     17\n",
            "      package      3\n",
            "\n",
            "=== Event Type Distribution ===\n",
            "Event Type  Count\n",
            " roi_enter     29\n",
            "  approach     16\n",
            "    depart     12\n",
            " loitering      9\n",
            "  roi_exit      4\n",
            "      away      3\n",
            "   arrival      1\n",
            "\n",
            "=== Object × Event Crosstab ===\n",
            "               roi_enter  loitering  roi_exit  away  approach  arrival  depart\n",
            "<event>                0          0         0     0        16        1      12\n",
            "other_person          13          5         4     3         0        0       0\n",
            "food_delivery         14          3         0     0         0        0       0\n",
            "package                2          1         0     0         0        0       0\n",
            "\n",
            "--- Event Clips (output\\golden_sample\\20250601_134922\\clips/) Status ---\n",
            "Found 0 video clips in output\\golden_sample\\20250601_134922\\clips.\n",
            "\n",
            "--- Event Snapshots (output\\golden_sample\\20250601_134922\\snapshots/) Status ---\n",
            "Found 74 snapshots in output\\golden_sample\\20250601_134922\\snapshots.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd      # ✅ 用來排版統計表\n",
        "\n",
        "# --- Review Event Logs, Clips, and Snapshots (Enhanced) --------------------\n",
        "print(f\"\\n--- Event Log ({EVENT_LOG_FILE}) Status ---\")\n",
        "object_counter   = Counter()                 # 物件類別 → 出現次數\n",
        "event_counter    = Counter()                 # 事件類別 → 出現次數\n",
        "obj_event_matrix = defaultdict(Counter)      # 物件類別 → (事件類別 → 次數)\n",
        "\n",
        "if Path(EVENT_LOG_FILE).exists():\n",
        "    try:\n",
        "        with open(EVENT_LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            logged_events_content = json.load(f)\n",
        "\n",
        "        total_events = len(logged_events_content)\n",
        "        print(f\"Found {total_events} events in {EVENT_LOG_FILE}.\")\n",
        "\n",
        "        # ▍統計迴圈\n",
        "        for evt in logged_events_content:\n",
        "            # 1. 事件類別 (可依實際欄位名稱增減備援鍵)\n",
        "            evt_type = evt.get(\"event_type\") or evt.get(\"event\") or evt.get(\"type\") or \"<unknown>\"\n",
        "            event_counter[evt_type] += 1\n",
        "\n",
        "            # 2. 物件類別 (單一字串或 list 皆可)\n",
        "            raw_obj = evt.get(\"class_name\") or evt.get(\"class\") or evt.get(\"object_classes\")\n",
        "            obj_classes = raw_obj if isinstance(raw_obj, list) else [raw_obj or \"<event>\"]\n",
        "\n",
        "            for cls in obj_classes:\n",
        "                object_counter[cls] += 1\n",
        "                obj_event_matrix[cls][evt_type] += 1\n",
        "\n",
        "        # ▍輸出統計表 -------------------------------------------------------\n",
        "        obj_df  = (pd.DataFrame(object_counter.items(), columns=[\"Object Class\", \"Count\"])\n",
        "                     .sort_values(\"Count\", ascending=False))\n",
        "        evt_df  = (pd.DataFrame(event_counter.items(),  columns=[\"Event Type\",  \"Count\"])\n",
        "                     .sort_values(\"Count\", ascending=False))\n",
        "        cross_df = (pd.DataFrame(obj_event_matrix).fillna(0).astype(int).T\n",
        "                      .loc[obj_df[\"Object Class\"]])   # 依物件出現頻次排序\n",
        "\n",
        "        print(\"\\n=== Object Class Distribution ===\")\n",
        "        print(obj_df.to_string(index=False))\n",
        "\n",
        "        print(\"\\n=== Event Type Distribution ===\")\n",
        "        print(evt_df.to_string(index=False))\n",
        "\n",
        "        print(\"\\n=== Object × Event Crosstab ===\")\n",
        "        print(cross_df.to_string())\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading event log: {e}\")\n",
        "else:\n",
        "    print(f\"Event log file {EVENT_LOG_FILE} not found.\")\n",
        "\n",
        "# --- Event Clips Status (原樣保留) -----------------------------------------\n",
        "print(f\"\\n--- Event Clips ({EVENT_CLIP_OUTPUT_DIR}/) Status ---\")\n",
        "if Path(EVENT_CLIP_OUTPUT_DIR).is_dir():\n",
        "    clips = list(Path(EVENT_CLIP_OUTPUT_DIR).glob(\"*.mp4\"))\n",
        "    print(f\"Found {len(clips)} video clips in {EVENT_CLIP_OUTPUT_DIR}.\")\n",
        "else:\n",
        "    print(f\"Event clips directory {EVENT_CLIP_OUTPUT_DIR} not found.\")\n",
        "\n",
        "# --- Event Snapshots Status (原樣保留) --------------------------------------\n",
        "print(f\"\\n--- Event Snapshots ({EVENT_SNAPSHOT_OUTPUT_DIR}/) Status ---\")\n",
        "if Path(EVENT_SNAPSHOT_OUTPUT_DIR).is_dir():\n",
        "    snaps = list(Path(EVENT_SNAPSHOT_OUTPUT_DIR).glob(\"*.jpg\"))\n",
        "    print(f\"Found {len(snaps)} snapshots in {EVENT_SNAPSHOT_OUTPUT_DIR}.\")\n",
        "else:\n",
        "    print(f\"Event snapshots directory {EVENT_SNAPSHOT_OUTPUT_DIR} not found.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_tracker",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
